{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfec46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  #1 time installation\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# import sys, subprocess\n",
    "#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipywidgets\"])\n",
    "#!pip install scikit-learn\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86a1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import Javascript\n",
    "from ipywidgets import Widget, Text, Button, VBox, Label\n",
    "from traitlets import Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba395c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = Path(r\"C:\\Users\\netsi\\OneDrive\\Desktop\\TU Wien\\NLP\\Topic12\\corpus\\corpus.json\")  \n",
    "\n",
    "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "doc_ids   = [doc[\"id\"] for doc in corpus]\n",
    "doc_titles = [doc.get(\"title\", \"\") for doc in corpus]\n",
    "doc_texts = [doc[\"text\"] for doc in corpus]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),   # unigrams + bigrams\n",
    "    max_df=0.9,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(doc_texts)  # num_docs, num_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c37836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50406\n",
      "Sample features: ['crime' 'crime absence' 'crime acknowledge' 'crime actually'\n",
      " 'crime aggregations' 'crime analysis' 'crime appeared' 'crime attractors'\n",
      " 'crime baltimore' 'crime best']\n",
      "\n",
      "TF-IDF values for sample features):\n",
      "crime                     0.6713\n",
      "crime absence             0.0043\n",
      "crime acknowledge         0.0043\n",
      "crime actually            0.0043\n",
      "crime aggregations        0.0043\n",
      "crime analysis            0.0043\n",
      "crime appeared            0.0043\n",
      "crime attractors          0.0043\n",
      "crime baltimore           0.0043\n",
      "crime best                0.0043\n",
      "\n",
      "Top 10 TF-IDF terms in Document 0:\n",
      "                   term     tfidf\n",
      "1406              crime  0.671330\n",
      "3568           mobility  0.209394\n",
      "986              cities  0.138597\n",
      "1484             crimes  0.134266\n",
      "2412        forecasting  0.129443\n",
      "3175                 lb  0.125604\n",
      "3756                 nn  0.106497\n",
      "1437  crime forecasting  0.090954\n",
      "863                cell  0.088116\n",
      "4189                poi  0.086623\n"
     ]
    }
   ],
   "source": [
    "start = 12005\n",
    "stop = start + 10\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "print(\"Sample features:\", feature_names[start:stop])   \n",
    "\n",
    "\n",
    "first_doc_vector = tfidf_matrix[0].toarray()[0]\n",
    "nonzero_idx = first_doc_vector.nonzero()[0]\n",
    "\n",
    "print(\"\\nTF-IDF values for sample features):\")\n",
    "for idx in range(start, stop):\n",
    "    print(f\"{feature_names[idx]:25s} {first_doc_vector[idx]:.4f}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"term\": [feature_names[i] for i in nonzero_idx],\n",
    "    \"tfidf\": [first_doc_vector[i] for i in nonzero_idx]\n",
    "}).sort_values(\"tfidf\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF terms in Document 0:\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6003821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require.undef('text_sender');\n",
       "\n",
       "define('text_sender', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "\n",
       "    var TextSenderView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "\n",
       "            const box = document.createElement(\"div\");\n",
       "            box.style.display = \"flex\";\n",
       "            box.style.flexDirection = \"row\";\n",
       "            box.style.width = \"100%\";        // full width inside widget\n",
       "            box.style.maxWidth = \"100%\";\n",
       "\n",
       "            const input = document.createElement(\"input\");\n",
       "            input.type = \"text\";\n",
       "            input.placeholder = \"Type something…\";\n",
       "\n",
       "            input.style.padding = \"6px\";\n",
       "            input.style.marginRight = \"8px\";\n",
       "\n",
       "            // FORCE width expansion\n",
       "            input.style.flex = \"1\";          // <--- This is the important part\n",
       "            input.style.minWidth = \"0\";      // <--- prevents shrink issues\n",
       "\n",
       "            const button = document.createElement(\"button\");\n",
       "            button.innerHTML = \"Send to Python\";\n",
       "            button.style.padding = \"6px 12px\";\n",
       "\n",
       "            button.onclick = () => {\n",
       "                this.send({text: input.value});\n",
       "            };\n",
       "\n",
       "            box.appendChild(input);\n",
       "            box.appendChild(button);\n",
       "\n",
       "            this.el.appendChild(box);\n",
       "        }\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        TextSenderView : TextSenderView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Javascript(\"\"\"\n",
    "require.undef('text_sender');\n",
    "\n",
    "define('text_sender', [\"@jupyter-widgets/base\"], function(widgets) {\n",
    "\n",
    "    var TextSenderView = widgets.DOMWidgetView.extend({\n",
    "        render: function() {\n",
    "\n",
    "            const box = document.createElement(\"div\");\n",
    "            box.style.display = \"flex\";\n",
    "            box.style.flexDirection = \"row\";\n",
    "            box.style.width = \"100%\";        // full width inside widget\n",
    "            box.style.maxWidth = \"100%\";\n",
    "\n",
    "            const input = document.createElement(\"input\");\n",
    "            input.type = \"text\";\n",
    "            input.placeholder = \"Type something…\";\n",
    "\n",
    "            input.style.padding = \"6px\";\n",
    "            input.style.marginRight = \"8px\";\n",
    "\n",
    "            // FORCE width expansion\n",
    "            input.style.flex = \"1\";          // <--- This is the important part\n",
    "            input.style.minWidth = \"0\";      // <--- prevents shrink issues\n",
    "\n",
    "            const button = document.createElement(\"button\");\n",
    "            button.innerHTML = \"Send to Python\";\n",
    "            button.style.padding = \"6px 12px\";\n",
    "\n",
    "            button.onclick = () => {\n",
    "                this.send({text: input.value});\n",
    "            };\n",
    "\n",
    "            box.appendChild(input);\n",
    "            box.appendChild(button);\n",
    "\n",
    "            this.el.appendChild(box);\n",
    "        }\n",
    "    });\n",
    "\n",
    "    return {\n",
    "        TextSenderView : TextSenderView\n",
    "    };\n",
    "});\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e8d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tfidf(query: str, k: int = 5):\n",
    "    \n",
    "    # Returns top-k documents.\n",
    "\n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Vectorize query\n",
    "    q_vec = vectorizer.transform([query])  # shape: (1, num_terms)\n",
    "\n",
    "    # Cosine similarity with all docs\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix)[0]  # shape: (num_docs,)\n",
    "\n",
    "    # Get top-k indices\n",
    "    topk_idx = np.argsort(sims)[::-1][:k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(topk_idx, start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(sims[idx]),\n",
    "            \"id\": doc_ids[idx],\n",
    "            \"title\": doc_titles[idx],\n",
    "            \"text\": doc_texts[idx]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a0e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4552abd0584d6da0e6e9c5a1f50853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter your question:'), Text(value='', placeholder='Type your question here...'), …"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = Label(\"Enter your question:\")\n",
    "txt = Text(placeholder=\"Type your question here...\")\n",
    "btn = Button(description=\"Submit\")\n",
    "\n",
    "def on_click(b):\n",
    "    global query\n",
    "    query = txt.value\n",
    "    \n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "VBox([label, txt, btn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ec8584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can we detect sacasm using deep learning?\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80cf0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how can we detect sacasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] 2510.10729v1  (cosine similarity=0.0208)\n",
      "Paper title: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "--------------------------------------------------------------------------------\n",
      "Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through  ...\n",
      "\n",
      "[2] 2510.08770v1  (cosine similarity=0.0133)\n",
      "Paper title: Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform\n",
      "--------------------------------------------------------------------------------\n",
      "Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform. This paper presents a real-time spill detection system that utilizes pretrained deep learning models with RGB and thermal imaging to classify spill vs. no-spill scenarios across varied environments. Using a balanced binary dataset 4 000 images our experiments demonstrate the advantages of thermal imaging in inference speed accuracy and model size. We achieve up to 100% accuracy using lightweight mode ...\n",
      "\n",
      "[3] 2510.13137v1  (cosine similarity=0.0132)\n",
      "Paper title: Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 20 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show results at title level\n",
    "results = retrieve_tfidf(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] {r['id']}  (cosine similarity={r['score']:.4f})\")\n",
    "    print(f\"Paper title: {r[\"title\"]}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40cfc061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=220, overlap=40):\n",
    "    \n",
    "    #Split text into overlapping chunks.\n",
    "    #chunk_size: target words per chunk\n",
    "    #overlap: how many words to overlap between consecutive chunks\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(words)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        start = end - overlap  \n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebfe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_texts = []   \n",
    "passage_meta = []   \n",
    "\n",
    "for doc in corpus:\n",
    "    doc_id = doc[\"id\"]\n",
    "    title = doc.get(\"title\", \"\")\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size=220, overlap=40)\n",
    "    start_word = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        end_word = start_word + len(chunk.split())\n",
    "        passage_texts.append(chunk)\n",
    "        passage_meta.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"start_word\": start_word,\n",
    "            \"end_word\": end_word,\n",
    "        })\n",
    "        start_word = end_word - 40  # keep aligned with overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbbf5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n",
      "Number of passages:  491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of passages:  {len(passage_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1c403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.9,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "tfidf_matrix_passages = vectorizer.fit_transform(passage_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf0eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tfidf_chunks(query: str, k: int = 3):\n",
    "    \n",
    "    #Retrieve top-k passages (chunks)\n",
    "    \n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Vectorize query\n",
    "    q_vec = vectorizer.transform([query])\n",
    "\n",
    "    # Cosine similarity against all passages\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix_passages)[0]\n",
    "\n",
    "    # Top-k indices\n",
    "    topk_idx = np.argsort(sims)[::-1][:k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(topk_idx, start=1):\n",
    "        meta = passage_meta[idx]\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(sims[idx]),\n",
    "            \"text\": passage_texts[idx],\n",
    "            \"doc_id\": meta[\"doc_id\"],\n",
    "            \"title\": meta[\"title\"],\n",
    "            \"chunk_id\": meta[\"chunk_id\"],\n",
    "            \"start_word\": meta[\"start_word\"],\n",
    "            \"end_word\": meta[\"end_word\"],\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5492e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how can we detect sacasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] Cosine similarity=0.0932\n",
      "Paper: 2510.10729v1 — Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "Chunk: 2510.10729v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through  ...\n",
      "\n",
      "[2] Cosine similarity=0.0838\n",
      "Paper: 2510.05736v1 — Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes\n",
      "Chunk: 2510.05736v1_chunk_2 (words 360–580)\n",
      "--------------------------------------------------------------------------------\n",
      "Trees BDTs trained on parameterized image features or goodness-of-fit parameters for this task 3 5. Consequently a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Multiple studies have explored deep learning methods for identifying γ-rays and demonstrated exceptional performance on simulated data 6 10. Most model architectures use convolutional neural networks CNNs for extracting i ...\n",
      "\n",
      "[3] Cosine similarity=0.0782\n",
      "Paper: 2509.20913v1 — Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales\n",
      "Chunk: 2509.20913v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles an ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_tfidf_chunks(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] Cosine similarity={r['score']:.4f}\")\n",
    "    print(f\"Paper: {r['doc_id']} — {r['title']}\")\n",
    "    print(f\"Chunk: {r['chunk_id']} (words {r['start_word']}–{r['end_word']})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c78709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dwh)",
   "language": "python",
   "name": "dwh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
