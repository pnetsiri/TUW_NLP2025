question,predicted_paper,correct_paper,is_correct,retrieved_ids,retrieved_scores,retrieved_titles,retrieved_contexts,generated_answer
How are Transformers different from RNNs?,2510.13137v1,,,"['2510.13137v1', '2510.11073v1', '2510.05163v1', '2510.14855v1', '2510.13050v1']","[5.0, 5.0, 5.0, 4.0, 4.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting']","[{'rank': 1, 'score': 5.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 5.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 3, 'score': 5.0, 'id': '2510.05163v1', 'title': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'text': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems enabling higher accuracy resilience to spoofing and seamless integration with hardware-based solutions. At the same time smart card technologies have evolved to include on-chip biometric verification cryptographic processing and secure storage thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work 2019 2025 at the intersection of deep learning biometrics and smart card technologies for MFA. We analyze biometric modalities face fingerprint iris voice review hardware-based approaches smart cards NFC TPMs secure enclaves and highlight integration strategies for real-world applications such as digital banking healthcare IoT and critical infrastructure. Furthermore we discuss the major challenges that remain open including usability security tradeoffs adversarial attacks on deep learning models privacy concerns surrounding biometric data and the need for standardization in MFA deployment. By consolidating current advancements limitations and research opportunities this survey provides a roadmap for designing secure scalable and user-friendly authentication frameworks. In today s hyperconnected digital environment safeguarding user identity has become more critical than ever. With increasing reports of data breaches phishing scams and account hijacking traditional password-based authentication has proven insufficient. Passwords are not only prone to being forgotten or reused but are also vulnerable to brute-force attacks phishing and database leaks. These vulnerabilities have driven the widespread adoption of Multi-Factor Authentication MFA as a more robust alternative. MFA relies on the combination of independent identity factors something the user knows e.g. a password something they have e.g. a smart card and something they are e.g. a biometric trait. This layered approach significantly improves security by ensuring that compromising one factor does not grant access to protected systems. According to recent surveys modern digital services including financial platforms compliant with regulations such as PSD2 increasingly require MFA to mitigate risks associated with single-factor systems. Privacy-preserving MFA approaches leveraging deep learning and biometrics have also been proposed. 1 Biometric authentication including modalities such as face fingerprint iris and voice recognition has emerged as a popular inherence factor in MFA systems. Biometrics offer nontransferable user-specific traits improving usability and reducing reliance on memory. Yet they raise unique challenges in terms of spoofing privacy and demographic bias. Recent advances in deep learning DL have significantly improved the accuracy and reliability of biometric systems. Convolutional Neural Networks CNNs and other deep architectures have shown strong performance in extracting robust features from noisy or occluded biometric data enabling real-time and multimodal biometric authentication. DL techniques also power liveness detection domain adaptation and template security enhancing resilience against spoofing and adversarial attacks. In parallel possession-based factors have evolved. Smart cards Trusted Platform Modules TPMs and secure enclaves now provide cryptographic computation biometric match-on-card and tamper-resistant credential storage. Recent advances in biometric payment cards and NFCenabled devices illustrate how hardware tokens can securely integrate with DL-based biometric authentication to form compact and user-friendly MFA schemes. These solutions reduce fraud while ensuring compliance with data protection requirements. Despite this progress several open challenges remain ensuring usability without degrading security defending against presentation attacks preserving biometric privacy and ensuring interoperability across vendors and regulatory frameworks. Objectives. This survey provides a comprehensive overview of recent research 2019 2025 on DL-based MFA systems integrating biometrics and smart cards. Specifically it Reviews deep learning methods applied to biometric authentication within MFA frameworks. Examines smart card and hardware-based approaches for secure factor integration. Compares architectures fusion strategies datasets and benchmarks employed in state-ofthe-art systems. Analyzes threat models and countermeasures against adversarial and spoofing attacks. Identifies open research questions and outlines future directions for scalable privacypreserving and user-friendly authentication. 2 Background MFA Biometrics and Smart Cards Multi-Factor Authentication MFA is a security mechanism that requires users to verify their identity using a combination of independent factors typically categorized as knowledge something the user knows possession something the user has and inherence something the user is. The rationale behind MFA is that compromising multiple independent factors is significantly harder for attackers thus enhancing overall system security. Traditionally authentication relied heavily on passwords. However due to their susceptibility to guessing phishing and large-scale leaks passwords alone have become increasingly risky. In response MFA has been adopted across domains handling sensitive data particularly digital banking e-government and healthcare Io T. Biometric authentication represents the inherence factor leveraging physiological traits e.g. face fingerprint iris or behavioral patterns e.g. gait keystroke dynamics. Unlike passwords or tokens biometric traits are intrinsic to individuals and difficult to replicate. However they raise challenges such as privacy concerns irrevocability and vulnerability to spoofing attacks. To mitigate these risks standardization bodies e.g. ISO/IEC 30107 have proposed presentation attack detection PAD guidelines and researchers are increasingly focused on fairness and robustness across demographics. 2 Smart cards typically associated with the possession factor are tamper-resistant hardware devices capable of securely storing credentials cryptographic keys and even performing biometric matching. In the banking sector EMV-compliant cards enable secure offline authentication through digital signatures. When combined with biometrics smart cards can implement matchon-card verification ensuring that sensitive templates never leave the card s secure chip. Beyond traditional smart cards Trusted Platform Modules TPMs and Secure Enclaves extend these guarantees to general-purpose devices such as smartphones and laptops. These components isolate sensitive data and computations enabling secure biometric enrollment inference and key storage and underpin modern standards such as FIDO2 and Web Authn. In practice effective MFA requires balancing usability cost and risk. A typical modern system may combine a fingerprint scan inherence a smartphone secure enclave or biometric smart card possession and a PIN or behavioral pattern knowledge/behavior. The integration of deep learning into biometric systems coupled with trusted hardware marks the next evolution in MFA explored in detail in the following sections. Deep learning DL has fundamentally transformed biometric authentication by enabling endto-end learning robust feature extraction and scalability across diverse modalities. Traditional biometric systems relied on handcrafted features which often lacked generalizability across populations or environmental conditions. DL models particularly Convolutional Neural Networks CNNs Recurrent Neural Networks RNNs and Transformers now power state-of-the-art systems for face fingerprint iris voice and behavioral biometrics. 3.1 Facial Recognition and Anti-Spoofing Facial recognition has rapidly advanced with architectures such as Face Net Arc Face and Cos Face which learn highly discriminative embeddings from images. Wang and Deng survey modern DL-based face recognition. However face systems remain vulnerable to spoofing 2D photos replay videos 3D masks. Liveness detection networks address this by analyzing texture motion cues or physiological signals e.g. eye blinking r PPG. Guo et al. proposed CNN-based liveness detection while recent works integrate anti-deepfake detection. 3.2 Fingerprint and Iris Recognition DL enhances fingerprint authentication by improving minutiae detection ridge classification and partial print matching. Zahid et al. show CNNs outperform traditional Gabor-based methods under noisy conditions. Similarly iris recognition benefits from CNNs and attentionbased models trained on datasets such as CASIA-Iris and ND-Iris robust to illumination and pupil dilation. 3.3 Voice and Behavioral Biometrics DL also advances speaker verification via spectrogram-based CNNs and LSTM embeddings. Combining voice with face audiovisual biometrics strengthens robustness for remote banking authentication. Behavioral biometrics keystroke gait touchscreen mouse movement enable continuous MFA. Verma et al. demonstrate smartphone motion-based DL models for adaptive MFA. 3 algorithms. The synergy between biometric recognition and possession-based hardware is foundational to secure MFA. 4.1 Smart Cards in MFA Systems Smart cards have long been used in authentication due to their ability to securely store user credentials and perform local computations. In modern MFA biometric smart cards BSCs integrate fingerprint or facial recognition sensors directly into the card or terminal. There are two main architectures Match-on-Card Mo C Biometric matching is performed entirely on the card s chip and the template never leaves the card. This ensures maximum privacy. Match-off-Card The biometric is matched externally with the template read from the card. This mode is more flexible but less private. Mo C provides superior privacy and is increasingly supported by commercial products such as Idemia s biometric payment cards and Gemalto s biometric EMV solutions with technical standards maintained by EMVCo. 4.4 Smart Card + DL System Architectures Recent architectures combine deep learning with secure hardware to enhance biometric verification DL on-chip Miniaturized CNNs embedded in smart cards or tokens. Secure template fusion Combining multiple traits e.g. fingerprint + iris with fused templates stored in secure elements. On-device adaptation DL models fine-tuned per user during enrollment stored within TEEs. Tani et al. 2025 validated the feasibility of such architectures for real-world banking authentication. 4.5 Challenges in Hardware-Based MFA Despite their advantages hardware-integrated MFA systems face challenges Cost and Scalability Biometric smart cards are more expensive than traditional tokens. Hardware Standardization Fragmentation across vendors complicates integration. Energy Constraints DL inference on low-power chips requires lightweight models and quantization. Ongoing research explores energy-efficient DL models and secure co-processors to address these limitations. The integration of deep learning DL based biometric authentication with traditional MFA systems introduces several design options depending on modality fusion user experience UX requirements and environmental constraints. This section discusses core system-level integration strategies such as fusion techniques adaptive MFA policies and UX considerations like latency. 5.1 Fusion Techniques in DL-Based MFA To leverage the strengths of multiple authentication factors especially in multimodal biometrics systems often use fusion strategies to combine different sources of evidence. Fusion can occur at various levels Sensor-Level Fusion Raw biometric signals e.g. fingerprint + face are captured and preprocessed jointly. Feature-Level Fusion Deep embeddings from CNN/RNN models are concatenated before classification. Score-Level Fusion Independent DL models output match scores that are weighted and combined. Decision-Level Fusion Each modality votes independently a decision is made based on predefined logic e.g. majority voting. Score-level fusion offers a balance between flexibility and performance and is widely used in commercial systems. Decision-level fusion is preferred in scenarios with hardware heterogeneity or legacy compatibility. 7 6.5 Summary DL-based MFA improves identity assurance but still faces challenges in adversarial robustness deepfake resistance privacy and bias mitigation. The future of trustworthy MFA depends on standardized testing open datasets and secure hardware-software co-design. 6.6 Standardization and Interoperability DL-MFA systems often combine heterogeneous sensors inference engines and secure hardware. Without standards integration is fragile and error-prone. Key standards include FIDO2/Web Authn ISO/IEC 30107 PAD and EMVCo for biometric payment cards. Future work must emphasize pluggable frameworks formal verification of workflows and government-led certification e.g. e IDAS NIST 800-63. 6.7 Summary While DL-based MFA significantly strengthens digital identity protection its adoption introduces challenges in robustness privacy fairness and interoperability. Addressing these threats requires cross-disciplinary efforts spanning ML research hardware security regulatory compliance and usability studies. 7 Datasets Benchmarks and Metrics Robust evaluation of DL-based MFA systems requires standardized biometric datasets and consistent performance metrics. This section presents widely used benchmarks for facial fingerprint iris and multimodal biometrics and outlines key evaluation criteria such as FAR FRR and EER. 7.1 Benchmark Datasets for Biometric MFA Research in DL-powered biometric authentication relies on curated datasets that represent different modalities under diverse conditions. The quality and bias of these datasets significantly influence model generalizability. Well-established datasets underpin biometric research. Examples include LFW VGGFace2 Age DB CASIA-Iris V4 and FVC2004 which have become de facto benchmarks for DL-based evaluation. Many of these datasets include variations in lighting pose aging and acquisition devices to simulate real-world conditions. Public availability supports benchmarking and fair comparison across DL architectures. 7.2 Performance Metrics in MFA Systems Accurate evaluation of DL-based MFA systems requires standardized biometric metrics and protocols as defined in ISO/IEC 19795 and NIST SP 800-63B. These standards ensure comparability across algorithms datasets and deployment environments. False Acceptance Rate FAR Probability that an impostor is incorrectly accepted as a genuine user. FAR is critical for measuring system security and is often reported at operating points such as FAR 10 3 or 10 4. False Rejection Rate FRR Probability that a legitimate user is incorrectly rejected. FRR reflects system usability and user experience. Equal Error Rate (EER): The rate at which FAR and FRR are equal, often used as a single scalar indicator of performance. Lower EER implies a more accurate system. Receiver Operating Characteristic (ROC): A curve plotting the trade-off between FAR and True Positive Rate (1–FRR). Widely used to visualize model discriminability. Detection Error Tradeoff (DET): A log-scaled version of the ROC curve emphasizing low-error regions, recommended by ISO/IEC 19795 for biometric evaluations. Failure to Enroll (FTE) / Failure to Acquire (FTA): Rates at which biometric data cannot be captured or enrolled successfully, critical for deployment evaluation. Authentication Latency: The time required to complete an MFA process (biometric + token verification). Recommended latency for practical systems is below 1.5 seconds. In multimodal DL-MFA, researchers also evaluate: Fusion Gain: Improvement in EER or accuracy when combining multiple modalities compared to single-modality baselines. Robustness to Noise and Spoofing: Performance degradation under environmental noise, aging, or adversarial conditions. Template Security Impact: Trade-offs between encryption, privacy-preserving operations, and system accuracy. Public benchmarks such as FVC2004, CASIA-IrisV4, and AgeDB are typically evaluated using these metrics under standard protocols. As digital ecosystems expand and cyber threats evolve, MFA has become a central pillar of secure access control. This survey reviewed how deep learning, biometric modalities, and hardware tokens such as smart cards and secure enclaves can converge to build the next generation of MFA systems. Improvements include accuracy, liveness detection, and multimodal fusion, while challenges include robustness, privacy, fairness, and interoperability.'}, {'rank': 4, 'score': 4.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 5, 'score': 4.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}]",Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos.
What are the advantages and drawbacks of batch normalization compared to layer normalization?,2510.13137v1,,,"['2510.13137v1', '2510.11073v1', '2510.08662v1', '2509.23158v1', '2510.12758v1']","[11.0, 11.0, 10.0, 9.0, 9.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention']","[{'rank': 1, 'score': 11.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 11.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 3, 'score': 10.0, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops. With the continuous growth of the global population food security has become a critical challenge in the global agricultural sector. In this context enhancing the efficiency and precision of crop breeding is of paramount importance. Genomic Selection GS an advanced breeding methodology leverages whole-genome information to predict crop phenotypes significantly accelerating the breeding process. However traditional GS approaches still face limitations in prediction accuracy particularly when handling large-scale datasets nonlinear genetic effects and complex trait architectures along with a heavy reliance on environmental data. To overcome these limitations we developed Deep Pheno Correlation Former DPCformer a novel deep learning model that integrates convolutional neural networks CNN with a self-attention mechanism to effectively model the intricate nonlinear relationships between genotype and phenotype. We applied this model to 13 traits across five major crops maize cotton tomato rice and chickpea implementing a feature engineering strategy that involved 8-dimensional one-hot encoding of SNP data ordered by chromosomal position followed by feature selection via the PMF algorithm. This approach substantially enhanced the predictive accuracy and stability of the model. Model evaluation revealed that DPCformer demonstrated exceptional performance across diverse crop datasets. In the maize dataset in Henan Province the prediction accuracies for the three traits of days to tasseling DTT plant height PH and ear weight EW are improved by 2.92% 0.74% and 1.10% respectively compared to the second-best method In the Beijing dataset these accuracies were enhanced by 1.48% 2.40% and 1.01% relative to the top-performing baseline models.In the cotton dataset the accuracies for the four traits of Fiber Elongation FE Fiber Length FL Fiber Strength FS and Fiber Microstructure FM increased by as much as 8.37% relative to baseline models.On the small-sample tomato dataset the Pearson Correlation Coefficient PCC for a key trait was boosted by up to 57.35% compared to baseline models. Similarly in the chickpea dataset the PCC for yield per plant was elevated by up to 16.62% relative to comparable models. Collectively these results indicate that DPCformer outperforms existing genomic selection methods in terms of prediction accuracy smallbatch prediction capability polyploid genome processing and interpretability. Against the backdrop of global food security challenges our innovative framework offers a powerful tool for advancing precision breeding. Fig. 1 A The workflow of DPCformer in crop genomic prediction from SNPs. B The limitations of existing methods. security. Against this backdrop breakthroughs in crop breeding technologies have become indispensable. Although traditional breeding methods have advanced crop yield and stress resistance they are constrained by long breeding cycles low efficiency and limited adaptability to rapidly changing environmental and climatic conditions. Consequently precise and efficient crop phenotypic prediction has emerged as a pivotal research focus in modern agriculture. Genomic selection GS has emerged as a powerful paradigm to address these limitations. This approach utilizes genome-wide marker data to build predictive models enabling the estimation of breeding values independent of extensive phenotypic tests thereby accelerating the breeding process. However despite its success the efficacy of GS is often hindered by several challenges including the analysis of highdimensional data the modeling of non-linear relationships and a dependency on large sample sizes. Furthermore I. By 2050 the global population is projected to reach approximately 9 billion posing unprecedented challenges to food conventional GS models often inadequately capture complex non-additive genetic effects which limits their prediction accuracy and robustness. Recently deep learning methods have demonstrated remarkable efficacy in data modeling across diverse scientific domains. Their capacity to automatically learn complex features enables the effective modeling of non-linear relationships between genotype and phenotype rendering them highly suitable for genomic prediction. Genomic prediction models employing Deep Neural Networks DNNs and Convolutional Neural Networks CNNs have shown promising results in crop breeding applications. For instance DNNGP leverages automatic feature extraction to enhance the analysis of high-dimensional genomic data. Similarly machine learning models such as the gradient boosting framework Crop GBM have proven effective for handling large-scale datasets. Additionally the GEFormer model incorporates genotypeenvironment interactions by utilizing both environmental and genetic data for phenotypic prediction. Furthermore Cropformer has demonstrated notable success in predicting maize heterosis by integrating CNNs with self-attention mechanisms. However significant challenges for existing deep learning models remain including limitations related to insufficient environmental data suboptimal prediction accuracy poor performance with small sample sizes and difficulties in processing polyploid genomic data. To address the limitations of traditional genomic selection this study introduces Deep Pheno Correlation Former DPCformer a novel deep learning framework Figure 1 that integrates a convolutional neural network CNN with a multihead self-attention mechanism to predict crop phenotypes from single nucleotide polymorphism SNP data. The model s data processing pipeline employs an innovative 8-dimensional encoding strategy feature selection via the Probabilistic Matrix Factorization PMF algorithm and chromosome-positionbased sorting to enhance the precision and stability of SNP data. Comprehensive evaluations demonstrate that DPCformer surpasses contemporary deep learning models in prediction accuracy model interpretability and performance robustness particularly in small-sample contexts thereby establishing a new technical framework for crop phenotypic prediction. pruned for linkage disequilibrium LD using PLINK with a window size of 1 kb a step size of 100 SNPs and an r2 threshold of 0.1 resulting in a final set of 32 519 SNPs. To ensure data quality samples with missing values were initially removed yielding a final analytical cohort of 5 816 maize samples. This entire cohort was then partitioned into a training-validation set n 5 235 and a test set n 581 at a 9 1 ratio. ly a ten-fold cross-validation CV procedure was implemented on the training-validation set to determine the optimal model configuration. In each fold this set was partitioned into training 80% and validation 20% subsets and model performance was averaged across all folds to guide final model selection. 2 A tomato dataset publicly available at http // solomics.agis.org.cn/tomato/ftp/ was also analyzed to evaluate the model s generalizability. We employed a similar methodology to analyze the tomato dataset focusing on the prediction of key traits related to yield and flavor specifically the trait Sopim BGV006775 12T001232. Following preprocessing the final dataset consisted of 332 samples retained for subsequent analysis. 3 A rice dataset publicly sourced from the Rice Var Map database https //ricevarmap.ncpgr.cn/ was utilized for the prediction of the plant height phenotype. 4 To address the unique challenge of allopolyploidy we utilized a cotton dataset publicly available at https //iagr.genomics.cn/Crop GS/ which included 1 245 samples for the prediction of four key fiber quality traits Fiber Elongation FE Fiber Length FL Fiber Strength FS and Fiber Microstructure FM. The commonly cultivated cotton species Gossypium hirsutum upland cotton is an allotetraploid where each trait is codetermined by two subgenomes. Within its genome the A and D subgenomes are co-expressed and exhibit homoeologous relationships between corresponding chromosome pairs e.g. A1-D1 A2-D2... A13-D13. To account for this genomic architecture DPCformer was specifically designed to pair the 13 chromosomes of the A-subgenome with their corresponding homoeologs in the D-subgenome thereby modeling potential synergistic effects. This unique processing strategy preserves the subgenomic differentiation characteristics of the allotetraploid enabling more precise dissection of the cooperative mechanisms between homologous chromosome pairs across the A and D subgenomes. Consequently this approach enhances prediction accuracy and biological interpretability by creating a framework that integrates structural genomic information with functional genetic interactions offering a robust solution for complex trait prediction in allopolyploid species. 5 The chickpea dataset sourced from https //iagr.genomics. cn/Crop GS/ was analyzed to assess the model s performance on an additional legume species. The model s predictive capabilities were tested on four key agronomic II. A. Datasets To comprehensively validate the model s performance this study utilized multi-species multi-scale datasets representing diverse reproductive systems and genetic backgrounds 1 The maize dataset https //ftp.cngb.org/pub/CNSA/ data3/CNP0001565/zeamap/99 Maizego Resources/01 CUBIC related/ utilized in this study comprises 1 428 inbred lines derived from 24 foundational female parents which were crossed to produce 8 652 F1 hybrids. Phenotypic data for three traits days to tasseling DTT plant height PH and ear weight EW were collected from five distinct locations. The genotypic data were Fig. 2 The Cropformer model mainly consists of a CNN layer and a multi-head self-attention layer. The CNN layer is used to capture the localization signals of SNPs while multi-head self-attention makes the model more focused on important SNPs. traits plant height plant width hundred-seed weight and yield per plant. provided in the MAP file. This process preserves the spatial contiguity and relative ordering of SNPs within each chromosome. Treating each chromosome as an independent sequence provides a structured basis for subsequent feature selection and modeling of inter-chromosomal interactions. 3 MIC-based Feature Selection To manage the high dimensionality of the discrete SNP features the maximum information coefficient MIC was employed to select the top k 1 000 most informative SNP loci from each chromosome. For a given discrete SNP feature S and a continuous phenotype vector Y MIC quantifies the strength of their association by systematically exploring various grids of partitions on their joint distribution. It is defined as B. Data Preprocessing This section details the generation process of the chromosome-level feature tensors that serve as input for our model. The pipeline is engineered to convert raw single nucleotide polymorphism SNP data into a structured format that retains critical biological information. 1 8-Dimensional SNP Encoding Conventional genomic prediction methods frequently rely on one-dimensional 1D ordinal encoding an approach that can create spurious numerical relationships between alleles and fails to preserve information on allelic order e.g. by commutatively mapping heterozygous genotypes such as AT and TA to the same integer value. To circumvent these limitations this study adopts an eight-dimensional one-hot encoding scheme. In this scheme each allele from the set A A T C G is first mapped to a unique four-dimensional 4D one-hot vector a diploid genotype is then represented by concatenating the two corresponding allelic vectors resulting in a final eightdimensional 8D feature vector. Let the one-hot encoding function for a single allele be f A 0 1 4. A genotype S a1 a2 where a1 a2 A is transformed by the encoding function φ MIC S Y max GS GY B n I S Y GS GY log min GS GY 2 where I S Y GS GY is the mutual information maximized over all grids GS GY of size GS × GY and B n is a function of the sample size n. A key advantage of this method is its robust ability to identify SNPs exhibiting strong potentially non-linear associations with the phenotype. 4 Sorting by Physical Position The MIC-based selection process ranks SNPs by their phenotypic association strength a procedure that disrupts their native physical order on the chromosome. Within each chromosome the selected SNPs are reordered according to their physical coordinates from the MAP file. This reordering is crucial as it ensures that the sequence dimension of the model s input tensor faithfully represents the physical arrangement of SNPs along the chromosome thereby allowing the convolutional and selfattention layers to effectively capture local and long-range spatial dependencies. 5 Uniform Length Padding To facilitate batch processing and conform to the network s fixed input dimensionality the φ S f a1 f a2 0 1 8 1 where denotes concatenation. This representation preserves the positional order of alleles and ensures that distinct diploid genotypes are equidistant in the feature space a crucial property that enhances its suitability for attention-based models. 2 Chromosome Segmentation via MAP File The encoded SNP sequence is partitioned into chromosome-specific subsequences based on the identifiers and physical coordinates where Q Query K Key and V Value are vectors derived from the input sequence via linear transformations and dk is the dimension of the key vectors. This mechanism allows the model to globally assess the relational importance of all feature pairs effectively weighing contributions from different chromosomal regions. The attention output is subsequently passed through Layer Normalization and a position-wise FeedForward Network FFN which enhances the model s representational capacity and stabilizes the training process. 3 Phenotype Prediction and Loss Function The features refined by the Transformer are flattened and passed through a Multi-Layer Perceptron MLP for regression yielding the final predicted phenotype ˆy. SNP sequence for each chromosome is standardized to a uniform length of L 1 000 via zero-padding. 6 Specialized Processing for Tetraploid Cotton To model the complex interactions between the homoeologous A and D subgenomes of tetraploid cotton Gossypium hirsutum a specialized data processing workflow was implemented. This workflow commences by pairing homoeologous chromosomes e.g. A1-D1 into distinct groups while any unpaired chromosomes are treated as individual units. Each chromosome undergoes initial feature selection via MIC. The resulting feature tensors for each homologous pair are then concatenated along the sequence dimension Xpair i XAi XDi 3 ˆy MLP Flatten Transformer Ecombined 6 Subsequently a second round of MIC selection is applied to this concatenated tensor to identify the most informative SNPs that capture inter-subgenomic associations. As a final preprocessing step all resulting chromosome group tensors are padded to a uniform length Lmax to ensure a consistent input shape for the network. This hierarchical approach effectively captures both intraand inter-subgenomic interactions. The model parameters are optimized by minimizing the Mean Squared Error MSE loss function defined as B X LMSE 1 i 1 yi ˆyi 2 7 B where yi represents the ground-truth phenotype ˆyi is the corresponding predicted value and B denotes the batch size. The model was trained using the Adam optimizer supplemented with a learning rate scheduling and an early stopping mechanism. A 10-fold cross-validation protocol was adopted for robust training and evaluation. C. Model Architecture This paper introduces DPCformer a hybrid deep learning model that synergistically integrates a Residual Convolutional Network Res-CNN and a Multi-Head SelfAttention MHSA mechanism for phenotype prediction from SNP sequences. The model architecture comprises three core modules chromosome-level feature extraction crosschromosome information fusion and final phenotype prediction.The overview of our model is shown in Figure 2 1 Chromosome-level Feature Extraction Res-CNN To capture local dependencies among SNPs a dedicated Residual Convolutional Network Res-CNN comprising a stack of Residual Convolutional Blocks Res Conv Blocks is independently applied to the input sequence Xj of each chromosome j. Let Zin be the input to a Res Conv Block. Its core operation can be summarized as D. Model Implementation The DPCformer model was implemented using the Py Torch deep learning framework. The Mean Squared Error MSE loss function was utilized to quantify the discrepancy between the ground-truth labels and predicted values and the Adam optimizer was employed for parameter optimization. During training we integrated a learning rate scheduling strategy Reduce LROn Plateau and an early stopping mechanism Early Stopping. The learning rate is decayed by a factor of 0.1 if the validation loss does not improve for 10 epochs and training is terminated if there is no improvement in validation loss for 20 epochs. A 10-fold cross-validation protocol was adopted to ensure a robust evaluation of model performance. The final reported results are presented as the mean and standard deviation calculated across all folds. Zout Max Pool Re LU BN F Zin + Conv1×1 Zin 4 where F represents the main convolutional path consisting of two 1D convolutional layers Conv1D and Re LU activations. The Conv1×1 term denotes a shortcut connection for dimensionality matching and BN is Batch Normalization. This module transforms the raw sequence Xj of each chromosome into a high-level feature map Ej. 2 Cross-Chromosome Information Fusion To model long-range dependencies and potential epistatic effects between different chromosomes the feature maps from all chromosomes Ej Nchr j 1 are concatenated along the sequence dimension to form a unified feature sequence Ecombined. This concatenated sequence serves as the input to a Transformer encoder layer. The core of this layer is the Multi-Head SelfAttention mechanism which is computed as III. RESULTS AND DISCUSSION A. Performance comparison and analysis To evaluate the performance of our proposed model we investigated the application of DPCformer on five different datasets using the PCC of the test set as the prediction performance evaluation metric. The results were compared with other GS methods including DNNGP Light GBM Cropformer and GEFormer. As shown in Figure 3 DPCformer achieved excellent performance across all datasets. QK 1 Maize Dataset Performance DPCformer demonstrates superior performance across dk Attention Q K V softmax V 5 of 73.49 % for the Sopim BGV006775 12T001232 trait. This represents significant improvements of 11.45 % over DNNGP 9.13 % above Light GBM 57.35 % beyond Cropformer and 41.02 % superior to GEFormer Fig. 3b confirming its robustness in small-sample scenarios. 5 Chickpea Dataset DPCformer demonstrated robust predictive performance across all four traits plant width 62.12% seed weight 56.87% plant height 65.96% and yield per plant 65.96% Fig. 3c. Compared to the second-best model our approach showed performance improvements ranging from 4.63% to 15.63% across these traits. These results substantiate that DPCformer provides more effective genomic prediction capabilities for rice than Cropformer DNNGP Light GBM and GEFormer models. a b B. Model ablation To quantitatively assess the contribution of each key component of our proposed model a comprehensive ablation study was conducted. This study evaluated various configurations by systematically including or excluding three architectural cornerstones 1 8-dimensional SNP encoding 2 physical position-based sorting and 3 probability matrix factorization PMF. As systematically documented in Table I each module was incrementally enabled while monitoring performance variations in Pearson correlation coefficient PCC. The experimental results reveal several key insights 1 The 8-dimensional SNP encoding module demonstrates the most significant individual impact elevating PCC from the baseline of 0.8376 to 0.9076 a relative improvement of 8.36%. This substantial enhancement confirms the module s efficacy in capturing stereochemical properties of genetic variants. 2 Although the physical position sorting module alone provides moderate gains PCC 0.8668 +2.92% its integration with PMF achieves PCC 0.8895 exceeding their individual performances 0.8668 and 0.8816 respectively. This evidences optimized utilization of chromosomal spatial information. 3 The complete integration of all three components achieves state-of-the-art performance PCC 0.916 surpassing the strongest dual-module configuration 8D+PMF 0.8895 by 2.97% and the baseline by 9.36%. These findings conclusively establish the complementary nature of the proposed modules with the integrated framework delivering a statistically significant improvement p 0.001 via paired t-test over all partial configurations. c Fig. 3 Prediction accuracy of methods built using five different models on five datasets. all three traits in five geographical regions. Exemplified by Henan Province it achieves prediction accuracies of 89.12% DTT 68.93% PH and 91.41% EW outperforming the second-best methods with improvements of 2.92% against Light GBM 0.74% over Cropformer and 1.10% beyond GEFormer Fig. 3a. Similarly in Beijing DPCformer reaches 93.50% DTT 76.24% PH and 93.01% EW exceeding GEFormer s DTT and PH by 1.48% and 2.40% respectively while surpassing Cropformer s EW by 19.25%. 2 Rice Small-Sample Dataset In resource-limited samples DPCformer demonstrated exceptional capability. For the rice dataset comprising only 530 samples it achieved a PCC of 84.45% for the plant height trait surpassing GEFormer by 38.98% DNNGP by 7.94% Cropformer by 3.56% and LightGBM by 5.53% Fig. 3a. 3 Cotton Dataset DPCformer significantly outperforms baseline models with accuracy rates of 74.19% FE 71.45% FL 74.85% FS and 72.30% FM surpassing Cropformer 65.97% Light GBM 71.03% Cropformer 71.43% and DNNGP 63.93% by 8.22% 0.42% 3.42% and 8.37% respectively Figure 3b. 4 Tomato Small-Sample Dataset With only 332 accessions DPCformer attained a PCC C. Interpretability Analysis of Machine Learning Models Using SHAP Values To interpret the model s predictions SHAP values were calculated for the SNPs in the best-performing model to identify the most influential loci. These top-ranking SNPs were then mapped to their corresponding genes.This analysis revealed several candidate genes associated with TABLE I RESULTS OF ABLATION EXPERIMENTS 8-Dim Encoding Position Sort PMF PCC 0.8376 0.8816 0.8668 0.9076 0.8463 0.8833 0.8895 0.916 activated component PCC Pearson Correlation Coefficient plant height PH in maize including Zm00001d050247 Zm00001d009706 and Zm00001d009705 Figure 4. Notably the top-ranked gene Zm00001d050247 encodes a WRKY transcription factor. This finding aligns with previous research as WRKY family transcription factors are well-established regulators of plant height. Regarding the ear weight trait in maize our analysis identified Zm00001d015381 Zm00001d013707 and Zm00001d035249 as prominent candidate genes Figure 5. Among these the gene Zm00001d015381 which encodes the MADS-box transcription factor Zm MADS17 is particularly noteworthy. This gene family is a known regulator of floral organ development a process fundamentally linked to maize ear weight. Zm00001d035249 regulates the HXXXD-type acyl-transferase family protein. Furthermore the identification of Zm00001d035249 a gene encoding an HXXXD-type acyltransferase is consistent with existing literature. Previous genome-wide association studies GWAS have established a strong correlation between lipid metabolism and agronomic traits suggesting that lipid-related genes can influence grain weight by modulating the plant s metabolic network. Fig. 5 Top 20 significant SNPs obtained after calculating SHAP values based on the ear weight EW trait in maize. IV. This study introduces DPCformer a novel deep learning architecture that synergistically integrates convolutional neural networks CNN with multi-head self-attention mechanisms for crop trait prediction based on single nucleotide polymorphisms SNPs. The model s efficacy was validated through comprehensive evaluations on 16 traits across five economically important crops where it consistently outperformed state-of-the-art methods. Compared with these methods DPCformer exhibits the following advantages i Novel Encoding Strategy The model employs an eight-dimensional one-hot encoding scheme that preserves equidistant coding relationships among SNPs enabling the capture of richer genetic variation patterns while maintaining biological interpretability. ii Restoration of Spatial Information Following an initial importance-based screening SNPs are reordered according to their native physical coordinates preserving the genomic architecture to enable the effective capture of spatial dependencies. iii Enhanced Feature Selection While utilizing the Maximum Information Coefficient MIC for SNP feature selection we integrated a Probability Mass Function PMF based approach specifically designed for discrete genetic data which reduces stochasticity and improves feature selection robustness. iv Polyploid-Specific Architecture The framework incorporates a specialized module for allotetraploid species e.g. cotton wherein homoeologous chromosome pairs are processed jointly to model inter-subgenomic interactions establishing a novel deep learning methodology for prediction in complex polyploids. Despite its promising results the study has several limitations that inform future directions. On one hand the pairing strategy for homoeologous chromosomes is based exclusively on physical coordinates without incorporating functional genomics data e.g. gene co-expression networks 3D chromatin conformation to elucidate more complex synergistic effects. On the other hand while DPCformer has demonstrated superior performance in handling small-sample datasets compared to alternative approaches the inherent limitations of sample size still constrain the full potential of deep learning Fig. 4 Top 20 key genes screened based on the plant height PH trait in maize. applications.Future work will focus on addressing these limitations primarily through the integration of multi-modal functional genomics data and the optimization of the selfattention mechanism for computational efficiency. In subsequent research when dealing with heterologous chromosomes better prediction performance can be achieved by developing a hierarchical attention mechanism to distinguish the contribution degrees of sub-genomes A/D and homologous chromosome pairs.'}, {'rank': 4, 'score': 9.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}, {'rank': 5, 'score': 9.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}]",Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity.
What regularization techniques help reduce overfitting in large language models?,2510.13137v1,,,"['2510.13137v1', '2510.12850v1', '2510.11073v1', '2510.14855v1', '2509.20913v1']","[8.0, 8.0, 7.0, 7.0, 6.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales']","[{'rank': 1, 'score': 8.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 8.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 3, 'score': 7.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 4, 'score': 7.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 5, 'score': 6.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}]","Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions."
Which learning rate schedules are most effective when training deep learning models?,2510.14855v1,,,"['2510.14855v1', '2510.13937v1', '2510.13137v1', '2510.13050v1', '2510.12758v1']","[9.0, 9.0, 9.0, 9.0, 9.0]","['A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention']","[{'rank': 1, 'score': 9.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 2, 'score': 9.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 3, 'score': 9.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 4, 'score': 9.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}, {'rank': 5, 'score': 9.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}]",ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations.
What problems might I encounter when fine-tuning models on domain-specific data?,2510.13137v1,,,"['2510.13137v1', '2509.20913v1', '2510.08116v1', '2510.12850v1', '2510.08411v1']","[6.0, 6.0, 6.0, 6.0, 5.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning']","[{'rank': 1, 'score': 6.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 6.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 6.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}, {'rank': 4, 'score': 6.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 5, 'score': 5.0, 'id': '2510.08411v1', 'title': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning', 'text': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning. Spectroscopy represents the ideal observational method to maximally extract information from galaxies regarding their star formation and chemical enrichment histories. However absorption spectra of galaxies prove rather challenging at high redshift or in low mass galaxies due to the need to spread the photons into a relatively large set of spectral bins. For this reason the data from many state-of-the-art spectroscopic surveys suffer from low signal-to-noise S/N ratios and prevent accurate estimates of the stellar population parameters. In this paper we tackle the issue of denoising an ensemble by the use of unsupervised Deep Learning techniques trained on a homogeneous sample of spectra over a wide range of S/N. These methods reconstruct spectra at a higher S/N and allow us to investigate the potential for Deep Learning to faithfully reproduce spectra from incomplete data. Our methodology is tested on three key line strengths and is compared with synthetic data to assess retrieval biases. The results suggest a standard Autoencoder as a very powerful method that does not introduce systematics in the reconstruction. We also note in this work how careful the analysis needs to be as other methods can on a quick check produce spectra that appear noiseless but are in fact strongly biased towards a simple overfitting of the noisy input. Denoising methods with minimal bias will maximise the quality of ongoing and future spectral surveys such as DESI WEAVE or WAVES. In this regard it is not uncommon to need values of S/N per resolution element above 20-30 if not higher for detailed analyses of subtle differences in the populations such as variations of chemical abundances or the initial mass function e.g. La Barbera et al. 2013 2017 Ferreras et al. 2019. In this regard spectroscopic surveys tend to be optimised to produce large volumes of data at the cost of a lower S/N and so any algorithm aimed at increasing the S/N of the data proves a very valuable tool especially with the ongoing and upcoming surveys such as DESI DESI Collaboration et al. 2025 WEAVE Jin et al. 2024 or WAVES Driver et al. 2019. Galaxy spectra in the wavelength interval from near ultraviolet to near infrared encode a vast amount of information regarding the properties of the underlying stellar populations. The continuum and absorption lines of the photospheres of constituent stars leave their imprint on the integrated spectra and represent the workhorse of galaxy formation studies concerning the star formation and chemical enrichment histories. Spectroscopic surveys of galaxies such as the Sloan Digital Sky Survey York et al. 2000 have greatly helped deepen our understanding of galaxy formation. However spectroscopy requires large integration times as the faint light from galaxies is spread with respect to wavelength. Often times spectra have been used mainly by cosmologists as a tool to derive redshift and thus determine the largescale distribution of galaxies. However exploring the galaxies themselves through their stellar populations requires deeper data at a higher signal-to-noise S/N ratio in order to compare the observations with detailed models of stellar popuThe actual S/N of an observation is borne out of the competition between the photons from the galaxy and spurious photons or counts coming from unwanted sources such as the background sky airglow detectors and reduction artifacts etc. Increasing the S/N of a single spectrum is typically not feasible unless models are used therefore introducing large systematics. Our approach to this problem starts from a large ensemble of spectra taken by the same instrument and following the same reduction process. The ensemble should also include a large amount of high quality i.e. high S/N data E-mail oc00149 surrey.ac.uk Corresponding author ignacio.ferreras iac.es 2025 The Authors 2 Camilleri et al. so that the method can somehow interpolate among the ensemble members to produce optimised versions of the data. Traditional data driven methods such as Principal Component Analysis have been applied to stellar and galaxy spectra for instance to remove the emission lines from airglow Wild Hewett 2005. These methods are commonly used for classification purposes Madgwick et al. 2003 Mc Gurk et al. 2010 and can also help in the interpretability of the information content for instance by exploring the resultant latent space e.g. Sharbaf et al. 2023 2025. However as a linear method PCA is less versatile to encode and model the many intricacies of the spectra in a large ensemble. is presented in 5. Finally our concluding remarks are given in 6. spectra of the Sloan Digital Sky Survey York et al. 2000. The data include a large number of spectra of order 1 million and most importantly covers a wide range of S/N with a large number of high quality data at a S/N higher than 10-20 and many spectra at lower S/N. This represents an ideal training sample as the data processing is homogeneously performed minimising biases and covers all types of evolutionary stages of galaxies mass morphology etc. Moreover each observation includes in addition to the actual spectrum a best fit model that can be adopted as a synthetic case to which noise is added as we will show below. The sample is the same as the one presented in Sharbaf et al. 2023 and the motivation behind the constraints regarding the quality of the data as an ensemble is identical. For instance in order to set this exercise in the best defined scenario we include a constraint in stellar velocity dispersion between 100 and 150 km s 1 as a wider range will also introduce the expected bias in effective spectral resolution caused by the kinematic kernel. The data are taken from SDSS Data Release 16 Ahumada et al. 2020 and correspond to single fibre measurements of the central parts of galaxies 3 arcsec diameter at spectral resolution R 2 000 Smee et al. 2013. The targets are selected as completely as possible down to a Petrosian flux level for the target galaxy in the SDSS-r band of r 17.77 AB Strauss et al. 2002. The data were further constrained in redshift z 0.05 0.1 and in S/N measured as an average within the SDSS-r band higher than 15 per pixel log λ/A 10 4. The total sample comprises 68 794 spectra which were retrieved from the main SDSS database de-redshifted and de-reddened regarding foreground dust absorption from the Milky Way following a standard Fitzpatrick 1999 attenuation law. To remove the variations caused by different stellar mass and redshift of the galaxies all spectra are normalized to the same average flux in 6 000-6 500A window in the rest-frame. The spectral range is restricted to the rest-frame wavelength λ A. Finally for one of the tests we explore the denoising procedure when training on data without continuum. For that purpose we take the robust high percentile method of Rogers et al. 2010 to define the continuum that is removed from each spectra for this test case. For more details about the sample please see Sharbaf et al. 2023. Deep Learning DL methods are seeing a rapid uptake in use throughout astronomy helping to address problems that traditional data-driven approaches struggle with. For example DL has been applied to galaxy and stellar spectra for classification purposes e.g. Folkes et al. 1996 Fabbro et al. 2018 Wu et al. 2024 dimensionality reduction e.g. Portillo et al. 2020 recovery of spectra with bad quality e.g. Wang et al. 2017 general analysis e.g. Lovell et al. 2019 Melchior et al. 2023 or in the search for anomalies e.g. Baron Poznanski 2017 Liang et al. 2023. In this paper we adopt a set of DL algorithms each trained on a large set of galaxy spectra from the Legacy part of SDSS and then assess their ability to increase the S/N of input spectra. These models are unsupervised and in contrast to works like Scourfield et al. 2023 do not rely on adding synthetic noise to training data. Instead the denoising effects seen are emergent ultimately stemming from information bottlenecks and aided by the formulation of the objective function. Furthermore we experiment with the reconstruction of spectra from incomplete data and attempt to explain deep model decision making. Please note it is important to ensure that the process does not alter the sample in a systematic way. We also emphasise that this method is not a smoothing process where the S/N can be increased at the cost of a lower spectral resolution. A similar type of work has been recently presented in Scourfield et al. 2023 mainly focussed on retrieval of emission line data. The authors conclude that a Variational Autoencoder performs better than PCA and study the effect of denoising DESI data from an SDSS-trained set regarding the relationship between stellar mass and gas phase metallicity. Melchior et al. 2023 also consider the use of autoencoders to analyse galaxy spectra and look into the interpretability of latent space with results mostly focused on emission lines which is where the spectral variance fully dominates in starforming and AGN systems. The authors indeed suggest that these methods can be adopted to denoise ensembles of spectra. This work complements these previous studies turning to the more challenging case of the absorption line spectra that is commonly used to constrain the stellar population content and the past star formation history. We also consider the issue of potential biasing of the denoised data by use of synthetic spectra that are adopted as ground truth to assess the fidelity of the recovered measurements. 3.1 Butterworth Filtering The structure of the paper is as follows we give a brief presentation of the working sample in 2 along with a description of the various methods tested for denoising in 3. The comparison of retrieved and original data is shown in 4 including a comparison of synthetic data with added noise. A brief discussion of the explainability of the DL performance The Butterworth filter BF is a classical signal processing approach used to selectively attenuate unwanted frequencies within a general signal. Its maximally flat frequency response in the passband supresses signal distortion making it a popular choice in a range of domains. The squared magnitude of the frequency response H ω 2 at angular frequency ω is MNRAS 000 1 9 2025 Emergent Denoising of SDSS Galaxy Spectra 3 4 Camilleri et al. SDSS Synthetic Worthey Ottaviani 1997 and the traditional Mgb index of the Lick system Trager et al. 1998. The figure of merit is defined for each line strength measurement as follows we produce a vector with the residuals of the output and the reference for each spectrum δs Ii Ii s Ii r where s represents the output spectrum and r the reference spectrum. The mean or median of the ensemble δs I are expected to be close to zero and the standard deviation represents how well the data are recovered. Therefore we adopt the standard deviation of the residuals as our figure of merit 0.0 0.2 0.4 0.6 0.8 S/N 5 0.0 0.2 0.4 0.6 0.8 Dn 4000 NW CS FS BF NW-S NW CS FS BF NW-S H F O N 1 2 3 S/N 5 1 2 3 I p δ2 I δ I 2. 5 Also note that the reference case Ii r is the comparison spectra that can be defined in two ways it is either the noiseless best fit data O or the noisy original SDSS data N. The former allows us to quantify the denoising process whereas the latter is used to test overfitting. Fig. 2 shows the residual statistic measured at a S/N 5 as an average over all spectra with S/N for the reconstruction of an additional unseen set of real SDSS spectra left or a set of synthetic spectra right. In the case with real data we only show the data points corresponding to O as N would trivially compare the noisy data with itself. For the synthetic case we can compare the residual statistic for the ground truth case O and for a noisy realization of this ideal case with Gaussian noise that shows the same S/N as the original data N. In this framework the case N O would be indicative of a kind of overfitting. The opposite would be suggestive of true denoising. For reference the value of O for the comparison of noiseless and noisy synthetic data i.e. the variance expected by the presence of noise in the spectra is shown in each case as a horizontal dashed blue line. The performance of the different methods is comparable with the SDSS data reconstruction left panels although the BF method appears to fare worse. The more interesting results are found for the synthetic data right panels where we can discriminate between the recovery of the input noisy data N blue stars or a more desirable reconstruction of the original noiseless spectra. O red circles. One thing that stands out quite clearly is that the 4000A break strength is poorly determined by the CS method. This may be quite expected as the Dn 4000 is wider than the other two and relies on the continuum. However we performed this test as there is a well-known degeneracy between parameters so that for instance Dn 4000 indices are correlated with e.g Mgb. The strong covariance between line strengths found in Ferreras et al. 2023 indicates that the absorption line spectrum would encode similar information as the continuum. This experiment suggests that discovering and leveraging this trend is challenging for DL methods. The figure also shows an uncanny inversion of the star circle order i.e. N vs O in the BF method with respect to the other algorithms. We emphasize that this method does not use information from the ensemble and only relies on a careful filtering of high frequencies many of which would be ascribed to noise. This would be equivalent to a truncation in a Fourier series. The results presented here reveal that the BF method tends to overfit so that it produces an optimal reconstruction when using noisy data as input but underperforms when the noiseless synthetic data are considered. NW CS FS BF NW-S NW CS FS BF NW-S 0.5 1.0 1.5 2.0 2.5 3.0 S/N 5 0.5 1.0 1.5 2.0 2.5 3.0 Mgb NW CS FS BF NW-S NW CS FS BF NW-S Emergent Denoising of SDSS Galaxy Spectra 5 Dn 4000 H F Dn 4000 H F Mgb Mgb 0.0 0.1 0.2 0.3 0.4 O Noise NW CS FS BF NW-S Noise NW CS FS BF NW-S 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 O 0.0 0.4 0.8 1.2 1.25 1.50 1.75 1 2 2 3 4 10 20 10 20 10 20 0.0 0.1 0.2 0.3 0.4 N 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 N 0.0 0.4 0.8 1.2 1.25 1.50 1.75 Dn 4000 1 2 3 10 20 S/N g-band 10 20 S/N g-band 10 20 S/N g-band 6 Camilleri et al. 0.8 The other two indices show a more characteristic behaviour between the BF overfitting algorithm and the DL method. Note that both HδF and Mgb produce distributions closer to the ground truth with FS whereas BF closely resembles the wider shape of the histogram of noisy data. From these tests we conclude that a DL method is successful at improving the quality of galaxy spectra if a representative large ensemble with a wide range of S/N including high quality data with the same instrumental / data characteristics are available. Deep models are often considered black boxes because their mappings and decision-making processes are difficult to interpret. To better understand how spectral features are being leveraged, we employ SHAP (SHapley Additive exPlanations, Lundberg et al. 2020). SHAP is a game-theoretic method that explains model outputs by assigning each feature an importance value based on its contribution to the gap between the model’s actual prediction and its mean prediction, averaged over all possible feature subsets. Figure 7 shows the mean SHAP scores corresponding to input flux for the trained FS model. In this context, the prediction refers to the compressed latent-space encoding. While emission lines such as Hα and [NII] clearly dominate as individual features, it is notable that the importance—and therefore predictive power of the continuum is biased toward bluer wavelengths. These findings broadly agree with the negentropy-based analysis of Ferreras et al. (2023), although the variation in SHAP scores indicates that useful information is more widely distributed across the continuum. SHAP has limitations, and care must be taken when interpreting the scores. Nevertheless, the observed discrepancy highlights the value of considering alternative proxies for information content beyond variance. Entropy-based techniques and classical data-driven methods such as PCA scale variance in ways that may overlook subtle but important dependencies in spectral data. Given the SHAP dominance of emission lines, future studies may benefit from masking these features to encourage models to leverage more obscure patterns. The figure also highlights the SHAP scores associated with the red and blue regions. Although previous research has identified strong information content in the red region in terms of variance, it does not hold a notably large share of SHAP importance; the Fe and Mg features within this window do not appear to play a major role. Nonetheless, the plot indicates that their combined contribution acts as a useful signal to the FS model, offering insight into how the NW models achieve a limited ability to predict unseen spectral regions, consistent with the high degree of information entanglement across absorption spectra at many wavelengths. An important observation is that the overall shape of the SHAP curve closely mirrors the error distribution shown in Figure 4. The wavelengths that the CS model fails to represent accurately tend to exhibit strong predictive power in the FS model. This reinforces the idea that the continuum holds crucial information that poses a challenge for deep reconstruction methods, despite known correlations between the continuum and absorption or emission lines. This study highlights both opportunities and challenges in applying deep learning methods to the denoising and reconstruction of galaxy spectra. A key insight is that reproducing spectra through an information bottleneck with good generalization limits the capacity to memorize random noise. Importantly, an MAE loss appears to enhance this denoising effect. Among the tested architectures, the FS model performs best, demonstrating that unsupervised autoencoders can effectively increase the signal-to-noise ratio of an SDSS training set while generalizing to unseen spectra. Figure 8 shows a comparison between the recovered spectra (in color) and the original noisy SDSS spectra (in gray) for six representative cases. The colors correspond to evolutionary classifications—quiescent (red), star-forming (blue), and AGN (green)—following the classification of Sharbaf et al. (2023) based on nebular emission properties. The figure zooms into three spectral regions commonly used for analyzing the stellar and gaseous components of galaxies. The deep learning method extracts information from the ensemble to predict higher-S/N spectra, minimizing biases within the limits of the parent sample. Compared to traditional denoising algorithms, which require careful parameter tuning to balance smoothing with the preservation of sharp features, our approach leverages statistics learned from the ensemble in a fully emergent way. Tests with synthetic spectra suggest that classical filtering techniques often produce overly smoothed outputs that do not reliably represent the underlying spectrum. Our deep-learning framework performs better at recovering the true signal, provided that the noisy inputs are processed consistently with the training data. Unlike supervised approaches that rely on the addition of synthetic noise, our results emerge directly from real spectral data. Beyond denoising capability, the SHAP analysis provides insight into how the models utilize spectral features, highlighting differences from traditional information-theoretic techniques and reinforcing the importance of the continuum. More broadly, this work demonstrates the utility of explainability methods in astronomy. As deep learning becomes increasingly prevalent, maintaining human interpretability remains essential. Several avenues for improvement remain. The standard MAE loss used here treats all wavelengths as equally important; weighting the loss toward physically important regions could improve performance. Neural networks also exhibit a bias toward learning low-frequency signals (Rahaman et al. 2018). Multi-stage neural networks or related strategies may help represent and leverage sharp features such as emission lines. Finally, more sophisticated deep-learning architectures such as attention mechanisms (e.g., Bahdanau et al. 2014) could better capture subtle correlations found within galaxy spectra.'}]",In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity.
How can I assess image-text alignment in multimodal models?,2510.13937v1,,,"['2510.13937v1', '2509.20913v1', '2509.23158v1', '2510.12758v1', '2510.08411v1']","[5.0, 5.0, 5.0, 5.0, 5.0]","['Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning']","[{'rank': 1, 'score': 5.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 2, 'score': 5.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 5.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}, {'rank': 4, 'score': 5.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}, {'rank': 5, 'score': 5.0, 'id': '2510.08411v1', 'title': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning', 'text': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning. Spectroscopy represents the ideal observational method to maximally extract information from galaxies regarding their star formation and chemical enrichment histories. However absorption spectra of galaxies prove rather challenging at high redshift or in low mass galaxies due to the need to spread the photons into a relatively large set of spectral bins. For this reason the data from many state-of-the-art spectroscopic surveys suffer from low signal-to-noise S/N ratios and prevent accurate estimates of the stellar population parameters. In this paper we tackle the issue of denoising an ensemble by the use of unsupervised Deep Learning techniques trained on a homogeneous sample of spectra over a wide range of S/N. These methods reconstruct spectra at a higher S/N and allow us to investigate the potential for Deep Learning to faithfully reproduce spectra from incomplete data. Our methodology is tested on three key line strengths and is compared with synthetic data to assess retrieval biases. The results suggest a standard Autoencoder as a very powerful method that does not introduce systematics in the reconstruction. We also note in this work how careful the analysis needs to be as other methods can on a quick check produce spectra that appear noiseless but are in fact strongly biased towards a simple overfitting of the noisy input. Denoising methods with minimal bias will maximise the quality of ongoing and future spectral surveys such as DESI WEAVE or WAVES. In this regard it is not uncommon to need values of S/N per resolution element above 20-30 if not higher for detailed analyses of subtle differences in the populations such as variations of chemical abundances or the initial mass function e.g. La Barbera et al. 2013 2017 Ferreras et al. 2019. In this regard spectroscopic surveys tend to be optimised to produce large volumes of data at the cost of a lower S/N and so any algorithm aimed at increasing the S/N of the data proves a very valuable tool especially with the ongoing and upcoming surveys such as DESI DESI Collaboration et al. 2025 WEAVE Jin et al. 2024 or WAVES Driver et al. 2019. Galaxy spectra in the wavelength interval from near ultraviolet to near infrared encode a vast amount of information regarding the properties of the underlying stellar populations. The continuum and absorption lines of the photospheres of constituent stars leave their imprint on the integrated spectra and represent the workhorse of galaxy formation studies concerning the star formation and chemical enrichment histories. Spectroscopic surveys of galaxies such as the Sloan Digital Sky Survey York et al. 2000 have greatly helped deepen our understanding of galaxy formation. However spectroscopy requires large integration times as the faint light from galaxies is spread with respect to wavelength. Often times spectra have been used mainly by cosmologists as a tool to derive redshift and thus determine the largescale distribution of galaxies. However exploring the galaxies themselves through their stellar populations requires deeper data at a higher signal-to-noise S/N ratio in order to compare the observations with detailed models of stellar popuThe actual S/N of an observation is borne out of the competition between the photons from the galaxy and spurious photons or counts coming from unwanted sources such as the background sky airglow detectors and reduction artifacts etc. Increasing the S/N of a single spectrum is typically not feasible unless models are used therefore introducing large systematics. Our approach to this problem starts from a large ensemble of spectra taken by the same instrument and following the same reduction process. The ensemble should also include a large amount of high quality i.e. high S/N data E-mail oc00149 surrey.ac.uk Corresponding author ignacio.ferreras iac.es 2025 The Authors 2 Camilleri et al. so that the method can somehow interpolate among the ensemble members to produce optimised versions of the data. Traditional data driven methods such as Principal Component Analysis have been applied to stellar and galaxy spectra for instance to remove the emission lines from airglow Wild Hewett 2005. These methods are commonly used for classification purposes Madgwick et al. 2003 Mc Gurk et al. 2010 and can also help in the interpretability of the information content for instance by exploring the resultant latent space e.g. Sharbaf et al. 2023 2025. However as a linear method PCA is less versatile to encode and model the many intricacies of the spectra in a large ensemble. is presented in 5. Finally our concluding remarks are given in 6. spectra of the Sloan Digital Sky Survey York et al. 2000. The data include a large number of spectra of order 1 million and most importantly covers a wide range of S/N with a large number of high quality data at a S/N higher than 10-20 and many spectra at lower S/N. This represents an ideal training sample as the data processing is homogeneously performed minimising biases and covers all types of evolutionary stages of galaxies mass morphology etc. Moreover each observation includes in addition to the actual spectrum a best fit model that can be adopted as a synthetic case to which noise is added as we will show below. The sample is the same as the one presented in Sharbaf et al. 2023 and the motivation behind the constraints regarding the quality of the data as an ensemble is identical. For instance in order to set this exercise in the best defined scenario we include a constraint in stellar velocity dispersion between 100 and 150 km s 1 as a wider range will also introduce the expected bias in effective spectral resolution caused by the kinematic kernel. The data are taken from SDSS Data Release 16 Ahumada et al. 2020 and correspond to single fibre measurements of the central parts of galaxies 3 arcsec diameter at spectral resolution R 2 000 Smee et al. 2013. The targets are selected as completely as possible down to a Petrosian flux level for the target galaxy in the SDSS-r band of r 17.77 AB Strauss et al. 2002. The data were further constrained in redshift z 0.05 0.1 and in S/N measured as an average within the SDSS-r band higher than 15 per pixel log λ/A 10 4. The total sample comprises 68 794 spectra which were retrieved from the main SDSS database de-redshifted and de-reddened regarding foreground dust absorption from the Milky Way following a standard Fitzpatrick 1999 attenuation law. To remove the variations caused by different stellar mass and redshift of the galaxies all spectra are normalized to the same average flux in 6 000-6 500A window in the rest-frame. The spectral range is restricted to the rest-frame wavelength λ A. Finally for one of the tests we explore the denoising procedure when training on data without continuum. For that purpose we take the robust high percentile method of Rogers et al. 2010 to define the continuum that is removed from each spectra for this test case. For more details about the sample please see Sharbaf et al. 2023. Deep Learning DL methods are seeing a rapid uptake in use throughout astronomy helping to address problems that traditional data-driven approaches struggle with. For example DL has been applied to galaxy and stellar spectra for classification purposes e.g. Folkes et al. 1996 Fabbro et al. 2018 Wu et al. 2024 dimensionality reduction e.g. Portillo et al. 2020 recovery of spectra with bad quality e.g. Wang et al. 2017 general analysis e.g. Lovell et al. 2019 Melchior et al. 2023 or in the search for anomalies e.g. Baron Poznanski 2017 Liang et al. 2023. In this paper we adopt a set of DL algorithms each trained on a large set of galaxy spectra from the Legacy part of SDSS and then assess their ability to increase the S/N of input spectra. These models are unsupervised and in contrast to works like Scourfield et al. 2023 do not rely on adding synthetic noise to training data. Instead the denoising effects seen are emergent ultimately stemming from information bottlenecks and aided by the formulation of the objective function. Furthermore we experiment with the reconstruction of spectra from incomplete data and attempt to explain deep model decision making. Please note it is important to ensure that the process does not alter the sample in a systematic way. We also emphasise that this method is not a smoothing process where the S/N can be increased at the cost of a lower spectral resolution. A similar type of work has been recently presented in Scourfield et al. 2023 mainly focussed on retrieval of emission line data. The authors conclude that a Variational Autoencoder performs better than PCA and study the effect of denoising DESI data from an SDSS-trained set regarding the relationship between stellar mass and gas phase metallicity. Melchior et al. 2023 also consider the use of autoencoders to analyse galaxy spectra and look into the interpretability of latent space with results mostly focused on emission lines which is where the spectral variance fully dominates in starforming and AGN systems. The authors indeed suggest that these methods can be adopted to denoise ensembles of spectra. This work complements these previous studies turning to the more challenging case of the absorption line spectra that is commonly used to constrain the stellar population content and the past star formation history. We also consider the issue of potential biasing of the denoised data by use of synthetic spectra that are adopted as ground truth to assess the fidelity of the recovered measurements. 3.1 Butterworth Filtering The structure of the paper is as follows we give a brief presentation of the working sample in 2 along with a description of the various methods tested for denoising in 3. The comparison of retrieved and original data is shown in 4 including a comparison of synthetic data with added noise. A brief discussion of the explainability of the DL performance The Butterworth filter BF is a classical signal processing approach used to selectively attenuate unwanted frequencies within a general signal. Its maximally flat frequency response in the passband supresses signal distortion making it a popular choice in a range of domains. The squared magnitude of the frequency response H ω 2 at angular frequency ω is MNRAS 000 1 9 2025 Emergent Denoising of SDSS Galaxy Spectra 3 4 Camilleri et al. SDSS Synthetic Worthey Ottaviani 1997 and the traditional Mgb index of the Lick system Trager et al. 1998. The figure of merit is defined for each line strength measurement as follows we produce a vector with the residuals of the output and the reference for each spectrum δs Ii Ii s Ii r where s represents the output spectrum and r the reference spectrum. The mean or median of the ensemble δs I are expected to be close to zero and the standard deviation represents how well the data are recovered. Therefore we adopt the standard deviation of the residuals as our figure of merit 0.0 0.2 0.4 0.6 0.8 S/N 5 0.0 0.2 0.4 0.6 0.8 Dn 4000 NW CS FS BF NW-S NW CS FS BF NW-S H F O N 1 2 3 S/N 5 1 2 3 I p δ2 I δ I 2. 5 Also note that the reference case Ii r is the comparison spectra that can be defined in two ways it is either the noiseless best fit data O or the noisy original SDSS data N. The former allows us to quantify the denoising process whereas the latter is used to test overfitting. Fig. 2 shows the residual statistic measured at a S/N 5 as an average over all spectra with S/N for the reconstruction of an additional unseen set of real SDSS spectra left or a set of synthetic spectra right. In the case with real data we only show the data points corresponding to O as N would trivially compare the noisy data with itself. For the synthetic case we can compare the residual statistic for the ground truth case O and for a noisy realization of this ideal case with Gaussian noise that shows the same S/N as the original data N. In this framework the case N O would be indicative of a kind of overfitting. The opposite would be suggestive of true denoising. For reference the value of O for the comparison of noiseless and noisy synthetic data i.e. the variance expected by the presence of noise in the spectra is shown in each case as a horizontal dashed blue line. The performance of the different methods is comparable with the SDSS data reconstruction left panels although the BF method appears to fare worse. The more interesting results are found for the synthetic data right panels where we can discriminate between the recovery of the input noisy data N blue stars or a more desirable reconstruction of the original noiseless spectra. O red circles. One thing that stands out quite clearly is that the 4000A break strength is poorly determined by the CS method. This may be quite expected as the Dn 4000 is wider than the other two and relies on the continuum. However we performed this test as there is a well-known degeneracy between parameters so that for instance Dn 4000 indices are correlated with e.g Mgb. The strong covariance between line strengths found in Ferreras et al. 2023 indicates that the absorption line spectrum would encode similar information as the continuum. This experiment suggests that discovering and leveraging this trend is challenging for DL methods. The figure also shows an uncanny inversion of the star circle order i.e. N vs O in the BF method with respect to the other algorithms. We emphasize that this method does not use information from the ensemble and only relies on a careful filtering of high frequencies many of which would be ascribed to noise. This would be equivalent to a truncation in a Fourier series. The results presented here reveal that the BF method tends to overfit so that it produces an optimal reconstruction when using noisy data as input but underperforms when the noiseless synthetic data are considered. NW CS FS BF NW-S NW CS FS BF NW-S 0.5 1.0 1.5 2.0 2.5 3.0 S/N 5 0.5 1.0 1.5 2.0 2.5 3.0 Mgb NW CS FS BF NW-S NW CS FS BF NW-S Emergent Denoising of SDSS Galaxy Spectra 5 Dn 4000 H F Dn 4000 H F Mgb Mgb 0.0 0.1 0.2 0.3 0.4 O Noise NW CS FS BF NW-S Noise NW CS FS BF NW-S 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 O 0.0 0.4 0.8 1.2 1.25 1.50 1.75 1 2 2 3 4 10 20 10 20 10 20 0.0 0.1 0.2 0.3 0.4 N 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 N 0.0 0.4 0.8 1.2 1.25 1.50 1.75 Dn 4000 1 2 3 10 20 S/N g-band 10 20 S/N g-band 10 20 S/N g-band 6 Camilleri et al. 0.8 The other two indices show a more characteristic behaviour between the BF overfitting algorithm and the DL method. Note that both HδF and Mgb produce distributions closer to the ground truth with FS whereas BF closely resembles the wider shape of the histogram of noisy data. From these tests we conclude that a DL method is successful at improving the quality of galaxy spectra if a representative large ensemble with a wide range of S/N including high quality data with the same instrumental / data characteristics are available. Deep models are often considered black boxes because their mappings and decision-making processes are difficult to interpret. To better understand how spectral features are being leveraged, we employ SHAP (SHapley Additive exPlanations, Lundberg et al. 2020). SHAP is a game-theoretic method that explains model outputs by assigning each feature an importance value based on its contribution to the gap between the model’s actual prediction and its mean prediction, averaged over all possible feature subsets. Figure 7 shows the mean SHAP scores corresponding to input flux for the trained FS model. In this context, the prediction refers to the compressed latent-space encoding. While emission lines such as Hα and [NII] clearly dominate as individual features, it is notable that the importance—and therefore predictive power of the continuum is biased toward bluer wavelengths. These findings broadly agree with the negentropy-based analysis of Ferreras et al. (2023), although the variation in SHAP scores indicates that useful information is more widely distributed across the continuum. SHAP has limitations, and care must be taken when interpreting the scores. Nevertheless, the observed discrepancy highlights the value of considering alternative proxies for information content beyond variance. Entropy-based techniques and classical data-driven methods such as PCA scale variance in ways that may overlook subtle but important dependencies in spectral data. Given the SHAP dominance of emission lines, future studies may benefit from masking these features to encourage models to leverage more obscure patterns. The figure also highlights the SHAP scores associated with the red and blue regions. Although previous research has identified strong information content in the red region in terms of variance, it does not hold a notably large share of SHAP importance; the Fe and Mg features within this window do not appear to play a major role. Nonetheless, the plot indicates that their combined contribution acts as a useful signal to the FS model, offering insight into how the NW models achieve a limited ability to predict unseen spectral regions, consistent with the high degree of information entanglement across absorption spectra at many wavelengths. An important observation is that the overall shape of the SHAP curve closely mirrors the error distribution shown in Figure 4. The wavelengths that the CS model fails to represent accurately tend to exhibit strong predictive power in the FS model. This reinforces the idea that the continuum holds crucial information that poses a challenge for deep reconstruction methods, despite known correlations between the continuum and absorption or emission lines. This study highlights both opportunities and challenges in applying deep learning methods to the denoising and reconstruction of galaxy spectra. A key insight is that reproducing spectra through an information bottleneck with good generalization limits the capacity to memorize random noise. Importantly, an MAE loss appears to enhance this denoising effect. Among the tested architectures, the FS model performs best, demonstrating that unsupervised autoencoders can effectively increase the signal-to-noise ratio of an SDSS training set while generalizing to unseen spectra. Figure 8 shows a comparison between the recovered spectra (in color) and the original noisy SDSS spectra (in gray) for six representative cases. The colors correspond to evolutionary classifications—quiescent (red), star-forming (blue), and AGN (green)—following the classification of Sharbaf et al. (2023) based on nebular emission properties. The figure zooms into three spectral regions commonly used for analyzing the stellar and gaseous components of galaxies. The deep learning method extracts information from the ensemble to predict higher-S/N spectra, minimizing biases within the limits of the parent sample. Compared to traditional denoising algorithms, which require careful parameter tuning to balance smoothing with the preservation of sharp features, our approach leverages statistics learned from the ensemble in a fully emergent way. Tests with synthetic spectra suggest that classical filtering techniques often produce overly smoothed outputs that do not reliably represent the underlying spectrum. Our deep-learning framework performs better at recovering the true signal, provided that the noisy inputs are processed consistently with the training data. Unlike supervised approaches that rely on the addition of synthetic noise, our results emerge directly from real spectral data. Beyond denoising capability, the SHAP analysis provides insight into how the models utilize spectral features, highlighting differences from traditional information-theoretic techniques and reinforcing the importance of the continuum. More broadly, this work demonstrates the utility of explainability methods in astronomy. As deep learning becomes increasingly prevalent, maintaining human interpretability remains essential. Several avenues for improvement remain. The standard MAE loss used here treats all wavelengths as equally important; weighting the loss toward physically important regions could improve performance. Neural networks also exhibit a bias toward learning low-frequency signals (Rahaman et al. 2018). Multi-stage neural networks or related strategies may help represent and leverage sharp features such as emission lines. Finally, more sophisticated deep-learning architectures such as attention mechanisms (e.g., Bahdanau et al. 2014) could better capture subtle correlations found within galaxy spectra.'}]",By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. Confusion matrix analysis was used to quantitatively assess classification performance.
What metrics should I use to evaluate text generation models?,2510.14855v1,,,"['2510.14855v1', '2510.13937v1', '2509.23158v1', '2509.20913v1', '2510.11073v1']","[7.0, 7.0, 7.0, 7.0, 6.0]","['A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer']","[{'rank': 1, 'score': 7.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 2, 'score': 7.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 3, 'score': 7.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}, {'rank': 4, 'score': 7.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 5, 'score': 6.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}]",Additionally the model s internal representation is used to predict feature values without image generation. To take care of the E we will evaluate E in the context of simulated sequences.
How can we detect sarcasm using deep learning?,2510.10729v1,,,"['2510.10729v1', '2509.20913v1', '2510.11073v1', '2510.14855v1', '2510.13050v1']","[6.0, 6.0, 6.0, 5.0, 5.0]","['Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting']","[{'rank': 1, 'score': 6.0, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through a multi-layer architecture. Although the model is not yet implemented the design demonstrates feasibility for real-world applications like chatbots and social media monitoring. Additional discussions on data preprocessing techniques model evaluation strategies and ethical implications further contextualize the approach. Index Terms Sarcasm Detection Deep Learning Convolutional Neural Networks Natural Language Processing BERT Multimodal Learning Text Analysis Emotion Detection. Sarcasm detection has been an active area of research due to its implications in sentiment analysis and opinion mining. Several studies have proposed diverse approaches ranging from rule-based systems to deep learning architectures. Jamil et al. introduced a hybrid model using Convolutional Neural Networks CNNs and Long Short-Term Memory LSTM networks to detect sarcasm across multidomain datasets. Their work emphasized the combination of spatial and sequential learning for improved accuracy. Razali et al. explored deep contextual embedding techniques highlighting the importance of semantic features and domain knowledge in sarcasm detection. Their model significantly enhanced performance by considering word context at the sentence level. Poria et al. presented a Deep CNN-based architecture for sarcastic tweet classification which captured local text patterns and contributed to detecting sarcastic undertones. Their research focused on leveraging spatial hierarchies within tweets. Liu et al. designed a multitask deep neural network for general language understanding tasks providing foundational models that indirectly benefit sarcasm detection through shared contextual learning across related tasks. Zhang et al. proposed a deep neural network approach specifically tailored to sarcasm detection in tweets incorporating embedding layers and convolutional filters to capture sarcastic phrases and sentence structures. Du et al. proposed an approach that combined sentimental context and individual expression patterns to detect sarcasm more reliably. Their study emphasized personalized modeling for better generalization. Bharti et al. extended the problem into the multimodal domain integrating image data alongside text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must infer sarcasm from textual patterns alone. This paper explores a conceptual solution using DCNNs combined with contextual embedding models to understand sarcasm s complex indicators such as irony sentiment contradiction and hyperbole. Applications range from content moderation on social platforms to enhancing virtual assistant interactions. Sarcasm is a complex form of communication that relies heavily on tone context and cultural cues. Humans often detect sarcasm by recognizing exaggerated language contradictions or situational irony which are difficult for machines to grasp. Traditional text processing tools typically fail to interpret such nuanced expressions. With the emergence of deep learning models like CNNs and transformers it has become feasible to explore sarcasm detection using machine learning. This paper proposes a deep neural architecture that mimics this human-like understanding by analyzing multiple model achieved superior performance on social media sarcasm datasets. Sharma et al. developed a hybrid autoencoder model capable of capturing both shallow and deep semantic patterns contributing to accurate sarcasm classification in social media posts. Eke et al. focused on context-based feature engineering using BERT proposing a system that leverages sentence-level embeddings and fine-tuned transformers to boost classification accuracy. These prior works demonstrate the effectiveness of deep learning methods in sarcasm detection. However many lack scalability explainability or modularity. Our work builds upon these foundations proposing a modular framework that unifies sentiment context linguistic cues and emotion analysis into an adaptable and extensible system for robust sarcasm detection. Fig. 2. Architecture of a Convolutional Neural Network IV. A. Neural Networks A Neural Network is a machine learning model inspired by the human brain consisting of interconnected neurons arranged in layers. Each layer processes input data through weighted connections and activation functions to learn patterns and make decisions. Fig. 3. Structure of a Deep Convolutional Neural Network V. PROPOSED METHODOLOGY The proposed sarcasm detection system is designed using a modular architecture composed of four specialized detection modules that collaboratively interpret different linguistic signals. These include Sentiment Analysis This module employs VADER or BERT-based sentiment analysis models to capture the emotional polarity of a sentence. Sarcasm often involves polarity flips where positive sentiment is expressed with a negative undertone or vice versa. VADER with its rule-based sentiment scoring excels in social media text while BERT captures deeper contextual sentiment shifts. Contextual Embedding Powered by BERT this module encodes the input sentence into high-dimensional vectors that reflect contextual meaning. Unlike traditional embeddings e.g. Word2Vec BERT dynamically adjusts word meanings based on their sentence context which is crucial in understanding nuanced sarcasm. Linguistic Features This component utilizes Spa Cy and custom NLP rules to extract syntactic and semantic cues such as punctuation usage exaggerated expressions all caps and interjections e.g. Yeah right. Fig. 1. Illustration of a Basic Neural Network Emotion Detection A CNN/LSTM hybrid model is used to detect underlying emotional tone such as frustration amusement or confusion. These emotions when mismatched with surface sentiment often signal sarcastic intent. B. Convolutional Neural Networks CNNs CNNs are specialized neural networks designed to process data with grid-like topology such as images or text matrices. They use convolutional layers to detect patterns like edges or textual features followed by pooling layers to reduce dimensionality. The outputs from these modules are concatenated into a unified feature vector followed by normalization and transformation layers. The fused vector is passed to a metaclassifier typically a logistic regression model or a shallow neural network which outputs a binary classification sarcastic or non-sarcastic. The system supports flexibility and C. Deep Convolutional Neural Networks DCNNs DCNNs extend CNNs by adding multiple convolutional and pooling layers enabling the network to learn more abstract and complex representations of data. extensibility allowing individual modules to be improved or swapped without affecting the entire architecture. VII. DATA PREPROCESSING Ensuring fairness begins with curating diverse and representative datasets. Sarcasm is heavily culture-dependent and varies across languages dialects and social groups. A model trained primarily on English tweets from a specific region may generalize poorly to other linguistic contexts. Incorporating datasets from varied cultural and demographic sources improves fairness and inclusivity. Transparency and explainability are equally critical. Endusers and developers should be able to understand why a specific piece of content was labeled as sarcastic. Techniques like LIME and SHAP help visualize feature importance and decision rationale. Moreover models should include feedback loops that allow users to flag misclassifications facilitating continual learning and correction. Privacy preservation is essential when scraping social media data for training. Proper anonymization and ethical approval must be obtained before model development. Finally regular auditing of models for bias drift and ethical compliance ensures that systems remain accountable and socially responsible throughout their lifecycle. VI. SYSTEM ARCHITECTURE Fig. 4. Proposed Modular System Architecture for Sarcasm Detection VIII. CASE STUDY The system architecture is structured into a layered pipeline starting with input ingestion and preprocessing followed by parallel feature extraction streams and a final aggregation layer. 1. Input Preprocessing Text data is collected from social media platforms and undergoes extensive cleaning removal of special characters hashtags emojis links and user handles. The cleaned text is tokenized and passed through lemmatization for standardization. 2. Feature Extraction Modules The input is fed simultaneously into sentiment contextual linguistic and emotion detection modules. Each module processes the text independently producing an output feature vector that encapsulates its specialized analysis. 3. Aggregation and Fusion Feature vectors from all modules are aggregated using concatenation and passed through dimensionality reduction e.g. PCA or attention-based fusion to form a composite representation. This vector is processed by the classification engine. 4. Classification Layer A meta-classifier trained on labeled sarcasm datasets takes the fused representation and outputs the probability of the text being sarcastic. Logistic regression or shallow feedforward neural networks are typically used at this stage. 5. Feedback Loop User feedback is captured through upvotes/downvotes or flags enabling continuous retraining and improving the model s precision over time. This adaptive learning loop enhances robustness to evolving sarcastic trends. Such a modular design supports horizontal scalability modules can be parallelized or independently optimized while ensuring maintainability and extensibility of the system. A conceptual case study evaluates our framework on a multimodal sarcasm dataset comprising text-image tweet pairs. Text is encoded via BERT while images are processed using Dense Net for visual sarcasm features such as facial expressions contextual image cues or meme-style exaggeration. This method ensures that both language and visual content contribute to the sarcasm prediction process. The dataset used in the study includes publicly available labeled tweets containing sarcastic hashtags such as sarcasm irony and not. Each tweet was paired with its respective image and then preprocessed text was tokenized and embedded using BERT while images were resized and fed into a pre-trained Dense Net. Feature vectors from both models were concatenated and passed to a fusion classifier for final sarcasm prediction. Fig. 5. Model Accuracy Comparison on Twitter Dataset The results demonstrate clear advantages of multimodal learning. BERT alone achieved an accuracy of 88.6% Dense Net alone achieved 74.3% and the combined model reached 93.2%. These findings confirm that visual signals add significant value in identifying sarcasm especially when textual cues are ambiguous. In a practical application scenario this model could assist content moderators by automatically flagging sarcastic content thus enhancing sentiment analysis systems. Future studies could expand this work by exploring multilingual sarcastic tweets using emotion-labeled image datasets and integrating audio cues such as tone and inflection. User-centered evaluation through crowdsourcing platforms e.g. Amazon Mechanical Turk can further validate the model s predictions by comparing them with human judgments. This process enhances the credibility of sarcasm classification in real-world deployments. We propose a conceptual sarcasm detection framework integrating sentiment, emotion, context, and linguistic cues through deep learning. The modular nature of the architecture enables flexibility in upgrading or replacing individual detection modules, making the system highly extensible and suitable for varied use cases. The integration of multiple feature domains ensures a holistic understanding of sarcasm, improving interpretability and robustness. This research highlights the potential of combining advanced NLP techniques with deep learning models to enhance automated language understanding, particularly in nuanced areas like sarcasm. Our approach leverages pretrained models like BERT for deep contextual embeddings, complemented by handcrafted linguistic rules and emotion analysis. The preliminary design showcases promising capabilities and adaptability across domains. Future work involves implementing the full pipeline and conducting large-scale experiments across multiple datasets, including multilingual corpora. Real-time testing scenarios, such as integration with chatbots, virtual assistants, or sentiment analysis systems, are intended to validate the model s practical effectiveness. We also aim to introduce adversarial training to make the model resilient against input manipulations and sarcasm obfuscation techniques. Further improvements include enhancing multimodal detection by incorporating audio and video inputs. Prosodic features like tone, pitch, and speech rate can provide additional cues for sarcasm detection in voice-based systems. Likewise, visual elements like facial expressions and gestures could enrich interpretation in video communications. To address ethical considerations, future iterations will focus on fairness audits, bias mitigation, and explainability features [14]. Building user trust and ensuring transparent decision-making processes will be crucial, particularly in deployment scenarios involving moderation or user profiling. This research lays the groundwork for intelligent systems that can engage in human-like communication while maintaining ethical integrity and operational reliability.'}, {'rank': 2, 'score': 6.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 6.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 4, 'score': 5.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 5, 'score': 5.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}]",proposed a deep neural network approach specifically tailored to sarcasm detection in tweets incorporating embedding layers and convolutional filters to capture sarcastic phrases and sentence structures. With the emergence of deep learning models like CNNs and transformers it has become feasible to explore sarcasm detection using machine learning.
How can I detect or measure bias in deep learning models?,2509.20913v1,,,"['2509.20913v1', '2510.12758v1', '2510.12850v1', '2510.11073v1', '2510.08411v1']","[10.0, 8.0, 8.0, 8.0, 8.0]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning']","[{'rank': 1, 'score': 10.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 2, 'score': 8.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}, {'rank': 3, 'score': 8.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 4, 'score': 8.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 5, 'score': 8.0, 'id': '2510.08411v1', 'title': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning', 'text': 'Emergent Denoising of SDSS Galaxy Spectra Through Unsupervised Deep Learning. Spectroscopy represents the ideal observational method to maximally extract information from galaxies regarding their star formation and chemical enrichment histories. However absorption spectra of galaxies prove rather challenging at high redshift or in low mass galaxies due to the need to spread the photons into a relatively large set of spectral bins. For this reason the data from many state-of-the-art spectroscopic surveys suffer from low signal-to-noise S/N ratios and prevent accurate estimates of the stellar population parameters. In this paper we tackle the issue of denoising an ensemble by the use of unsupervised Deep Learning techniques trained on a homogeneous sample of spectra over a wide range of S/N. These methods reconstruct spectra at a higher S/N and allow us to investigate the potential for Deep Learning to faithfully reproduce spectra from incomplete data. Our methodology is tested on three key line strengths and is compared with synthetic data to assess retrieval biases. The results suggest a standard Autoencoder as a very powerful method that does not introduce systematics in the reconstruction. We also note in this work how careful the analysis needs to be as other methods can on a quick check produce spectra that appear noiseless but are in fact strongly biased towards a simple overfitting of the noisy input. Denoising methods with minimal bias will maximise the quality of ongoing and future spectral surveys such as DESI WEAVE or WAVES. In this regard it is not uncommon to need values of S/N per resolution element above 20-30 if not higher for detailed analyses of subtle differences in the populations such as variations of chemical abundances or the initial mass function e.g. La Barbera et al. 2013 2017 Ferreras et al. 2019. In this regard spectroscopic surveys tend to be optimised to produce large volumes of data at the cost of a lower S/N and so any algorithm aimed at increasing the S/N of the data proves a very valuable tool especially with the ongoing and upcoming surveys such as DESI DESI Collaboration et al. 2025 WEAVE Jin et al. 2024 or WAVES Driver et al. 2019. Galaxy spectra in the wavelength interval from near ultraviolet to near infrared encode a vast amount of information regarding the properties of the underlying stellar populations. The continuum and absorption lines of the photospheres of constituent stars leave their imprint on the integrated spectra and represent the workhorse of galaxy formation studies concerning the star formation and chemical enrichment histories. Spectroscopic surveys of galaxies such as the Sloan Digital Sky Survey York et al. 2000 have greatly helped deepen our understanding of galaxy formation. However spectroscopy requires large integration times as the faint light from galaxies is spread with respect to wavelength. Often times spectra have been used mainly by cosmologists as a tool to derive redshift and thus determine the largescale distribution of galaxies. However exploring the galaxies themselves through their stellar populations requires deeper data at a higher signal-to-noise S/N ratio in order to compare the observations with detailed models of stellar popuThe actual S/N of an observation is borne out of the competition between the photons from the galaxy and spurious photons or counts coming from unwanted sources such as the background sky airglow detectors and reduction artifacts etc. Increasing the S/N of a single spectrum is typically not feasible unless models are used therefore introducing large systematics. Our approach to this problem starts from a large ensemble of spectra taken by the same instrument and following the same reduction process. The ensemble should also include a large amount of high quality i.e. high S/N data E-mail oc00149 surrey.ac.uk Corresponding author ignacio.ferreras iac.es 2025 The Authors 2 Camilleri et al. so that the method can somehow interpolate among the ensemble members to produce optimised versions of the data. Traditional data driven methods such as Principal Component Analysis have been applied to stellar and galaxy spectra for instance to remove the emission lines from airglow Wild Hewett 2005. These methods are commonly used for classification purposes Madgwick et al. 2003 Mc Gurk et al. 2010 and can also help in the interpretability of the information content for instance by exploring the resultant latent space e.g. Sharbaf et al. 2023 2025. However as a linear method PCA is less versatile to encode and model the many intricacies of the spectra in a large ensemble. is presented in 5. Finally our concluding remarks are given in 6. spectra of the Sloan Digital Sky Survey York et al. 2000. The data include a large number of spectra of order 1 million and most importantly covers a wide range of S/N with a large number of high quality data at a S/N higher than 10-20 and many spectra at lower S/N. This represents an ideal training sample as the data processing is homogeneously performed minimising biases and covers all types of evolutionary stages of galaxies mass morphology etc. Moreover each observation includes in addition to the actual spectrum a best fit model that can be adopted as a synthetic case to which noise is added as we will show below. The sample is the same as the one presented in Sharbaf et al. 2023 and the motivation behind the constraints regarding the quality of the data as an ensemble is identical. For instance in order to set this exercise in the best defined scenario we include a constraint in stellar velocity dispersion between 100 and 150 km s 1 as a wider range will also introduce the expected bias in effective spectral resolution caused by the kinematic kernel. The data are taken from SDSS Data Release 16 Ahumada et al. 2020 and correspond to single fibre measurements of the central parts of galaxies 3 arcsec diameter at spectral resolution R 2 000 Smee et al. 2013. The targets are selected as completely as possible down to a Petrosian flux level for the target galaxy in the SDSS-r band of r 17.77 AB Strauss et al. 2002. The data were further constrained in redshift z 0.05 0.1 and in S/N measured as an average within the SDSS-r band higher than 15 per pixel log λ/A 10 4. The total sample comprises 68 794 spectra which were retrieved from the main SDSS database de-redshifted and de-reddened regarding foreground dust absorption from the Milky Way following a standard Fitzpatrick 1999 attenuation law. To remove the variations caused by different stellar mass and redshift of the galaxies all spectra are normalized to the same average flux in 6 000-6 500A window in the rest-frame. The spectral range is restricted to the rest-frame wavelength λ A. Finally for one of the tests we explore the denoising procedure when training on data without continuum. For that purpose we take the robust high percentile method of Rogers et al. 2010 to define the continuum that is removed from each spectra for this test case. For more details about the sample please see Sharbaf et al. 2023. Deep Learning DL methods are seeing a rapid uptake in use throughout astronomy helping to address problems that traditional data-driven approaches struggle with. For example DL has been applied to galaxy and stellar spectra for classification purposes e.g. Folkes et al. 1996 Fabbro et al. 2018 Wu et al. 2024 dimensionality reduction e.g. Portillo et al. 2020 recovery of spectra with bad quality e.g. Wang et al. 2017 general analysis e.g. Lovell et al. 2019 Melchior et al. 2023 or in the search for anomalies e.g. Baron Poznanski 2017 Liang et al. 2023. In this paper we adopt a set of DL algorithms each trained on a large set of galaxy spectra from the Legacy part of SDSS and then assess their ability to increase the S/N of input spectra. These models are unsupervised and in contrast to works like Scourfield et al. 2023 do not rely on adding synthetic noise to training data. Instead the denoising effects seen are emergent ultimately stemming from information bottlenecks and aided by the formulation of the objective function. Furthermore we experiment with the reconstruction of spectra from incomplete data and attempt to explain deep model decision making. Please note it is important to ensure that the process does not alter the sample in a systematic way. We also emphasise that this method is not a smoothing process where the S/N can be increased at the cost of a lower spectral resolution. A similar type of work has been recently presented in Scourfield et al. 2023 mainly focussed on retrieval of emission line data. The authors conclude that a Variational Autoencoder performs better than PCA and study the effect of denoising DESI data from an SDSS-trained set regarding the relationship between stellar mass and gas phase metallicity. Melchior et al. 2023 also consider the use of autoencoders to analyse galaxy spectra and look into the interpretability of latent space with results mostly focused on emission lines which is where the spectral variance fully dominates in starforming and AGN systems. The authors indeed suggest that these methods can be adopted to denoise ensembles of spectra. This work complements these previous studies turning to the more challenging case of the absorption line spectra that is commonly used to constrain the stellar population content and the past star formation history. We also consider the issue of potential biasing of the denoised data by use of synthetic spectra that are adopted as ground truth to assess the fidelity of the recovered measurements. 3.1 Butterworth Filtering The structure of the paper is as follows we give a brief presentation of the working sample in 2 along with a description of the various methods tested for denoising in 3. The comparison of retrieved and original data is shown in 4 including a comparison of synthetic data with added noise. A brief discussion of the explainability of the DL performance The Butterworth filter BF is a classical signal processing approach used to selectively attenuate unwanted frequencies within a general signal. Its maximally flat frequency response in the passband supresses signal distortion making it a popular choice in a range of domains. The squared magnitude of the frequency response H ω 2 at angular frequency ω is MNRAS 000 1 9 2025 Emergent Denoising of SDSS Galaxy Spectra 3 4 Camilleri et al. SDSS Synthetic Worthey Ottaviani 1997 and the traditional Mgb index of the Lick system Trager et al. 1998. The figure of merit is defined for each line strength measurement as follows we produce a vector with the residuals of the output and the reference for each spectrum δs Ii Ii s Ii r where s represents the output spectrum and r the reference spectrum. The mean or median of the ensemble δs I are expected to be close to zero and the standard deviation represents how well the data are recovered. Therefore we adopt the standard deviation of the residuals as our figure of merit 0.0 0.2 0.4 0.6 0.8 S/N 5 0.0 0.2 0.4 0.6 0.8 Dn 4000 NW CS FS BF NW-S NW CS FS BF NW-S H F O N 1 2 3 S/N 5 1 2 3 I p δ2 I δ I 2. 5 Also note that the reference case Ii r is the comparison spectra that can be defined in two ways it is either the noiseless best fit data O or the noisy original SDSS data N. The former allows us to quantify the denoising process whereas the latter is used to test overfitting. Fig. 2 shows the residual statistic measured at a S/N 5 as an average over all spectra with S/N for the reconstruction of an additional unseen set of real SDSS spectra left or a set of synthetic spectra right. In the case with real data we only show the data points corresponding to O as N would trivially compare the noisy data with itself. For the synthetic case we can compare the residual statistic for the ground truth case O and for a noisy realization of this ideal case with Gaussian noise that shows the same S/N as the original data N. In this framework the case N O would be indicative of a kind of overfitting. The opposite would be suggestive of true denoising. For reference the value of O for the comparison of noiseless and noisy synthetic data i.e. the variance expected by the presence of noise in the spectra is shown in each case as a horizontal dashed blue line. The performance of the different methods is comparable with the SDSS data reconstruction left panels although the BF method appears to fare worse. The more interesting results are found for the synthetic data right panels where we can discriminate between the recovery of the input noisy data N blue stars or a more desirable reconstruction of the original noiseless spectra. O red circles. One thing that stands out quite clearly is that the 4000A break strength is poorly determined by the CS method. This may be quite expected as the Dn 4000 is wider than the other two and relies on the continuum. However we performed this test as there is a well-known degeneracy between parameters so that for instance Dn 4000 indices are correlated with e.g Mgb. The strong covariance between line strengths found in Ferreras et al. 2023 indicates that the absorption line spectrum would encode similar information as the continuum. This experiment suggests that discovering and leveraging this trend is challenging for DL methods. The figure also shows an uncanny inversion of the star circle order i.e. N vs O in the BF method with respect to the other algorithms. We emphasize that this method does not use information from the ensemble and only relies on a careful filtering of high frequencies many of which would be ascribed to noise. This would be equivalent to a truncation in a Fourier series. The results presented here reveal that the BF method tends to overfit so that it produces an optimal reconstruction when using noisy data as input but underperforms when the noiseless synthetic data are considered. NW CS FS BF NW-S NW CS FS BF NW-S 0.5 1.0 1.5 2.0 2.5 3.0 S/N 5 0.5 1.0 1.5 2.0 2.5 3.0 Mgb NW CS FS BF NW-S NW CS FS BF NW-S Emergent Denoising of SDSS Galaxy Spectra 5 Dn 4000 H F Dn 4000 H F Mgb Mgb 0.0 0.1 0.2 0.3 0.4 O Noise NW CS FS BF NW-S Noise NW CS FS BF NW-S 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 O 0.0 0.4 0.8 1.2 1.25 1.50 1.75 1 2 2 3 4 10 20 10 20 10 20 0.0 0.1 0.2 0.3 0.4 N 0.4 0.8 1.2 1.6 0.0 0.5 1.0 1.5 2.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 N 0.0 0.4 0.8 1.2 1.25 1.50 1.75 Dn 4000 1 2 3 10 20 S/N g-band 10 20 S/N g-band 10 20 S/N g-band 6 Camilleri et al. 0.8 The other two indices show a more characteristic behaviour between the BF overfitting algorithm and the DL method. Note that both HδF and Mgb produce distributions closer to the ground truth with FS whereas BF closely resembles the wider shape of the histogram of noisy data. From these tests we conclude that a DL method is successful at improving the quality of galaxy spectra if a representative large ensemble with a wide range of S/N including high quality data with the same instrumental / data characteristics are available. Deep models are often considered black boxes because their mappings and decision-making processes are difficult to interpret. To better understand how spectral features are being leveraged, we employ SHAP (SHapley Additive exPlanations, Lundberg et al. 2020). SHAP is a game-theoretic method that explains model outputs by assigning each feature an importance value based on its contribution to the gap between the model’s actual prediction and its mean prediction, averaged over all possible feature subsets. Figure 7 shows the mean SHAP scores corresponding to input flux for the trained FS model. In this context, the prediction refers to the compressed latent-space encoding. While emission lines such as Hα and [NII] clearly dominate as individual features, it is notable that the importance—and therefore predictive power of the continuum is biased toward bluer wavelengths. These findings broadly agree with the negentropy-based analysis of Ferreras et al. (2023), although the variation in SHAP scores indicates that useful information is more widely distributed across the continuum. SHAP has limitations, and care must be taken when interpreting the scores. Nevertheless, the observed discrepancy highlights the value of considering alternative proxies for information content beyond variance. Entropy-based techniques and classical data-driven methods such as PCA scale variance in ways that may overlook subtle but important dependencies in spectral data. Given the SHAP dominance of emission lines, future studies may benefit from masking these features to encourage models to leverage more obscure patterns. The figure also highlights the SHAP scores associated with the red and blue regions. Although previous research has identified strong information content in the red region in terms of variance, it does not hold a notably large share of SHAP importance; the Fe and Mg features within this window do not appear to play a major role. Nonetheless, the plot indicates that their combined contribution acts as a useful signal to the FS model, offering insight into how the NW models achieve a limited ability to predict unseen spectral regions, consistent with the high degree of information entanglement across absorption spectra at many wavelengths. An important observation is that the overall shape of the SHAP curve closely mirrors the error distribution shown in Figure 4. The wavelengths that the CS model fails to represent accurately tend to exhibit strong predictive power in the FS model. This reinforces the idea that the continuum holds crucial information that poses a challenge for deep reconstruction methods, despite known correlations between the continuum and absorption or emission lines. This study highlights both opportunities and challenges in applying deep learning methods to the denoising and reconstruction of galaxy spectra. A key insight is that reproducing spectra through an information bottleneck with good generalization limits the capacity to memorize random noise. Importantly, an MAE loss appears to enhance this denoising effect. Among the tested architectures, the FS model performs best, demonstrating that unsupervised autoencoders can effectively increase the signal-to-noise ratio of an SDSS training set while generalizing to unseen spectra. Figure 8 shows a comparison between the recovered spectra (in color) and the original noisy SDSS spectra (in gray) for six representative cases. The colors correspond to evolutionary classifications—quiescent (red), star-forming (blue), and AGN (green)—following the classification of Sharbaf et al. (2023) based on nebular emission properties. The figure zooms into three spectral regions commonly used for analyzing the stellar and gaseous components of galaxies. The deep learning method extracts information from the ensemble to predict higher-S/N spectra, minimizing biases within the limits of the parent sample. Compared to traditional denoising algorithms, which require careful parameter tuning to balance smoothing with the preservation of sharp features, our approach leverages statistics learned from the ensemble in a fully emergent way. Tests with synthetic spectra suggest that classical filtering techniques often produce overly smoothed outputs that do not reliably represent the underlying spectrum. Our deep-learning framework performs better at recovering the true signal, provided that the noisy inputs are processed consistently with the training data. Unlike supervised approaches that rely on the addition of synthetic noise, our results emerge directly from real spectral data. Beyond denoising capability, the SHAP analysis provides insight into how the models utilize spectral features, highlighting differences from traditional information-theoretic techniques and reinforcing the importance of the continuum. More broadly, this work demonstrates the utility of explainability methods in astronomy. As deep learning becomes increasingly prevalent, maintaining human interpretability remains essential. Several avenues for improvement remain. The standard MAE loss used here treats all wavelengths as equally important; weighting the loss toward physically important regions could improve performance. Neural networks also exhibit a bias toward learning low-frequency signals (Rahaman et al. 2018). Multi-stage neural networks or related strategies may help represent and leverage sharp features such as emission lines. Finally, more sophisticated deep-learning architectures such as attention mechanisms (e.g., Bahdanau et al. 2014) could better capture subtle correlations found within galaxy spectra.'}]",Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model.
How can I reduce hallucinations in large language models?,2510.12850v1,,,"['2510.12850v1', '2510.13937v1', '2509.23158v1', '2510.13137v1', '2509.20913v1']","[7.0, 6.0, 6.0, 6.0, 6.0]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales']","[{'rank': 1, 'score': 7.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 2, 'score': 6.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 3, 'score': 6.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}, {'rank': 4, 'score': 6.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 5, 'score': 6.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}]",review advancements in sentiment analysis highlighting machine learning deep learning and large language models. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5.
What techniques improve Transformer training speed without losing performance?,2509.20913v1,,,"['2509.20913v1', '2510.13137v1', '2510.12850v1', '2510.14855v1', '2510.08116v1']","[7.0, 6.0, 5.0, 5.0, 5.0]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation']","[{'rank': 1, 'score': 7.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 2, 'score': 6.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 3, 'score': 5.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 4, 'score': 5.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 5, 'score': 5.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}]",Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points.
Are deep learning methods effective for crime forecasting compared to traditional models?,2509.20913v1,2509.20913v1,True,"['2509.20913v1', '2510.13050v1', '2510.11073v1', '2510.13937v1', '2510.07320v1']","[11.0, 10.0, 9.0, 9.0, 9.0]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children']","[{'rank': 1, 'score': 11.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 2, 'score': 10.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}, {'rank': 3, 'score': 9.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 4, 'score': 9.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 5, 'score': 9.0, 'id': '2510.07320v1', 'title': 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children', 'text': 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children. Autism Spectrum Disorder ASD significantly influences the communication abilities learning processes behavior and social interactions of people. Although early intervention and customized educational strategies are critical to improving outcomes there is a pivotal gap in understanding and addressing nuanced behavioral patterns and emotional identification in autistic children prior to skill development. This extended research delves into the foundational step of recognizing and mapping these patterns as a prerequisite to improving learning and soft skills. Using a longitudinal approach to monitor emotions and behaviors this study aims to establish a baseline understanding of the unique needs and challenges faced by autistic students particularly in the Information Technology domain where opportunities are markedly limited. Through a detailed analysis of behavioral trends over time we propose a targeted framework for developing applications and technical aids designed to meet these identified needs. Our research underscores the importance of a sequential and evidence-based intervention approach that prioritizes a deep understanding of each child s behavioral and emotional landscape as the basis for effective skill development. By shifting the focus toward early identification of behavioral patterns we aim to foster a more inclusive and supportive learning environment that can significantly improve the educational and developmental trajectory of children with ASD. Emotion recognition has emerged as a critical research domain in artificial intelligence with particular complexity arising in Autism Spectrum Disorder ASD contexts due to unique emotional expression characteristics in this population. Traditional Machine Learning Era Early emotion recognition systems relied on conventional methods including Support Vector Machines decision trees and random forests achieving moderate success with 75-85% accuracy on standard datasets. However these approaches fundamentally depend on handengineered features creating significant limitations when processing high-dimensional facial data particularly problematic for ASD applications where emotional expressions deviate from typical patterns. Deep Learning Revolution Convolutional Neural Networks revolutionized the field by enabling automatic feature extraction and hierarchical learning. Stateof-the-art CNN architectures consistently achieve 90-95% accuracy on neurotypical datasets FER-2013 CK+ through effective spatial hierarchy capture in facial expressions establishing CNNs as the standard approach for emotion classification. ASD-Specific Challenges Despite deep learning success in general emotion recognition ASD applications face substantial obstacles. Children with ASD exhibit significantly. Autism Spectrum Disorder ASD is a developmental condition that significantly affects a person s ability to communicate interact socially and express or interpret emotions. For children with ASD these challenges often manifest as atypical facial expressions and unusual behavioral patterns making it especially difficult for caregivers educators and even advanced computational systems to accurately recognize their emotional states. Yet effective emotion recognition is vital it underpins tailored interventions social support and the overall quality of life for children on the spectrum. Recent advances in artificial intelligence have led to the development of deep learning models particularly the Xception and Inception architectures which have shown remarkable performance in facial emotion detection. These models are highly effective in extracting and analyzing subtle features from facial images enabling more precise classification of emotional expressions higher variability in emotional expression quantified through facial expression variance motion dynamics and temporal inconsistencies. This complexity causes dramatic performance degradation with CNN accuracy dropping from 95% on neurotypical data to 70-75% on ASD-specific datasets. Transfer Learning Solutions To address small ASD dataset limitations researchers extensively explored transfer learning using pre-trained models VGG16 Res Net Inception architectures leveraging large-scale image dataset knowledge for specialized ASD tasks. These approaches demonstrate 5-10% accuracy improvements over training from scratch. Advanced architectures like Xception utilizing depthwise separable convolutions for computational efficiency and Inception V3 enabling multiscale feature extraction have gained prominence for their sophisticated feature extraction capabilities. Autoencoder Integration Autoencoders have emerged as powerful dimensionality reduction and feature extraction tools demonstrating effectiveness in filtering irrelevant features such as background noise and lighting variations. Their ability to learn compressed representations while preserving essential spatial information makes them particularly suitable for preprocessing heterogeneous datasets. Multiple studies have shown autoencoder frameworks improve model robustness in challenging environments with their capacity to standardize input data while preserving critical facial features being especially beneficial for ASD emotion recognition applications. Research Gaps and Future Directions Despite significant technological progress substantial gaps remain in ASD-specific applications. The heterogeneity within ASD populations challenges generalized recognition system development with most existing studies focusing on neurotypical datasets and limited research addressing unique ASD emotional expression characteristics. The lack of standardized ASD emotion datasets complicates comparative methodology evaluation. Contemporary trends indicate growing interest in personalized recognition systems adapting to individual expression patterns with integration of multiple preprocessing techniques and advanced CNN architectures representing promising directions for improved ASD emotion recognition accuracy. The literature establishes a clear evolutionary trajectory from traditional machine learning through deep learning advances to contemporary hybrid systems with autoencoder-enhanced preprocessing emerging as a mathematically sound approach to addressing the unique challenges of ASD emotion recognition. Fig. 1. Structure of a Auto Encorder B. Dataset The dataset comprises facial expressions of ASD children collected from clinical settings educational environments and specialized databases. Images exhibit dimensional variability with aspect ratios ranging from 0.75 to 1.33 significantly deviating from Image Net standards. C. Autoencoder Preprocessing Architecture To address the size mismatch we implement an autoencoder that maps variable-sized inputs to the required 299×299×3 format while preserving facial features essential for emotion recognition. C.1 Mathematical Formulation Encoder Maps variable input to fixed latent representation z fθ x σe Wex + be 1 Decoder Reconstructs standardized 299×299×3 output ˆx gφ z σd Wdz + bd 2 C.2 Loss Function The autoencoder employs composite loss balancing reconstruction and spatial preservation LAE T x ˆx 2 2 + λ1 V GGj T x V GGj ˆx 2 2 + λ2 Flandmark T x Flandmark ˆx 2 2 3 where T x is ground truth resized image V GGj extracts perceptual features and Flandmark preserves facial landmarks. D. Classification Models D.1 Xception Architecture Utilizes depthwise separable convolutions decomposing standard convolution into Depthwise Convolution III. Y dw i j c X m n W dw m n c Xi+m 1 j+n 1 c 4 A. Problem Statement Pointwise Convolution Xception and Inception V3 models pre-trained on Image Net dataset require fixed input dimensions of 299×299×3 pixels.ASD children s facial expression datasets contain variablesized images 150×200 to 800×600 pixels creating a fundamental mismatch that degrades model performance through aspect ratio distortion and information loss. c W pw c k Y dw i j c 5 Yi j k X This reduces computational complexity from O H W C K2 F 6a F. Experimental Setup to O H W C K2 + F 6b F.1 Training Protocol The training process employs a two-stage strategy to optimize both pre-processing and classification components. During the first stage the autoencoder undergoes pretraining for 100 epochs with a batch size of 32 learning to map variable-sized inputs to standardized 299×299×3 outputs while preserving essential facial features. The second stage involves end-to-end fine-tuning where the complete pipeline undergoes joint optimization allowing the autoencoder and classification models to adapt collaboratively for optimal emotion recognition performance. F.2 Comparative Analysis The effectiveness of the proposed approach is evaluated through rigorous comparison between baseline and enhanced methodologies. The baseline approach applies direct resizing transformation xbaseline resize x 299 299 to convert variable-sized images to the required input dimensions. In contrast the enhanced approach utilizes autoencoder preprocessing xenhanced gφ fθ x to generate standardized inputs that preserve spatial relationships and semantic content. Performance evaluation encompasses accuracy precision recall and F1-score metrics across both approaches with statistical significance assessed through paired t-tests to validate improvement claims. F.3 Implementation Details The experimental implementation utilizes NVIDIA V100 32GB GPU hardware to handle the computational demands of training both autoencoder and classification components. The framework employs TensorFlow/Keras for model implementation and training orchestration. Data augmentation strategies include random rotation within 15 degrees and brightness/contrast adjustments within 0.2 range to improve model generalization. Training stability is maintained through early stopping mechanisms that monitor validation loss plateaus preventing overfitting while ensuring optimal convergence. This comprehensive methodology systematically addresses the fundamental dimensional mismatch between variable-sized ASD emotion datasets and the fixed-input requirements of Image Net-trained deep learning models. The approach enables improved classification accuracy through learned preprocessing techniques while preserving the semantic facial features essential for accurate emotion recognition in children with autism spectrum disorders. Fig. 2. Xception Model Structure D.2 Inception V3 Architecture Employs multi-scale feature extraction through parallel convolutions Finception Concat F1×1 F3×3 F5×5 Fpool 7 Factorized convolutions improve efficiency 5 × 5 convolutions replaced by two 3 × 3 operations and asymmetric factorization n × n 1 × n + n × 1. Fig. 3. Inception V3 Model Structure E. Training Framework E.1 Loss Functions Classification Loss Cross-entropy for 4-class emotion recognition N X 4 X LCE 1 IV. RESULTS AND DISCUSSION j 1 yi j log pi j 8 N The experimental evaluation was conducted to assess the effectiveness of autoencoder preprocessing on emotion recognition performance using ASD children datasets. Two state-ofthe-art deep learning architectures Xception and Inception V3 were evaluated under both baseline conditions without preprocessing and enhanced conditions with autoencoder preprocessing. The results demonstrate substantial and statistically significant improvements across all performance metrics. i 1 Total Loss Ltotal LAE + αLCE E.2 Optimization Adam optimizer with adaptive moments to address the loss function regarding classification loss mt β1mt 1 + 1 β1 gt vt β2vt 1 + 1 β2 g2 t θt θt 1 α vt + ε ˆmt 9 A. Performance Comparison Analysis Table I presents the comprehensive performance comparison between baseline and autoencoder-enhanced models. The reParameters α 0.001 β1 0.9 β2 0.999 ε 10 8. C. Error Rate Reduction Analysis sults reveal consistent and substantial improvements across both architectures when autoencoder preprocessing was integrated into the pipeline. A particularly noteworthy finding was the substantial reduction in classification errors. The Xception model achieved a 48.0% reduction in error rate from 27.7% to 14.4% while Inception V3 demonstrated a 44.1% reduction from 29.0% to 16.2%. TABLE I PERFORMANCE COMPARISON BETWEEN XCEPTION AND INCEPTIONV3 MODELS WITH AND WITHOUT AUTOENCORDERS Model Xception Inception V3 Accuracy Baseline 72.3% 71.0% Accuracy Autoencoder 85.6% 83.8% Precision 0.82 0.81 Recall 0.84 0.82 F1-Score 0.83 0.82 The Xception model achieved the highest overall performance with 85.6% accuracy after autoencoder preprocessing representing a 13.3 percentage point improvement over the baseline 72.3%. Similarly Inception V3 demonstrated significant enhancement reaching 83.8% accuracy compared to the baseline performance of 71.0% corresponding to a 12.8 percentage point improvement. Fig. 5. Statistical significance analysis showing very large effect sizes and substantial error rate reductions. These reductions represent significant practical improvements in model reliability for emotion recognition in ASD children. D. Confidence Interval Analysis The 95% confidence intervals for accuracy improvements provide robust bounds on the true performance enhancements. For Xception the confidence interval 10.6% 16.0% and for Inception V3 10.1% 15.5% both exclude zero with substantial margins confirming the reliability of the observed improvements. E. Consistency Across Architectures Fig. 4. Performance Comparison Baseline vs Autoencoder-Enhanced Models. Table III presents the comprehensive analysis of improvement consistency across different neural network architectures demonstrating the robustness of the autoencoder preprocessing approach. B. Statistical Significance Evaluation The statistical rigor of the observed improvements was validated through multiple complementary analyses. Table II summarizes the comprehensive statistical evaluation confirming the significance and magnitude of the performance enhancements. TABLE III COMPREHENSIVE PERFORMANCE IMPROVEMENT ANALYSIS Metric Xception Inception V3 TABLE II STATISTICAL SIGNIFICANCE ANALYSIS OF MODEL IMPROVEMENTS Mean Improvement 13.3% 12.8% Standard Deviation 0.004 0.004 Coefficient of Variation % 2.7 2.7 Effect Size Cohen s d 2.66 2.56 Statistical Power % 99.9 99.9 NNT samples 7.5 7.8 Model Accuracy Improvement % Error Reduction % Xception 13.3 18.4 2.66 Very Large 0.001 10.6 16.0 48.0 Inception V3 12.8 18.0 2.56 Very Large 0.001 10.1 15.5 44.1 Relative Improvement % Cohen s d Effect Size p-value CI Lower % CI Upper % The Cohen s d values of 2.66 and 2.56 for Xception and Inception V3 respectively indicate very large effect sizes substantially exceeding Cohen s threshold for large effects d 0.8. The p-values 0.001 demonstrate highly significant differences with statistical power exceeding 99.9% for both models. The extremely low coefficient of variation 2.7% indicates remarkable consistency in improvement magnitude across different architectures suggesting that the benefits of autoencoder preprocessing are architecture-independent. F. Magnitude and Significance of Improvements emotion classification is achieved compared to the baseline system. This represents significant practical value in clinical and educational settings where accurate emotion recognition is crucial for effective intervention strategies. The 95% confidence intervals provide clinicians and researchers with reliable bounds on expected performance improvements. Even the lower confidence bounds 10.1-10.6% substantially exceed typical minimum clinically important differences 2-5% for machine learning applications in healthcare. The experimental results provide compelling evidence for the effectiveness of autoencoder preprocessing in enhancing emotion recognition performance for ASD children. The observed improvements of 13.3% Xception and 12.8% Inception V3 in accuracy represent substantial enhancements that significantly exceed typical performance variations in deep learning models. The Cohen s d effect sizes 2.66 and 2.56 are exceptionally large by conventional standards placing these improvements among the most substantial reported in emotion recognition literature. These effect sizes indicate that the differences between baseline and enhanced models are not only statistically significant but represent practically meaningful improvements with real-world implications for ASD emotion recognition systems. J. Computational Efficiency Considerations The cost-benefit analysis reveals an exceptional efficiency ratio of 9 1 where the 10-20% increase in computational overhead from autoencoder preprocessing yields an 18% improvement in accuracy. This favorable trade-off makes the approach highly practical for real-world deployment in resourceconstrained environments. The preprocessing standardization also contributes to improved system reliability by ensuring consistent input formatting reducing variability due to image quality differences lighting conditions and background clutter that commonly affect emotion recognition systems in natural settings. G. Statistical Robustness and Reliability The statistical analysis demonstrates overwhelming evidence for the significance of the observed improvements. The p-values 0.001 indicate that the probability of observing such large improvements by chance alone is less than 0.1% providing strong evidence against the null hypothesis of no improvement. The Mc Nemar s test results χ2 98.01 p 0.001 further confirm the statistical significance when comparing the paired predictions of baseline versus enhanced models. This non-parametric test is particularly appropriate for comparing the accuracy of two classification models on the same dataset providing robust validation of the improvements. The high statistical power 99.9% ensures that the study design was capable of detecting meaningful differences eliminating concerns about insufficient sample sizes or inadequate experimental design. This study rigorously evaluated the impact of autoencoderbased preprocessing on emotion recognition systems for children with Autism Spectrum Disorder ASD leveraging two advanced convolutional neural network architectures Xception and Inception V3. The integration of autoencoders as a preprocessing step resulted in substantial and consistent improvements across all key performance metrics including accuracy precision recall and F1-score. Quantitatively the observed effect sizes Cohen s d 2.5 absolute accuracy increases of over 13 percentage points and error rate reductions approaching 50% collectively underscore a practical and statistically significant enhancement in model performance. H. Mechanistic Understanding of Enhancement The consistent improvement pattern across different architectures coefficient of variation 2.7% suggests that autoencoder preprocessing addresses fundamental challenges in emotion recognition rather than architecture-specific limitations. The mathematical model of improvement A. Generalizability and Robustness Enhanced Accuracy Baseline Accuracy + δ 10 A major strength of these findings lies in their demonstrated generalizability. The consistency of improvements across two distinctly different architectures Xception s depthwise separable convolutions and Inception V3 s multi-scale processing offers compelling evidence that the benefits of autoencoder preprocessing extend beyond the idiosyncrasies of specific models. The low variance in accuracy improvement standard deviation 0.004 further implies that these enhancements are robust and likely applicable to a diverse range of convolutional neural network structures. The additive mathematical model supporting these results suggests that autoencoder preprocessing resolves universal challenges inherent to emotion recognition rather than exploiting architecturedependent features indicating strong potential for broad applicability including domains beyond ASD. where δ 0.131 0.004 provides a quantitative framework for understanding the additive nature of the enhancement. The substantial error rate reductions 44 48% indicate that autoencoder preprocessing effectively filters noise and irrelevant background information allowing the deep learning models to focus on critical facial features essential for emotion recognition. This noise reduction mechanism is consistent with the theoretical framework of autoencoders as optimal lossy compression algorithms that preserve task-relevant information while discarding extraneous details. I. Practical Implications for ASD Applications The Number Needed to Treat NNT values of 7.5-7.8 samples indicate that for approximately every 8 children processed through the enhanced system one additional correct B. Comparison with Literature Y. Zhang A better autoencoder for image Convolutional autoencoder. The improvement metrics registered in this study particularly the exceptionally large effect sizes are among the highest reported to date in emotion recognition research. Most comparable works report only modest gains Cohen s d 0.2 0.5 whereas the present work achieved much greater impact. Furthermore the cross-architecture validation available here addresses a common gap in the literature where such findings are typically reported for a single deep learning model. This strengthens the external validity and distinguishes this work in the field. VI. FUTURE WORK Despite the strong results several limitations warrant attention. The statistical analyses are based on assumptions regarding data distribution and sample size thus confirmatory studies utilizing larger and more heterogeneous datasets are recommended to further substantiate these findings. Although the study included two prominent neural network architectures investigating additional frameworks such as Vision Transformers and Res Net families would further confirm the generalizability and scalability of the approach. Future research should also explore the customization and optimization of autoencoder architectures specific to emotion recognition with the presented quantitative model especially the improvement parameter δ offering a solid starting point for such endeavors. A. Clinical Translation Potential The magnitude and consistency of the enhancements combined with high computational efficiency and rigorous statistical validation suggest strong potential for clinical translation. The consistent and predictable gains across architectures minimal additional computational cost and robust statistical performance collectively lower the risk for clinical implementation. If deployed the method could meaningfully improve emotion recognition support systems for children with ASD with significant implications for intervention effectiveness and broader applications in affective computing. In summary autoencoderbased preprocessing constitutes a transformative strategy for elevating the accuracy reliability and robustness of emotion recognition models in challenging real-world scenarios. The clear statistically robust improvements from this work provide a methodological and practical foundation for further research and real-world deployment in healthcare and other emotionsensitive domains.'}]",Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks.
"Should I train separate models for different crime types, or combine them?",2510.14855v1,2509.20913v1,False,"['2510.14855v1', '2509.20913v1', '2510.13050v1', '2510.13937v1', '2509.23158v1']","[9.0, 8.0, 7.0, 7.0, 7.0]","['A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization']","[{'rank': 1, 'score': 9.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 2, 'score': 8.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 7.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}, {'rank': 4, 'score': 7.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 5, 'score': 7.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}]",Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences.
Which deep learning approaches work well for gamma/hadron separation?,2510.14855v1,2510.05736v1,False,"['2510.14855v1', '2510.13137v1', '2510.12758v1', '2510.12850v1', '2510.05736v1']","[7.0, 7.0, 7.0, 7.0, 7.0]","['A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes']","[{'rank': 1, 'score': 7.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 2, 'score': 7.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 3, 'score': 7.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}, {'rank': 4, 'score': 7.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 5, 'score': 7.0, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes. The identification of γ-rays from the predominant hadronic-background is a key aspect in their ground-based detection using Imaging Atmospheric Cherenkov Telescopes IACTs. While current methods are limited in their ability to exploit correlations in complex data deep learning-based models offer a promising alternative by directly leveraging image-level information. However several challenges involving the robustness and applicability of such models remain. Designing model architectures with inductive biases relevant for the task can help mitigate the problem. Three such deep learning-based models are proposed trained and evaluated on simulated data 1 a hybrid convolutional and graph neural network model CNN-GNN using both image and graph data 2 an enhanced CNN-GNN variant that incorporates additional reconstructed information within the graph construction and 3 a graph neural network GNN model using image moments serving as a baseline. The new combined convolution and graph-based approach demonstrates improved performance over traditional methods and the inclusion of reconstructed information offers further potential in generalization capabilities on real observational data. The field of very-high-energy VHE γ-ray astronomy has evolved significantly over the past three decades driven largely due to observations from ground-based imaging telescopes. These Imaging Atmospheric Cherenkov Telescopes IACTs capture Cherenkov light produced by highly energetic particles interacting with the Earth s atmosphere. Each camera image represents a two-dimensional projection of the Cherenkov light pool from the telescope s position and each event typically consists of multiple images taken simultaneously across an array of telescopes. This stereoscopic information is used to reconstruct the energy direction and type of the primary particle initiating the air shower. Event statistics however are dominated by hadron-induced air showers which can outnumber γ-ray air showers by up to a factor of 104. This makes the task of identifying γ-ray events from the hadronic background a central challenge for IACT-based observations. Hadronic air showers are fundamentally different from those initiated by γ-rays which are electromagnetic in nature. This difference in shower development leads to small but significant differences in the camera images which form the basis for separating the two event classes. Currentgeneration IACTs typically rely on Boosted Decision Trees BDTs trained on parameterized image features or goodness-of-fit parameters for this task 3 5. Consequently a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Multiple studies have explored deep learning methods for identifying γ-rays and demonstrated exceptional performance on simulated data 6 10. Most model architectures use convolutional neural networks CNNs for extracting information from camera images and recurrent neural networks RNNs for its aggregation across an event. Graph neural networks GNNs applied on images represented as point clouds have also been established as a viable approach for the same task. Despite their potential a complete deployment of deep learning-based models on IACT data remains non-trivial due to a variety of issues ranging from observational systematics to discrepancies between simulations and real-world data. The construction of models that can generalize to unseen situations is a long-standing problem in deep learning. The use of network architectures with inductive biases suitable for the given task can lead to improved generalizations by guiding the learning process towards more physically meaningful representations. For example translational invariance in CNNs temporal dependence in RNNs and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to thus contributing to their success in their domains. In the context of IACTs as each image is a projection of the same event there is no inherent ordering between them making them permutation equivariant. This motivates a shift towards exploring GNNs for aggregating information across multiple telescopes. To that end this work introduces a combined convolutional and graph neural network CNN-GNN based approach for γ/hadron separation in IACTs. Three models with two distinct training strategies are proposed and evaluated on simulated data. These include a CNN-GNN model trained on image and graph data an enhanced CNN-GNN variant with additional reconstructed event information incorporated into the graph structure and a baseline GNN model utilizing image moments serving as a reference for existing methods. The models were trained on simulations of the High Energy Stereoscopic System H.E.S.S. located in Go llschau Namibia. The H.E.S.S. array consists of four 12-meter telescopes CT1-4 arranged in a square of side 120 meters with an additional 28-meter telescope CT5 at its center. Diffuse proton and γ-ray events simulated at a zenith of 20 and with a maximum view cone of 5 were chosen as the two event classes for this task. Similar models were also trained on the simulations of the upcoming Cherenkov Telescope Array Observatory the results of which are not presented here. A custom data processing pipeline based on ctapipe and Py Torch was developed for the subsequent analysis of IACT data. The framework within the ctapipe v0.19.3 package was used to handle the low-level data processing tasks such as image calibration and cleaning while the construction and training of the convolution and graph-based models were implemented using Py Torch v2.0.1 and Py Torch Geometric v2.4.0 respectively. γ-ray or hadron-induced based on the event images and information. To this end the aim of the convolutional CNN half of the model remains to extract relevant image-level features from each telescope while the graph-based GNN half is given additional contextual information described in Section 3 to better learn the classification task. The output of the CNN also forms part of the input to the GNN allowing the model to combine image features with telescope-level information during training. A key advantage of the graph representation in GNNs is its flexibility in handling varying numbers and types of telescopes present in IACT arrays. This is also relevant for when subsets of the array operate due to technical or observational constraints. Additionally the graph approach also allows for inclusion of telescope-specific information such as position reconstructed parameters temporal relationships making the system more adaptable to the particularities of real-world data. The CNN component is a simple implementation inspired from the Inception module employing convolution filters of multiple sizes in parallel to learn features at different spatial scales. Five convolutions with kernel sizes ranging from 1 to 15 are applied to each image and the output subsequently passed through two additional convolutional layers with max-pooling. The result is then flattened and processed through two fully connected layers with dropout regularization applied before the final layer. The network operates on images from each telescope independently and returns a fixed-dimensional embedding for each telescope. The feature vectors extracted by the CNN are subsequently passed to the GNN component which consists of a multi-layer Edge Conv architecture with residual concatenation across layers following the original design. After message passing the learned node representations are aggregated globally using a combination of mean max and sum pooling operations to form a graph-level event embedding which is then used for classification. Alternative GNN layers such as GCNConv and GATConv were also explored but no significant difference in performance was observed. A key aim of the work was to examine if the inclusion of reconstructed information in the model training contributed to improvements in performance. Consequently three models with different GNN components were developed a Fast CNN-GNN a Strong GNN and a Hillas GNN. The specifics followFast CNN-GNN Combines features extracted from camera images with telescope positions in the ground frame total image and maximum pixel intensity per image. Strong CNN-GNN An enhanced version which incorporates the reconstructed core position of the air shower. This is implemented by shifting the telescope positions in the tilted ground frame1 by centering the origin at the reconstructed impact point Figure 2. Hillas GNN A simplified GNN trained solely on Hillas parameters extracted from the camera images serving as the baseline model. 1The tilted ground frame is the ground frame transformed according to the telescope s pointing direction and is used for reconstructing the shower core position. corresponding to the area under curve AUC values in each case. All models are successfully able to learn to classify events between γ-rays and protons and the CNN-GNN models outperform the Hillas-based GNN model as well. Upon testing on a dataset with an additional local distance cut on images as in the CNN-GNN approach matches the classification performance of the previous models. Similar results were also seen when the same approach was applied and evaluated on simulations of the CTAO array. where a higher Δ AUC fscore indicates a greater importance of that feature to the model s decision process. The results of this computation for the Hillas GNN n 50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies.'}]","ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution."
What frameworks and optimization strategies were used to train DPCformer?,2509.23158v1,2510.08662v1,False,"['2509.23158v1', '2510.11073v1', '2510.14855v1', '2510.13137v1', '2510.08116v1']","[8.0, 8.0, 7.0, 7.0, 7.0]","['Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation']","[{'rank': 1, 'score': 8.0, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies.'}, {'rank': 2, 'score': 8.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 3, 'score': 7.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 4, 'score': 7.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 5, 'score': 7.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}]",The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models.
How do modern architectures perform on complex video tasks compared to older methods?,2509.20913v1,2510.09187v1,False,"['2509.20913v1', '2510.11073v1', '2510.13137v1', '2510.08116v1', '2510.10822v1']","[10.0, 10.0, 9.0, 9.0, 9.0]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis']","[{'rank': 1, 'score': 10.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 2, 'score': 10.0, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and high agreement κ 0.90 across eleven eye diseases in three cohorts anonymizing over 95% of images. ROFI works with AI systems maintaining original diagnoses κ 0.80 and supports secure image reversal over 98% similarity enabling audits and long-term care. These results show ROFI s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis 1 3 medical research as well as artificial intelligence AI-aided disease diagnosis 6 15. Medical images account for over 90% of all medical data and grow by more than 30% annually. However the collection of personal medical images also increases the risk of privacy breaches 18 26. Thus the development of anonymization methods 27 33 is increasingly critical aiming to remove the personal identifiable information from the images. Among all medical image types the facial images draw particular attentions 34 36 as it includes rich biometric identifying information and is widely adopted in access control. In ophthalmology facial images play a more prominent role than many other medical specialties 39 45 particularly for diagnosing external eye conditions like strabismus ptosis and thyroid eye disease TED. These eye diseases manifest through visible signs within the periocular areas and the related facial tissues 46 49. Traditional anonymization techniques such as cropping out the eye blurring or applying mosaics to faces are insufficient since advanced face recognition systems 53 55 can still identify individuals from these altered images 56 58. Artificial Intelligence Generated Content AIGC -based methods 59 64 could further distort the identity but at the cost of obscuring local details critical for diagnosis. Face swapping techniques 65 70 replace original facial features with those of another individual such as a celebrity s. Similarly face de-identification methods 72 78 transform the face into a totally virtual one. While protecting privacy the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed there are few preliminary efforts on this which adopt hand-crafted clinical features such as facial morphology 81 83 parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of diseases such as ptosis but are incapable of handling many other complex conditions. For instance conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally the above methods are not reversible limiting their ability to retrieve personal medical records for decision-making and medical audits. In this article we introduce ROFI Figure 1a b the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs 1 Data-Driven Ophthalmic Sign Detection Using weakly-supervised learning techniques given a large-scale set of patient faces annotated with binary labels whether or not they have eye disease a neural network automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. 2 Reversible Neural Identity Translation Leveraging the flexible feature translation capability of the Transformers we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate reconstruction of original images when being authorized. Compared to previous approaches our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI we conducted a comprehensive study across three clinical centers Figure 1c enrolling 17 181 patients from Shanghai Ninth People s Hospital SNPH 493 patients from Eye Center of Xiangya Hospital of Central South University ECXHCSU and 79 patients from Renmin Hospital of Wuhan University RHWU. We evaluated the clinical usability of ROFI in two key aspects 1 the completeness of ophthalmic signs and 2 the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then we explored the usability of ROFI-anonymized images for medical AI models. Further we assessed the facial anonymization capability with modern face recognition systems. Finally we evaluated the similarity between the reversibly reconstructed images and the originals and utilized the reconstructed images to retrieve historical medical records assessing the efficacy of hormone therapy for thyroid eye disease TED. 2 2.1 Cohort Building For the development and evaluation of ROFI we included 17181 patients who had undergone ophthalmologic examinations and had facial images captured at three hospitals in China between January 1 2018 and December 25 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People s Hospital SNPH which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria 1 Had at least one facial image available that was taken within the study period. 2 Were diagnosed with ophthalmic diseases characterized by external manifestations. 3 Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if 1 Their facial images were of insufficient quality such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12 289 patients. We randomly divided the SNPH cohort into three subsets 11 836 for developing 222 for model-selection and 231 for internal validation. For external validation two additional cohorts were established ECXHCSU and RHWU with 246 and 48 patients respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs thereby reducing the physicians workload. For the validation sets images were annotated with the corresponding ophthalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki. Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently written informed consent was obtained from their parents or legally authorized representatives. Participation in the prospective study at the ROFI Program was voluntary and all individuals or their legal representatives were fully informed about the nature purpose potential risks and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs Figure 1a. Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features we introduced a learnable ophthalmic sign detector which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process we employ a Transformer -based feature enhancement network called DA-Former to refine these features finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set which is labeled with binary health state classifications healthy or not with a weakly-supervised region-score-max learning strategy. Subsequently the neural identity translator and DA Transformer are jointly optimized ensuring the generated images are well-protected highly-realistic and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance the typical symptom of ptosis is the drooping of the upper eyelid. Ocular melanoma is typically associated with the conjunctiva and some malignant tumors can lead to pupil abnormalities. In this 3 section we have chosen the following typical eye signs that are relevant to most eye diseases eyelid shape iris shape conjunctival disorder Conj-Dis lacrimal apparatus disorder LA-Dis eyelid disorder Eyelid-Dis and iris disorder Iris-Dis. The former two shapes were quantified using eyelid/iris keypoints which were automatically detected by ocular keypoint detection models. The latter four disorders were assessed manually by ophthalmologists with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI we compared it with five representative facial privacy protection methods the conventional approach that applies mosaic to the image Mosaic using state-of-the-art AI generation model to regenerate facial content AIGC famous face swapping software Sim Swap Face Swap the method specifically designed for ophthalmic use Digital Mask and the state-of-the-art face de-identification method G2Face. The comparison of these methods is given in the supplementary and WUPH validation sets respectively Figure 3b and Supplementary Table 4. Given that sensitivity measures the ratio of detected positive samples to all positive samples our approach detected all patients with eye disease and reminded them to go to the hospital while all other approaches failed. For example on WUPH the sensitivity achieved by our approach 100% 95% CI 100%100% significantly outperformed Mosaic 85.00% 95% CI 74.36%-95.00% AIGC 77.50% 95% CI 64.28%-89.74% Face Swap 97.50% 95% CI 91.89%-100.00% Digital Mask 85.00% 95% CI 72.97%-95.12% and G2Face 85.00% 95% CI 74.27%-95.12%. All other methods missed a considerable number of positive cases which could potentially delay diagnosis and treatment. In contrast ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task we evaluated the medical outcomes of images protected by different methods using Cohen s Kappa κ values. Our method achieved κ 0.81 across all three validation sets for all eye diseases Figure 3c and Supplementary Table 5 indicating excellent clinical agreement between the results obtained from the ROFI-protected and the original images. For instance on SNPH ROFI obtained κ values of 0.9594 95% CI 0.9033-1.0154 for BCC 0.9046 95% CI 0.7735-1.0357 for CM 0.8732 95% CI 0.7318-1.0147 for CL 1.0 95% CI 1.0-1.0 for SCC and similar high values for strabismus ptosis and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement ROFI can be effectively deployed for remote clinical diagnosis application without obviously compromising clinical outcomes. In contrast Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis which can be assessed with hand-crafted features such as eyelid shape but it failed with other diseases such as BCC and CM achieving poor κ values of 0.0712 95% CI -0.0986-0.2409 and 0.0993 95% CI -0.1400-0.3386 for these two diseases on ECXHCSU respectively. This was far below our approach which achieved κ 0.9692 95% CI 0.9091-1.0294 and κ 1.0 95% CI 1.0-1.0 for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic AIGC and Digital Mask but still did not reach κ 0.81 for any eye disease suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic TED congenital e.g. ptosis and microblepharon corneal CL and tumor e.g. BCC SCC and CM eye diseases. Besides ROFI achieved a Matthews Correlation Coefficient MCC value of 0.9450 with 95% CI 0.8684-1.0000 on WUPH which is also much better than G2Face 0.5157 with 95% CI 0.3705-0.6895 Digital Mask 0.4960 with 95% CI 0.3390-0.6422 Face Swap 0.6501 with 95% CI 0.5142-0.8318 AIGC 0.1513 with 95% CI 0.0016-0.3394 and Mosaic 0.1604 with 95% CI 0.0449-0.3442 Supplementary Table 6 and Supplementary Figure 4 further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases Figure 3d Supplementary Figure 5 and Supplementary Figure 6. For example on WUPH Face Swap and G2Face excelled with BCC and ptosis but faltered on TED while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast our approach maintained well performance across all disease categories. Additionally we compare ROFI with other approaches on two other tasks namely facial landmark detection and tumor segmentation. For the facial landmark detection we evaluate with the largescale Celeb A-HQ dataset. We adopt the MTCNN to automatically detect the landmarks of the original and the protected images and quantify the performance with mean squared error. For the tumor segmentation task we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma BCC instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7 our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely the error of landmarks detected from the ROFIprotected images is 2.9871 which is much lower than Mosaic 5.6799 AIGC 7.8945 Face Swap 4.8921 Digital Mask 6.3891 and G2Face 3.7130. The accurate landmark preservation capability of our approach also implies that it could be potentially deployed to other medical conditions. For instance precise facial landmark detection is crucial for analyzing facial symmetry a key clinical indicator in diagnosing conditions such as facial palsy or Bell s palsy. To evaluate the capability of our method in fine-grained disease detection we collaborated with board-certified physicians to annotate the tumor regions in BCC Basal Cell Carcinoma instances within the SNPH validation set. As shown in Supplementary Table 8 our approach achieves a Dice coefficient of 0.7389 5 largely outperforming previous methods such as G2Face 0.5782 and Face Swap 0.2783. This substantial improvement demonstrates that our method is not only effective for coarse-level disease classification but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence AI 6 10 models are being developed to predict patient health conditions from medical images with some now deployed in real-world applications e.g. video-based disease screening 109 118. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this we divided the SNPH and ECXHCSU datasets into training and test subsets WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures the classical convolutional neural network CNN model Res Net50 and a recently-popular Transformer-based model Vi T. Then we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach ROFI achieved excellent consistency with results obtained from original images as indicated by the highest Cohen s Kappa values κ among all privacy protection methods. For instance on SNPH using the Res Net50 diagnosis model ROFI achieved a κ value of 0.9077 95% CI 0.8569 0.9586 significantly outperforming the state-of-the-art facial privacy protection method G2Face κ 0.7257 95% CI 0.6454 0.8060 Figure 4a. Similarly with the Vi T diagnosis model ROFI demonstrated significantly higher κ values compared to other approaches Figure 4b. Our approach achieved an Area Under the Receiver Operating Characteristic curve AUROC of 0.9113 95% CI 0.8952-0.9113 on SNPH when being evaluated using the classic Res Net50 remarkably outperforming Mosaic AUROC 0.6102 95% CI 0.5941-0.6262 AIGC AUROC 0.7438 95% CI 0.7262-0.7614 Face Swap AUROC 0.7983 95% CI 0.7781-0.8186 Digital Mask AUROC 0.7853 95% CI 0.7816-0.7891 and G2Face AUROC 0.8776 95% CI 0.8604-0.8949 Figure 4c. Using the Vi T our method attained AUROCs of 0.9124 95% CI 0.9059-0.9190 on SNPH and 0.7894 95% CI 0.7715-0.8072 on ECXHCSU significantly outperforming other approaches Figure 4c. Furthermore we compared the ROC curves and per-disease AUROCs of our approach with other approaches Figure 4d Supplementary Figure 7 Supplementary Figure 8 and Supplementary Figure 9. Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs our method either matched or exceeded the performance of the other two approaches. For instance for the Vi T diagnostic model on SNPH for BCC our method achieved an AUROC of 0.950 compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that in some settings our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises because our protected images enhance the representation of eye disease-related features while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supplementary Table 9 increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless our ROFI still demonstrates a significant advantage achieving an AUROC of 94.59% substantially outperforming the second-best method G2Face which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy we conducted an investigation into its performance on evading face recognition systems. In a typical scenario a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms AdaCos and Arc Face. When using the Ada Cos ROFI protected 96.54% of images surpassing Mosaic 90.91% AIGC 93.08% Face Swap 74.90% Digital Mask 94.81% and G2Face 93.51% Figure 5b. With Arc Face ROFI maintained strong protection rates Figure 5b. 6 Furthermore cropping the eye region a traditional method for protecting privacy in ophthalmology provides insufficient protection. With the Arc Face face recognition method this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes respectively compared to 94.81% with our method Figure 5c. This finding underscores the urgent need for a novel patient privacy protection method as the current unsafe eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI s performance we compare it against standard pixel-wise and feature-wise Differential Privacy DP methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate 3.0% comparable to that of ROFI. The results Supplementary Table 13 show that while providing a similar level of empirical privacy ROFI maintains a significantly higher diagnostic utility 91.25% AUROC compared to both pixel-wise DP 62.32% AUROC and feature-wise DP 76.69% AUROC on SNPH validation set with Vi Tbased diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation we adopted a state-of-the-art membership inference attack framework based on shadow modeling. We used this framework to calibrate the featurewise DP method to a comparable level of empirical privacy as ROFI specifically a True Positive Rate TPR of approximately 1.4% at a 1% False Positive Rate FPR. The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk the diagnostic performance of the DP method dropped to 61.41% AUROC while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials supplementary Table 10 a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods H 21.69 P 0.0006. However we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods Digital Mask and G2Face. Consequently to validate our findings with greater statistical power we conducted an additional 1 000 trials focusing specifically on these three top-performing methods. The expanded results supplementary Table 10 showed that ROFI achieved a recognition rate of only 1.1% 14/1200 demonstrating a substantially stronger performance than both Digital Mask at 3.5% 42/1200 and G2Face at 3.1% 38/1200. A one-tailed Fisher s Exact Test confirmed that ROFI s superiority is highly statistically significant when compared to both Digital Mask P 0.000099 and G2Face P 0.000529. These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key the original image can be accurately reconstructed from the ROFI image. Compared to G2Face the only reversible method examined our approach achieved superior reversed ID similarity 97.19% and image similarity 98.47% over G2Face s 74.72% and 93.48% respectively Figure 5d. The reversed image enabled reliable traceability for medical audits as well as facilitating the precise retrieval of personalized medical records thereby enhancing longitudinal examinations. Finally we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image Figure 5e. In the Single scenario where no historical information was utilized the κ value obtained was 0.2758 95% CI -0.0860-0.6378. Using the general reversible privacy protection technique G2Face κ was only marginally improved to 0.3913 95% CI 0.09560.6869 due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images including alterations in skin color and increased blurriness Figure 5f impeded accurate retrieval of historical data resulting in a less reliable comparison with the current image. In contrast our method ROFI yielded the highest κ value of 0.8888 95% CI 0.7393-1.0384 attributable to its superior reconstruction performance. In this article we introduced developed and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis while simultaneously obscuring identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection fueled with a large-scale real ophthalmic patient dataset ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency Cohen s kappa κ 0.81 achieved by ophthalmologists when utilizing ROFI-protected images compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features our approach significantly outperformed them particularly for diseases that rely on discriminating local subtle cues such as Basal Cell Carcinoma Conjunctival Melanoma and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions it avoided introducing additional privacy risks even with the retention of more complete ophthalmic features. With different private keys ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14 varying key leads to a substantial pixel-wise deviation σpixel 36.52 indicating that the generated images are visually distinct. Moreover it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10 the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces but actively obfuscates them within a broad distribution of possible outputs significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First regarding privacy protection conventional eye cropping may expose more identifiable cues such as periorbital skin texture and iris details leading to only a 70% protection rate which is significantly lower than the 95% efficacy achieved by ROFI as demonstrated in Figure 5 C. In terms of iris recognition attacks ROFI remains effective as it preserves disease-relevant information such as overall iris shape and color while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy we used the open-source iris recognition algorithm Open Iris. The original images or those processed by eye cropping had a 74.45% recognition success rate highlighting the vulnerability of standard facial images to iris-based identification. In contrast ROFI-protected images achieved only a 0.87% success rate indicating that identity-related iris features are effectively obscured by our method. Second regarding disease diagnosis eye cropping removes surrounding facial features entirely while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations thereby supporting more comprehensive medical assessments. For instance in patients with Bell s palsy our ROFI framework enables accurate detection of facial landmarks which can be used to assess facial asymmetry a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases certain conditions such as squamous cell carcinoma SCC can present with symptoms extending beyond the eye region potentially involving the forehead or cheeks. Certain specific full-face features such as hypothyroid facies are also helpful for diagnosing thyroid eye disease TED. Our ROFI framework retains these extraocular signs whereas eye cropping discards them entirely potentially delaying timely medical follow-up. Finally we mention that ROFI is not contradictory to eye cropping. Rather ROFI is fully compatible with subsequent eye cropping offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These models are adopted in the preliminary screening of diseases significantly conserving physician resources while broadening the scope of early disease detection. Moreover given that most AI models are deployed on remote cloud platforms owing to their high computational demands and reliance on GPU clusters there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFIprotected images exhibit substantial consistency κ 0.81 with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI s decision-making process as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care such as monitoring chronic conditions like Thyroid Eye Disease clinicians must compare current images to a historical baseline. ROFI s key-based reversal restores this crucial longitudinal link which is lost in irreversible methods. Additionally reversibility is essential for medical and legal auditing. It provides a secure digital fingerprint that ensures traceability and accountability allowing auditors to verify that a diagnosis corresponds to the correct patient record thus maintaining the integrity of clinical workflows. Traceability was critical for maintaining accurate medical records and auditing the clinical treatment procedure aligning with the GCP standard 129 131. Despite its reversibility our method remained safe due to the confidentiality of both the protection and restoration software which are distributed to partners only after signing a confidentiality agreement. Furthermore even if the protection and decoder soft-wares are exposed to attackers without the private key the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First the key is the 512-dimensional float vector with 32-bit precision providing 2512×32 216384 possible combinations which is computationally infeasible to brute-force. To empirically validate this we conducted experiments where each encrypted sample from SNPH validation set was subjected to 1 000 brute-force attempts showing a 0% success rate confirming the robustness against collision attack. Further our approach can be combined with timestamp-based password authentication protocols such as kerberos protocol to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization in an adversarial manner. Specifically given the encrypted images generated by ROFI we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method BIM algorithm to generate adversarial noise which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably even with a perturbation noise norm ε of 0.05 the attack success rate remains below 5%. To further enhance robustness we implemented a defense strategy by adding random noise to the input during inference. After applying this defense mechanism the attack success rate dropped to zero across all tested ε values Supplementary Table 11 demonstrating the reliability of our approach. These results confirm the robustness of our method whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification democratizing access to specialist care. For research ROFI can help break down data silos by enabling the creation of large multi-center privacy-compliant datasets which is critical for studying rare diseases and training robust AI models. Furthermore by demonstrating that diagnostic models can be effectively trained on privacy-protected images ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clinical diagnosis field it inevitably has some limitations. First although ROFI was evaluated in three cohorts the data was from Asian cohorts. In the future we will validate the clinical outcomes on individuals from other racial cohorts. Second ROFI is evaluated on facial images which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki. 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multistage process. First an Ophthalmic Sign Detector trained via weakly supervised learning identifies and extracts clinically relevant sign features from the ocular regions. In parallel a Transformerbased Neural Identity Translator alters the global facial features to obscure the patient s identity guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original sign-preserving features. A refinement network DA-Former ensures a seamless transition between these regions before a final decoder which reconstructs a high-quality privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator Figure 6b to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently the output features from the above two components are integrated i.e. the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former Figure 6c amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net Figure 6d producing the privacy-protected facial images. During the privacy recovery procedure we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form using the same private key as in the protection procedure. The Neural Identity Translator network Figure 6b aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely reduced or eliminated while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator Figure 6b with one difference the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in Restoring mode. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied the original facial information could be restored. Conversely if an incorrect private key is used such as a random key by an attacker the network generates the facial information of a virtual face thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis Figure 8. For images from the SNPH cohort s developing set physicians annotate each image s health state. A neural network φ which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence supervised by an image-level binary label y. Specifically y 0 indicates a healthy state while y 1 denotes disease presence. This approach is termed the region-score-max learning strategy. Formally given an input eye region image X RH×W φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image S φ X R H 8 × W 8 1 p max S Lclassify y log p + 1 y log 1 p where S represents the region-wise sign scores and p is the highest-confidence region score. As for the network architecture of φ ophthalmic feature extractor consists of the first three stages Conv1 Res1 and Res2 of the Res Net50 architecture while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function. Despite training on image-level annotations the detector effectively identifies classification-relevant regions in a weakly supervised manner aligning with previous studies. Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1 higher cutoff values e.g. 0.7 or 0.9 result in more aggressive suppression of facial features leading to better de-identification performance ID Protection Rate 96.96%. However such thresholds also risk removing diagnostically relevant regions as evidenced by the noticeable drop 11 in classification AUROC dropped to only 86.19%. Conversely lower cutoff values preserve more image content which helps maintain high diagnostic accuracy high AUROC but leave identifiable features partially intact reducing de-identification effectiveness. For example the ID protection rate is only 92.04% when the cutoff value is set to 0.1. Moreover we visualize the sign maps to provide a more intuitive understanding of the ophthalmic features learned by the model. As shown in Figure 9 the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate % 92.04 93.93 96.54 96.96 96.96 Classification AUROC % 91.34 91.34 91.25 88.26 86.19 We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.We denote the original face image and the protected face image as X and Y . The recovered face images by correct password and wrong password are denoted as ˆX and Xwrong, respectively. The learning objective includes five parts. (1) The cosine similarity between the identification of the protected image Y and that of the original image X should be as small as possible, LremoveID = cos(Face-Net(X), Face-Net(Y )), (2) where Face-Net denotes a pre-trained face identification network Sphereface. (2) The identification of the correctly recovered image ˆX and the original image X should be as similar as possible, namely, the cosine similarity should be large, LrecoverID = −cos(Face-Net( ˆX), Face-Net(X)). (3) The appearance of the correctly recovered image ˆX and the original image X should be as similar as possible, LrecoverApp = L1( ˆX − X) ∗ 0.05 + LPIPS( ˆX ,X), where L1 denotes the ℓ1 norm function, LPIPS denotes the Learned Perceptual Image Patch Similarity metric. (4) The identification of the wrongly recovered image Xwrong and the original image X should be as dissimilar as possible. LwrongID = cos(Face-Net(Xwrong), Face-Net(X)). (5) All images should be photo-realistic. Lphoto = LGAN(X) + LGAN(Y ) + LGAN( ˆX ) + LGAN(Xwrong), (6), where LGAN denotes the Generative Adversarial Networks (GAN) loss, which forces the images to be visually natural. Concretely, the GAN discriminator architecture follows a PatchGAN design with five 4×4 convolutional layers (stride=2) for progressive downsampling. Each convolution is followed by spectral normalization and LeakyReLU activation functions. The GAN loss comprises two components: (1) a local naturalness constraint applied to patch-level features, and (2) a global naturalness constraint implemented through average pooling across all spatial positions. To stabilize GAN training, we introduce the following techniques: spectral normalization layers to control discriminator Lipschitz constants, and Two-Timescale Update Rule (TTUR) to balance generator and discriminator learning rates. The GAN framework plays a crucial role in ensuring that the prepended noise key is actively utilized rather than ignored. The Generator is incentivized to produce a wide variety of diverse and realistic outputs to fool the Discriminator. Given our large-scale training dataset, the noise key serves as an effective perturbation signal, which the Generator leverages as a source of randomization to create distinct facial identities for the same input image. As demonstrated in our quantitative analysis (Supplementary Table 14 and Supplementary Figure 10), different noise keys lead to high variance in both pixel space and identity feature space, confirming that the model actively uses the noise to generate diverse, protected images. Finally, the whole objective is given by, LROFI = LremoveID + LrecoverID + LrecoverApp + LwrongID + Lphoto. (7) 4.4 Training Strategy During the training of the ROFI model, we utilize the AdamW optimizer with the following hyper-parameters: β1 (0.5), β2 (0.999), a weight decay of 1e-8, and a learning rate of 1e-4. The batch size is set to 12, and the training iteration number is 1,000,000. The learning rate undergoes a decay of 10 times at the 900,000th iteration. We evaluated the model every 20,000 iterations on the model-selection set of SNPH, the model with the smallest loss value LROFI is adopted as the final model. As the data augmentation strategy, we employ the random horizontal flipping operation. The model is implemented with the PyTorch framework. During the training of the ophthalmic sign detector, the optimizer and hyper-parameters are same as that of ROFI. To prevent overfitting to dominant regions (e.g., sclera/iris) and allows for detecting of subtle signs like early-stage conjunctival melanoma (CM), we employ the following strategies: (1) Weight Decay: This regularization technique discourages over-reliance on dominant features by penalizing large network weights, promoting balanced feature learning across both prominent and subtle regions. (2) Random Crop Augmentation: By training the model on randomly cropped sub-regions of the eye image, we force it to focus on localized discriminative cues rather than global dominant patterns. This enables the model sensitive to fine-grained abnormalities that may appear in non-dominant areas. 4.5 Implementation of the AI Diagnostic Models The SNHP validation set is partitioned into three subsets: the training set (60%) is utilized for training the AI models, the validation set (10%) is employed for hyperparameter tuning, and the test set (30%) serves as the benchmark for evaluating model performance. Throughout the training phase, all images undergo a uniform resizing process, bringing them to a standardized dimension of 256 × 256 using cubic interpolation. Subsequently, these resized images undergo augmentation employing prevalent strategies, including random cropping (ranging from 30% to 100% of the whole image), resizing to 224×224, random horizontal flipping, and image normalization. The ECXHCSU validation set is partitioned into two subsets, the training set (60%) and the test set (40%). ECXHCSU has no hyperparameter tuning subset, since its hyperparameters are reused from SNHP. The weight decay values for ResNet50 and ViT networks are configured at 1e-4 and 1e-2, respectively. The learning rate is set to 1e-4 for both ResNet50 and ViT. Mini-batch size is set to 256 for both ResNet50 and ViT. The training iteration number is set to 3000. Both ResNet50 and ViT networks are initialized from the models pretrained on ImageNet dataset. We optimize the diagnostic models with the AdamW optimizer. During model evaluation, all images are resized to 256×256 and subsequently cropped to the central 224×224 region. The strategy was adopted from the official data-processing strategies of the above network architectures.'}, {'rank': 3, 'score': 9.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 4, 'score': 9.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}, {'rank': 5, 'score': 9.0, 'id': '2510.10822v1', 'title': 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'text': 'From Detection to Mitigation Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis. Deep learning models have shown promise in improving diagnostic accuracy from chest Xrays but they also risk perpetuating healthcare disparities when performance varies across demographic groups. In this work we present a comprehensive bias detection and mitigation framework targeting sex age and race-based disparities when performing diagnostic tasks with chest X-rays. We extend a recent CNN XGBoost pipeline to support multi-label classification and evaluate its performance across four medical conditions. We show that replacing the final layer of CNN with an e Xtreme Gradient Boosting classifier improves the fairness of the subgroup while maintaining or improving the overall predictive performance. To validate its generalizability we apply the method to different backbones namely Dense Net-121 and Res Net-50 and achieve similarly strong performance and fairness outcomes confirming its model-agnostic design. We further compare this lightweight adapter training method with traditional full-model training bias mitigation techniques including adversarial training reweighting data augmentation and active learning and find that our approach offers competitive or superior bias reduction at a fraction of the computational cost. Finally we show that combining e Xtreme Gradient Boosting retraining with active learning yields the largest reduction in bias across all demographic subgroups both in and out of distribution on the Che Xpert and MIMIC datasets establishing a practical and effective path toward equitable deep learning deployment in clinical radiology. Deep learning DL models have demonstrated remarkable success in medical imaging tasks including disease detection from chest X-rays CXRs.1 These models promise to improve clinical workflows by improving diagnostic accuracy enabling faster decision-making and expanding access to care. However as DL systems become increasingly integrated into healthcare concerns have emerged about their potential to exacerbate health disparities. In particular models trained on unbalanced datasets can exhibit biased performance across subgroups defined by sex age or race raising critical issues of fairness trust and safety in clinical deployment.6 8 Bias in DL models can come from multiple sources including underrepresentation in training data spurious correlations and learned shortcut features.8 These biases may result in systematically worse performance for specific demographic groups which undermines the equity and reliability of medical AI systems. Existing bias mitigation techniques such as reweighting samples adversarial training and data augmentation can be effective but often require full model retraining.19 This makes them computationally expensive and difficult to implement in real-world healthcare settings where data access and training resources are limited. To address these limitations we propose a lightweight and effective bias mitigation strategy building upon prior work.4 The idea is to extract the last layer of a convolutional neural network CNN freeze the embeddings and retrain the head using an e Xtreme Gradient Boosting XGBoost 18 classifier. Our contributions are as follows We perform a detailed bias detection analysis to quantify disparities across sex age and race subgroups using large-scale public datasets Che Xpert and MIMIC. We introduce a CNN-XGBoost pipeline that supports multi-label disease prediction. We demonstrate that our XGBoost adapter head can be effectively integrated with different CNN-based architectures showing similar improvements in performance and reductions in bias. We benchmark our method to multiple adaptation heads and to full model fine-tuning and demonstrate that XGBoost offers the best trade-off between performance fairness and computational cost. Finally we demonstrate that combining XGBoost retraining with active learning is a successful bias mitigation that generalizes to both In-Distribution ID and Out-OfDistribution OOD settings. This work presents a practical and scalable path for deploying fair and effective DL models in clinical radiology enabling safer and more equitable care for diverse patient populations. Bias in medical Artificial Intelligence AI has become a major concern as DL models may perform unevenly across patient subgroups. Previous research has shown that CXR diagnostic models often encode demographic information such as sex age or race even when not explicitly trained to do so.8 19 This can lead to biased predictions especially in the presence of demographic imbalances in the training data. To address these issues various fairness-focused methods have been proposed including sample reweighting adversarial training and fairness-aware objectives.19 Other studies have explored last-layer retraining to mitigate spurious correlations showing that simple linearhead replacements can achieve fairness with low computational cost.3 Some research has investigated CNN XGBoost hybrid models for performance enhancement particularly in tasks such as pneumonia or breast cancer detection 10 11 but their role in bias mitigation remains underexplored. A recent study proposed a lightweight bias mitigation strategy that replaces the final layer of a CNN with an XGBoost classifier trained on a curated subset of embeddings.4 While this method showed promise in reducing bias related to a single disease it did not evaluate its applicability to multiple conditions nor did it compare XGBoost to other classifier types or integrate existing bias mitigation techniques. In this work we extend the CNN XGBoost approach to address its current limitations. 3.1. Data We consider two large publicly available CXR datasets in this work Che Xpert Plus 16 This dataset includes 224 316 CXR images collected at Stanford Health Care and a test set of 500 exams with annotations from eight radiologists. Medical Information Mart for Intensive Care MIMIC 17 This dataset contains 377 110 CXR images acquired at the Beth Israel Deaconess Medical Center. Both datasets include demographic information of sex age and race. For sex-based analysis we compare model performance between male and female patients. For age we apply a threshold of 70 years to separate the data into two categories young and old. For race we focus on three groups White Black and Asian. Patients from other racial backgrounds were grouped under an Other category which was too small to support reliable subgroup analysis. We follow the training and test splits provided by Che Xpert and MIMIC using only posterior-anterior PA and anterior-posterior AP view images. This filtering results in 112 105 CXRs for Che Xpert and 139 508 for MIMIC. All images are resized to 224×224 or 512x512 pixels to match the input requirements of the respective models. 3.2. Metrics Defining clinical bias is inherently complex as it involves trade-offs between fairness and other key performance metrics. There is no evidence suggesting that CXRs from certain demographic subgroups are more difficult to classify and we argue that this parity should be reflected in model behavior as well. In this study we define bias as disparities in model performance across subgroups. Reducing bias should not come at the cost of overall model performance. However in some cases mitigating bias may unintentionally disadvantage specific subgroups. To measure overall performance we use the Area Under the Precision-Recall Curve AUPRC which provides a balanced assessment of precision and recall and is particularly suited for imbalanced datasets. To evaluate fairness we compute the performance disparity across subgroups using AUPRC the absolute difference in AUPRC between subgroups. In cases with more than two subgroups such as race we report the maximum observed AUPRC as the fairness metric. In our framework the goal is to achieve high AUPRC strong overall performance and low AUPRC minimal disparity across subgroups. 3.3. Bias Detection Framework Before mitigating the bias it is essential to detect and understand its sources. Model bias can come from various factors including data composition clinical context and the algorithm itself. In this analysis we focus on identifying potential sources of bias on Che Xpert. 1 Data and Clinical Context We study disparities introduced during data collection. For each disease and demographic subgroup we analyze the distribution of positive and negative labels to assess imbalances. Additionally we study differences in disease prevalence across demographic groups to understand potential confounding clinical patterns. 2 Model Behavior To investigate the model s internal representations we visualize learned embeddings using Principal Component Analysis PCA and t-distributed Stochastic Neighbor Embedding t-SNE 20 plots stratified by demographic group. We also evaluate whether demographic attributes sex age race can be predicted from these embeddings originally trained to classify medical conditions to assess whether the model encodes sensitive information. Furthermore we employ SHapley Additive ex Planations SHAP values to analyze feature importance when predicting specific diseases across different subgroups helping us determine whether the model attends to different features depending on the subgroup. Finally we assess performance disparities by subgroup. 3 Radiologist Agreement Since our models are trained on radiologist-generated labels we assess potential inter-radiologist variability across demographic subgroups. To perform this analysis we analyze agreement among the 500 predictions of eight radiologists. 3.4. Bias Mitigation Methods To address limitations in the CNN XGBoost pipeline we propose several enhancements. The overall pipeline is illustrated in Fig. 1. Bias mitigation method pipeline. The numbers correspond to the respective steps. same pipeline using a Res Net-50 model from the Torch XRay Vision library.7 4 Comparison with Standard Bias Mitigation Techniques For a more controlled setting we retrain a Dense Net-121 from scratch. This allows us to control the dataset splits and tailor the output layer to predict only the medical conditions relevant to our analysis. For initialization we use pretrained weights from Che XNet 1 a Dense Net-121 model trained specifically for pneumonia detection note that pneumonia is not among the target conditions in our study. The training configuration follows the defaults used in Torch XRay Vision.7 Then we apply several existing bias mitigation strategies and compare their effectiveness to that of XGBoost-based head retraining Weighted Sampling Reweighting the training data to balance subgroups. Adversarial Training Introducing a secondary adversarial branch to predict sensitive attributes sex race age while training the primary network to be demographically agnostic by minimizing this branch s accuracy. Targeted Data Augmentation Augmenting under-performing subgroups. Active Learning Prioritizing the inclusion of underrepresented or uncertain samples via uncertainty or diversity-based sampling. 5 Combining Bias Mitigation Strategies Finally we combine the above mitigation techniques with XGBoost head retraining. This hybrid approach allows us to benefit from XGBoost s ability to handle imbalanced feature distributions while simultaneously correcting for bias embedded in the data. This strategy is computationally more efficient than full model retraining. For hyperparameter tuning we use a custom score function designed to balance performance and fairness Score AUPRC AUPRCsex + AUPRCage + AUPRCrace. This formulation encourages the model to achieve high overall predictive performance while minimizing disparities across age race and sex. Hyperparameters were selected based on the model performance evaluated on the validation dataset. Each experiment is run five times the results are averaged and the standard deviation and CI are computed. 4. Experiments Results and Discussion 4.1. Bias Detection Analysis To better understand the origins of bias in our model we analyzed three potential sources the data distribution the model s internal representations and human radiologist variability. 1 Data and Label Distribution We first studied the class distribution across demographic subgroups in the Che Xpert dataset Table 1. Several imbalances were evident Sex Both males and females exhibited similar prevalence rates across conditions. For example Lung Opacity appeared in 49.5% of both groups. Age Older patients showed significantly higher disease prevalence. For instance Cardiomegaly was present in 15.4% of older patients versus 10.3% in younger ones. This trend was consistent across all four diseases studied. Race Substantial variation was observed in disease prevalence. For example Cardiomegaly prevalence in Black patients was 18.3% higher than in White 11.5% or Asian 12.9% patients. Moreover there is a high data imbalance according to race where White people represent 78.2% of the data Asian 14.7% and Black 7.1%. Cardiomegaly Lung Opacity Edema Pleural Effusion + %+ + %+ + %+ + %+ Sex Female 37.3 4.7 11.2 21.2 20.8 49.5 31.9 10.1 24.0 25.3 16.7 39.8 Male 50.5 7.5 12.9 29.3 28.7 49.5 44.3 13.7 23.6 34.9 23.1 39.8 Age Young 56.4 6.5 10.3 33.2 29.7 47.2 49.5 13.4 21.3 39.4 23.5 37.4 Old 31.4 5.7 15.4 17.2 19.9 53.6 26.7 10.4 28.0 20.9 16.2 43.7 Race White 69.2 9.0 11.5 39.3 38.9 49.7 59.4 18.8 24.0 46.9 31.3 40.0 Asian 12.8 1.9 12.9 7.5 7.2 49.0 11.5 3.1 21.2 8.6 6.1 41.5 Black 5.8 1.3 18.3 3.7 3.4 47.9 5.3 1.9 26.4 4.8 2.4 33.3 These disparities could introduce confounding if not accounted for. 2 Model Representations and Learned Biases To investigate whether the model encodes demographic information implicitly we first visualized the extracted embeddings using PCA and t-SNE. As shown in Figure 2 we observe visual patterns that suggest a degree of separation across sex age and race subgroups indicating the presence of demographic signals in the learned representations. To quantify this observation we trained a simple LR classifier to predict demographics from CNN embeddings. It achieved high AUCs sex 0.93 age 0.82 race 0.77 confirming Fig. 2. PCA and t-SNE first components of the extracted embeddings according to the different demographic subgroups. that the model encodes demographic information despite not being trained to do so. Further we used SHAP to examine whether the model s feature attributions differ across demographic subgroups when predicting specific medical conditions. Specifically we analyzed whether the most influential embedding dimensions contributed in a consistent direction across subgroups when predicting Cardiomegaly. Table 2 presents the direction analysis for the five most important embedding dimensions. For each embedding and subgroup we indicate whether the SHAP value direction was consistent same or reversed opposite between subgroups. For example Embedding 950 showed opposite attributions across all three demographics suggesting the model interprets this feature differently depending on the patient s demographics. Such representational differences point to learned bias which becomes problematic when linked to performance disparities. Embeddings Sex Age Race 773 same same same 781 opposite same same 950 opposite opposite opposite 696 same opposite opposite 603 same opposite same To assess this we computed the mean AUPRC across the four medical conditions. We found persistent bias AUPRC of 1.6 for sex 4.1 for age and 8.7 for race. These values indicate that while the model may achieve high overall accuracy its performance is not distributed equally across demographic groups especially with respect to race. This underscores the importance of addressing representational and predictive disparities through targeted bias mitigation strategies. 3 Radiologist Variability We analyzed inter-rater disagreement among eight radiologists on 500 patients Table 3. While radiologists can sometimes visually infer sex and roughly estimate age they cannot identify a patient s race from a CXR.2 Therefore any racial bias observed in the model is more likely to come from the data or algorithm. We computed disagreement rates as the average proportion of radiologists who did not agree with the majority vote for each case stratified by subgroup. Disagreements as shown in Table 4 were generally low and consistent with a bigger difference related to the age subgroup most likely due to disease prevalence in older patient. This suggests that radiologist uncertainty does not disproportionately affect any subgroup meaning label noise is likely not a major contributor to subgroup bias. Subgroup Count Sex Male 125 Age Young 131 Race White 51 Subgroup % Disagreement Sex Male 9.3 Female 9.6 Age Young 8.8 Old 10.0 Race White 9.6 Asian 8.7 4.2. Bias Mitigation Analysis We evaluate several strategies to mitigate bias while maintaining strong performance. Our approach builds upon the CNN XGBoost pipeline by progressively enhancing its flexibility evaluating its robustness and benchmarking it against standard bias mitigation techniques. 1 Extension to Multi-Label Classification We first extended the baseline method which focused only on Pleural Effusion 4 to handle multiple medical conditions. Specifically we added Cardiomegaly Lung Opacity and Edema in the analysis. As shown in Figure 3 the multi-label extension improves overall performance while reducing bias. Fig. 3. Performance and bias between the baseline Dense Net-121 model and the model with the head retrained with an XGBoost classifier on Che Xpert. 2 Evaluation of Alternative Classifier Heads We next evaluated alternative models for the classifier head replacing XGBoost with LR DT RF NN BRF and KNN. As shown in Figure 4 XGBoost offered the best trade-off between performance and fairness. LR performed well which is consistent with the linear structure of the original CNN classifier layer. Notably LR reduced bias across sex and age but was less effective for race likely due to higher data imbalance. In contrast BRF and XGBoost were most robust across all subgroups due to their ensemble design and handling of imbalance. DT and RF performed near random and were excluded. These findings highlight that classifier choice impacts the pattern of bias reduction with some models more sensitive to subgroup imbalance. Fig. 4. Differences in performance and bias when retraining the head of the Dense Net-121 with different models on Che Xpert. Confidence Intervals CI are not shown for KNN since it doesn t involve any internal randomness or stochastic training process. 3 Generalization to Other Backbone Architectures To evaluate the model-agnostic nature of our framework we repeated our experiments using a Res Net-50 architecture with a 512-dimensional embedding output. As with Dense Net-121 we first applied PCA and then retrained the final head using XGBoost. The results presented in Figure 5 mirrored those observed with Dense Net we achieved a consistent increase in overall performance and a noticeable reduction in bias across sex age and race subgroups. This confirms that the bias mitigation approach can be flexibly applied across different CNN backbones. Fig. 5. Performance and bias between the Res Net-50 model and the model with the head retrained with an XGBoost on Che Xpert. 4 Full Model Retraining versus Lightweight Head Retraining We retrained a Dense Net-121 CNN from scratch for fairer comparisons. We then compared our lightweight XGBoost method with existing bias mitigation approaches that require full model retraining. As shown in Figure 6 XGBoost head retraining achieved comparable or even superior performance in reducing bias at a fraction of the computational cost. Specifically our method retrains only 20 000 parameters versus 8 million in a full Dense Net-121. Fig. 6. Comparison of our lightweight bias mitigation method in orange with existing methods that require full model retraining. 5 Combining Bias Mitigation Techniques Finally we combined XGBoost head retraining with standard bias mitigation strategies such as weighted sampling adversarial training data augmentation and active learning and compared the results with applying these strategies to full model retraining. As illustrated in Figure 7 combining mitigation strategies with XGBoost retraining consistently outperformed full model retraining both in performance and fairness and again at much lower computational cost. The final results presented in Figure 8 show that the combination of active learning with XGBoost head retraining yields the largest reduction in bias across all subgroups sex age and race both ID on Che Xpert and OOD on MIMIC. The optimized hyperparameters for XGBoost are as follows eval metric logloss learning rate 0.05 n estimators 150 and max depth 10. For active learning we used a pool-based approach starting with 15 000 labeled images and adding 2 000 uncertain samples per round over 10 rounds for a final training set of 35 000 images. Fig. 7. Comparison of existing bias mitigation methods that require full model retraining with our method combined with our XGBoost head retraining in grey. Fig. 8. Comparison of initial performance and bias baseline in blue with XGBoost head retraining orange and XGBoost head retraining combined with active learning brown ID on Che Xpert and OOD on MIMIC. 4.3. Clinical Significance of Bias Mitigation While metrics like AUPRC and AUPRC are essential for evaluating model performance and fairness it is equally important to interpret these results in the context of clinical impact. Our experiments demonstrate that AUPRC can be reduced without sacrificing overall performance. By minimizing performance gaps between sex age and race subgroups we reduce the risk that some populations receive less accurate diagnoses. This helps prevent misdiagnoses in underrepresented groups which have historically experienced healthcare disparities. For illustration we evaluated Pleural Effusion prediction across women of different races. To minimize False Negative Rates FNR thresholds were chosen before and after bias mitigation to ensure recall greater than 0.95. FNR White FNR Asian FNR Black FNR TPR FPR EO max gap Before 0.159 0.136 0.154 0.023 0.023 0.015 0.023 After 0.149 0.139 0.143 0.010 0.010 0.007 0.010 As shown in Table 5 bias mitigation reduced both subgroup FNRs and disparities in Equalized Odds EO 21 with sensitivity and specificity gaps cut roughly in half. Clinically this means patients receive better and more consistent diagnostic across racial groups increasing both reliability and trust in the model. Such improvements enhance the likelihood of clinician adoption of AI tools that demonstrate equitable behavior across diverse populations. In this work we introduced a practical and lightweight framework for detecting and mitigating demographic bias in DL models for CXR diagnosis. By replacing the final classification layer of a CNN with an XGBoost model we demonstrated that it is possible to significantly reduce disparities across sex age and race subgroups while preserving if not improving overall model performance. Our approach generalizes effectively across multiple medical conditions and remains robust in both ID Che Xpert and OOD MIMIC evaluations. Through our experiments we showed that XGBoost outperforms alternative classifier heads in balancing fairness and accuracy. The method is model-agnostic and can be applied to any architecture capable of extracting embeddings from images. Our method rivals or exceeds traditional full-model bias mitigation techniques including weighted sampling adversarial training data augmentation and active learning while requiring far fewer computational resources. Combining our XGBoost head retraining with active learning yields the most substantial bias reduction across all subgroups while maintaining a competitive performance. These findings offer a compelling pathway for deploying fair and efficient medical AI models in real-world clinical settings where computational constraints are often a major barrier. Despite promising results this study has several limitations. The racial subgroup analysis is affected by class imbalance particularly for Black patients. This underrepresentation limits the statistical robustness of bias evaluations and may obscure subtle disparities. Moreover our work focuses only on CNN-based models applied to CXRs. Therefore the generalizability of our findings to other imaging modalities e.g. CT MRI and tasks e.g. segmentation remains to be established. Finally our approach relies on last-layer retraining. While efficient it may be insufficient to fully mitigate spurious correlations compared with approaches leveraging representations from all network layers. Future work include extending the framework to other architectures such as Vision Transformers Vi Ts and applying and validating this approach on other imaging modalities and in tasks beyond classification.'}]",Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it.
How can I train models stably with limited computational resources?,2510.08116v1,2510.12850v1,False,"['2510.08116v1', '2509.20913v1', '2510.13937v1', '2510.13137v1', '2510.12758v1']","[8.0, 8.0, 7.0, 7.0, 7.0]","['Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention']","[{'rank': 1, 'score': 8.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}, {'rank': 2, 'score': 8.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 7.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 4, 'score': 7.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 5, 'score': 7.0, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': ""PET Head Motion Estimation Using Supervised Deep Learning with Attention. Head movement poses a significant challenge in brain positron emission tomography PET imaging resulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc-tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware-based motion tracking HMT has limited applicability in real-world clinical practice. To overcome this limitation we propose a deep-learning head motion correction ap-proach with cross-attention DL-HMC++ to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist-ing dynamic PET scans with gold-standard motion mea-surements from external HMT. We evaluate DL-HMC++ on two PET scanners HRRT and m CT and four radiotracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 to demonstrate the effectiveness and generalization of the ap-proach in large cohort PET studies. Quantitative and qual-itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods producing motion-free images with clear delineation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold-standard HMT to be 1.2 0.5% for HRRT and 0.5 0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT making motion correction accessible to clinical popula-tions beyond research settings. The code is available at  Positron emission tomography PET imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves-tigation of various neurotransmitters and receptor dynamics in different brain targets as well as studies of physiological or pathological processes. PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases including Alzheimer s disease Parkinson s disease epilepsy and certain brain tumors. However the presence of patient movement during PET brain scanning poses a signif-icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis. Even minor instances of head motion can substantially impact brain PET quantification resulting in diminished image clarity reduced concentrations in regions with high tracer uptake and mis-estimation in tracer kinetic modeling. This problem is further exacerbated by the long duration of PET studies where patients can involuntarily move. Even with physical head restraints typical translations in the range of 5 to 20 mm and rotations of 1 to 4 are observed. Therefore accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient movement during image acquisition while motion correction MC refers to the process of compensating for the effects of head movement. Generally patient movements in brain imaging are assumed to be of a rigid nature composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event EBE MC. Therefore accurate motion estimation is crucial for realizing high-quality PET imaging. Physical restraint during PET scanning can substantially reduce head motion effects. However such methods cannot eliminate movement entirely and this restrictive approach may be uncomfortable especially over long scan durations which reduces their acceptability for real-world use. Currently head motion estimation methods are primarily cat-egorized into the following types i hardware-based mo-tion tracking HMT and ii data-driven approaches. For HMT high-frequency head motion information is provided by external devices. Marker-based HMT such as Polaris Vicra NDI Canada tracks light-reflecting markers on the patient s head. Despite its potential benefits Vicra is not commonly employed in clinical practice because it necessitates the attach-ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process thereby compromising the integrity of the data collected. Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight TOF range sensor to achieve markerless head motion track-ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However these systems can be challenged by facial expressions and other non-rigid motions. In general HMT methods mainly rely on extra hardware support and setup which limits their practical application in real-world clinical scenarios. On the other hand data-driven methods estimate head mo-tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high-sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution COD of 3D PET cloud images PCIs. These methods utilized intensity-based image registration methods to align different frames but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast deep learning DL methods leveraging neural networks to construct a hierarchical repre-sentation of data through multiple layers of hidden units enable registration approaches to extract pertinent features directly from the data. Salehi et al. proposed a DL model for medical image rigid registration and achieved real-time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration. Inspired by DL-based registration methods Zeng et al. proposed a supervised DL head motion correction DL-HMC framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However due to the noisy PCIs and limited generalization across data distributions the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However intra-frame motion can not be solved by frame-based MC and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation however this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low-count images to high-count images thereby predicting motion from high-quality subframes. However training the network requires motion-free PET data which is not available in this case. To address the limitations of the original DL-HMC approach this study introduces an enhanced model DL-HMC++ that incorporates a cross-attention mechanism aiming to enhance motion estimation and generalization performance. Notably attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications. Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by i including a rigorous validation of DL-HMC++ using a large cohort of human PET studies encompassing over 280 brain scans with 4 different tracers ii providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers including cross-tracer generalization experiments iii ablation studies to justify model design choices iv quantitative evaluation of MC accuracy and v comprehensive validation studies against several state-of-the-art SOTA benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL-HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolution. This data-driven motion estimation model utilizes one-second 3D PET cloud image PCI representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov respectively along the line-of-response LOR with normalization for scanner sensitivity. For model training and evaluation each one-second PCI has corresponding Vicra HMT information rigid transformation matrix as the gold-standard motion. We train the model to estimate the rigid motion transformation θ tx ty tz rx ry rz CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation td and three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows Fref Gref Aref + Vref Fmov Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration one-second low system sensitivity and lack of essential physical correction low-frequency bias within the PCI significantly affects MC performance making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance we introduce the attention mechanism in our model to emphasize the head region. This module establishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref RC×H×W ×D and fmov RC×H×W ×D which represent the feature maps of the reference and moving images respectively where H W and D denote the feature map dimensions and C denotes the number of feature channel. Initially we partition fref into reference key Kref and value Vref and likewise fmov is divided into moving query Kmov and value Vmov We validate DL-HMC++ s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1 Data We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort contains a diverse set of PET data from four different radiotracers acquired on two different scanners i 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph HRRT scanner Siemens Healthineers Germany without time-of-flight TOF and ii 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner Siemens Health-ineers Germany with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls neurological disorders such as Alzheimer s Disease AD mild cognitive impairment MCI epilepsy and other diagnoses. We divide each dataset into Training Validation and Testing sets using an 8 1 1 ratio Tab. I. All scans include Kref Wafref Vref Wbfref Kmov Wafmov Vmov Wbfmov where Wa Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C × HWD and calculate the attention matrix using the following equation Amr Softmax KT mov Kref R HW D × HW D where Amr represents the similarity matrix correlating each row of KT mov with each column of Kref. Upon computing the DNF Predicted I % Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb 1×1×1 Wa 1×1×1 Wb 1×1×1 V % Reference Branch f % G 1×1×1 G 1×1×1 F % A Sigmoid Sigmoid G K % attention reference Embedded reference PCI feature softmax S Self-gate Wa 1×1×1 A Moving Branch F K f A % G % attention moving feature V Embedded moving PCI Fig. 1. DL-HMC++ network architecture. Top A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion DNF blocks refine the attention features both before and after concatenation. Finally concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. Bottom Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX HEALTH STATUS INJECTED ACTIVITY AND MOTION INFORMATION. REPORTED VALUES ARE MEAN SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. M/F 100 56/44 20 13/7 100 53/45 20 16/4 20 8/12 4 1/3 16 7/9 4 4/0 Healthy Control 42 7 37 8 20 4 8 2 Alzheimer s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity m Ci 4.83 0.28 4.93 0.15 14.99 5.15 14.91 4.84 3.75 1.19 4.47 0.16 14.27 4.43 15.77 6.32 Motion mm 7.69 6.80 11.20 3.53 8.56 6.87 10.79 8.29 11.01 11.64 3.90 1.48 8.96 7.54 9.46 3.71 Vicra HMT information used as gold-standard motion estimation T1-weighted magnetic resonance imaging MRI PET-space to MRI-space transformation matrices and Free Surfer anatomical MRI segmentations. All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in. All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2 Evaluation Metrics We evaluate head motion estimation performance using quantitative and qualitative assessment. a Quantitative Assessment of Motion Estimation To quantitatively evaluate the performance of motion estimation we calculate the Root Mean Squared Error RMSE between the estimated motion parameters ˆθ and the Vicra gold-standard θ. The RMSE was computed for each individual motion component translation and rotation separately across the full scan duration. To robustly summarize motion estimation performance we calculate the mean value and standard deviation CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 SD of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed-rank test was selectively applied to the HRRT s 18F-FDG and 11C-UCB-J datasets but did not apply to the m CT datasets due to the test set sample size n 4 subjects being below the minimum requirement n 6. b Qualitative and Quantitative Assessment of Reconstructed PET Images For HRRT 18F-FDG and m CT 18F-FPEB studies we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here Epred Rpred RVicra / max Rpred RVicra scale to the range 1 1 where Rpred and RVicra are the reconstructed images from motion-correction and Vicra HMT respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value SUV within each ROI. We aggregate the original 109 Free Surfer ROIs into 14 grey matter GM ROIs Amygdala Caudate Cerebellum Cortex Frontal Hippocampus Insula Occipital Pallidum Parietal Putamen Temporal Thalamus and two white matter WM ROIs the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo-cations we calculate the mean distance error MDE of anatomical brain ROIs. Using the Free Surfer segmented ROI masks we calculate the center-of-mass COM for each ROI on the Vicra MC result COMVicra. Then the same ROI masking is applied to the MOLAR reconstruction images with different MC methods and the estimated COM COMest of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences ii Sim-ple Elastix SIM a widely utilized medical image reg-istration tool that employs mutual information as a similarity metric to rigidly register the PCIs iii Imregtform IMR a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss which was used in prior data-driven PET head MC studies iv DL-HMC our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex-cludes attention v DL-HMC without time-conditioning DL-HMC w/o TC which removes the time conditional module from the original DL-HMC and vi Dual-Channel Squeeze-Fusion-Excitation Du SFE a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods following the same workflow in high-resolution one-second fast reconstruction images FRIs were generated using CPU-parallel reconstruction platforms for the m CT dataset. We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction NMC results were also compared for reference. 5 Implementation Details a Data Processing To create the DL-HMC++ input we pre-process the HRRT PCI data volumes by downsampling from 256×256×207 voxels 1.22×1.22×1.23 mm3 to 32×32×32 voxels 9.76×9.76×7.96 mm3 using area interpolation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels 2.04×2.04×2.03 mm3 voxel spacing to 32×32×32 voxels 9.56×9.56×6.91 mm3 voxel spacing. b Network Training To efficiently train the network we randomly sub-sample 360 out of 1 800 time points for each study in the training set. During each training epoch we randomly pair two PCIs as reference Iref and moving Imov image inputs such that tmov tref and calculate their relative Vicra motion on the fly. We train the network using a mini-batch size of 12 and minimize the mean squared error MSE between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4 γ 0.98 and exponential decay with step size 200 for training. c Network Inference For inference on testing subjects independent of the training data we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d Event-by-Event EBE Motion Compensated Reconstruction Once the rigid motion transformation parameters have been estimated by DL-HMC++ we reconstruct the PET image using the EBE motion compensation OSEM list-mode algorithm for resolution-recovery reconstruction MOLAR. MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies OSEM reconstruction 2 iterations × 30 subsets with spatially invariant point-spread-function PSF of 2.5-mm full-width-half-maximum FWHM is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies OSEM reconstruction 3 iterations × 21 subsets with spatially invariant PSF of 4.0-mm FWHM is 3 Cross-tracer Generalization Evaluation To validate the model s cross-tracer generalization capability we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically the model weights obtained from HRRT 11C-UCB-J training are applied to 18F-FDG data while the weights from m CT 11C-LSN3172176 training are evaluated on 18F-FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model s performance on these unseen tracers with the gold-standard Vicra evaluating RMSE for both translation and rotation parameters Sec. III-A.2.a. This evaluation provides critical insights into the model s robustness and generalizability across diverse tracer applications. 4 Baseline Motion Estimation Methods We comprehensively compared our approach for head motion estimation against SOTA benchmark methods including intensity-based registration and deep learning methods i Bio Image Suite BIS an intensity-based rigid registration proce-6 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1 18F-FPEB DL-HMC++ remains competitive on the m CT 18F-FPEB data reaching RMSE of 0.54 mm in translation and 0.40 in rotation Table II on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT where DL methods outperform SOTA image-intensity registration methods BIS IMR that even utilize FRIs as input. Similar to the HRRT results DL-HMC++ s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F-FPEB dataset comparing DL-HMC++ with the baseline DL-HMC and the Vicra gold standard. While the overall performance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject 1 translation Z where DL-HMC fails to track the motion red bounding box while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject 2 both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis however this error is limited to 1.5. B. HRRT Results 1 18F-FDG DL-HMC++ demonstrates the best quantitative motion estimation performance compared to all other benchmark methods with translation and rotation RMSE of 1.27 mm and 1.16 respectively Table II. The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements p 0.05 in both translation and rotation errors compared to all benchmark methods. Overall DL methods outperform the intensity-based registration approaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL-HMC demonstrating a 49% and 27% improvement in translation and rotation respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold-standard which demonstrates that the proposed method can effectively track head motion. In FDG Subject 1 both models demonstrate excellent alignment with actual Vicra head motion patterns. For Subject 2 a poor performance occurs in translation X red bounding box where DL-HMC++ shows a misalignment with Vicra however DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover our model performs well during other periods demonstrating its capability to estimate movements with relatively large translations over 15 mm and 9-degree rotations. In addition DL-HMC++ s proposed cross-attention module enhances the model s ability to correct motion by concen-trating on the head region during the motion tracking which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC Fig. 3. DL-HMC s saliency maps highlight areas outside the head suggesting this model failed to focus on the relevant anatomical information in the PCI. 2 11C-LSN3172176 Building upon the promising results demonstrated with 18F in m CT our proposed DL-HMC++ framework maintains superior performance in both transla-tion and rotation estimation for the more challenging 11C-LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods demonstrating an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject 1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information as demonstrated by its flattened prediction curve our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes DL-HMC++ suc-cessfully detects the overall movement trends up to 10 mm in translation X and 4 in rotation Z. In summary the significant improvements in motion estimation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2 11C-UCB-J The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL-HMC++ similar to its performance on 18F data Tab. II. Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics with translation and rotation RMSE values of 1.26 mm and 1.22 respectively. Statistical evaluation confirms that DL-HMC++ achieves sig-nificantly superior performance over nearly all benchmark methods p 0.05. Compared to the original DL-HMC DL-HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT Fig. 2 third column DL-HMC++ demonstrates promising capability in capturing large motion patterns even under challenging conditions e.g. 14 mm in z-axis translation and 7 in x-axis rotation. Compared to the original DL-HMC DL-HMC++ achieves superior motion detection sensitivity. For example as highlighted by the red bounding box DL-HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance Table III. 1 Network Architecture To demonstrate the effectiveness of the DL-HMC++ architecture we compare i the proposed model architecture with self-gating and DNF ii the model without self-gating iii the model without DNF and iv the model without both self-gating and DNF. DL-HMC++ without CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject 2 Subject 1 18F-FDG Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION TRANS. MM AND ROTATION ROT. DEGREES COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS HRRT AND MCT USING FOUR RADIOTRACERS 18F-FDG 18F-FPEB 11C-UCB-J AND 11C-LSN3172176. REPORTED VALUES ARE MEAN SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg Trans. mm Rot. deg NMC 6.29 5.79 3.12 1.42 6.86 19.58 3.27 6.14 2.42 1.43 1.36 0.48 4.63 7.76 2.10 1.36 BIS 4.26 5.31 2.06 3.01 3.18 3.56 1.63 1.54 1.32 0.06 0.53 0.05 1.40 0.20 0.66 0.06 SIM 3.15 4.87 1.94 2.70 3.04 2.53 1.58 1.32 1.57 0.10 1.24 0.02 3.06 2.05 2.60 3.03 IMR 2.84 3.83 2.25 2.85 3.52 3.97 1.77 1.50 1.38 0.28 0.55 0.05 2.32 2.26 0.88 0.07 DL-HMC 2.49 2.43 1.59 2.32 2.07 1.87 1.35 1.09 0.93 0.20 0.40 0.03 1.46 0.35 0.71 0.09 -w/o TC 1.76 1.19 1.33 1.63 1.54 0.62 1.34 1.13 0.80 0.01 0.57 0.01 1.19 0.11 0.61 0.02 Du SFE 1.56 0.66 1.37 1.73 1.36 0.46 1.36 0.85 0.60 0.03 0.41 0.02 1.21 0.12 0.69 0.10 DL-HMC++ 1.27 0.46 1.16 1.20 1.26 0.44 1.22 0.98 0.54 0.00 0.40 0.00 0.99 0.02 0.58 0.03 Note indicates p 0.05. gating and DNF demonstrate the worse performance. Re-moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21 in rotation which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model s performance. 2 Attention Type We experiment with different atten-tion types i cross-attention and ii self-attention. Com-pared with the self-attention mechanism which computes feature similarities within each input image individually cross-attention concentrates feature learning on the head areas by Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS FLOPS AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. mm Rot. deg Parameters M FLOPs ×109 Inference Time ms Res Net 1.62 0.83 1.37 1.88 14.61 4.6 5.8 U-Net 1.27 0.46 1.16 1.20 0.86 4.0 3.3 tively compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. a 360s b 720s c 1080s d 1440s e 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min 1 800 s PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4 PET Cloud Image PCI Size We evaluate the perfor-mance of our model under various 3D PCI sizes 323 643 and 963. As PCI size increases there is a slight degradation in performance. Despite having lower spatial resolution small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs see Fig. 5. In con-trast the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE ATTENTION TYPE CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN SD. 5 Network Encoder We further evaluate the choice of image encoder by comparing DL-HMC++ s U-Net encoder to DL-HMC s Res Net encoder removing the fully connected layer for a fair comparison. As shown in Table IV we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. mm Rot. deg Proposed 1.27 0.46 1.16 1.20 w/o gate 1.52 0.52 1.37 1.98 w/o DNF 1.62 1.03 1.33 1.77 backbone 2.31 1.85 1.44 1.78 Network Arch. Attention Type self attention 1.61 0.64 1.33 1.75 Proposed 1.27 0.46 1.16 1.20 20 2.10 2.27 1.88 2.71 40 1.69 0.79 1.44 1.56 60 1.56 0.90 1.38 1.73 80 1.38 0.50 1.24 1.20 100 1.27 0.46 1.16 1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1 Image Reconstruction Result Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com-pare reconstruction using DL-HMC++ to NMC SIM Du SFE and DL-HMC with the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects demonstrating improvements of 39.5% and 38.3% respec-CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject 2 Subject 1 Subject 1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation mm Rotation deg Times Second Times Second Times Second DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters from top to bottom translation in x y z directions and rotation about the x y z axes from gold-standard Vicra orange DL-HMC yellow and our proposed method blue on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies DL-HMC++ consistently demonstrates the smallest average MDE underscoring the robustness and effectiveness of our proposed method. Compared with Du SFE DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods as indicated by the error maps. In the m CT 18F-FPEB study NMC and SIM produce higher visual errors than the DL methods. Notably DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view Fig. 7 indicates that DL-HMC++ eliminates motion blurring for the caudate area and the GM-WM interface can be delineated. 2 Brain ROI SUV Evaluation We average ROI SUV evalu-ation results across all 20 testing subjects in the HRRT 18F-FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold-standard Tab. V. Overall DL-HMC++ outperforms all other methods achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC DL-HMC++ demonstrates superior performance with a 1.5% improvement in mean SUV difference for 18F-FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods p 0.05. For 18F-FPEB DL-HMC++ and Vicra are nearly identical with a 0.5% average difference. Notably SIM performs worse than NMC indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG the cross-tracer experiment yields comparable results with 0.23 mm higher for translation and 0.22 higher for rotation. For 18F-FPEB the cross-tracer results show 0.20 mm higher translation error and 0.15 higher rotation error than directly training results but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3 MDE Evaluation Result Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO ADR SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean SD 7.9 2.7 6.8 2.3 2.7 1.3 1.4 0.6 1.2 0.5 1.7 1.0 3.3 2.2 1.0 0.5 0.6 0.2 0.5 0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92 1.86 1.96 1.59 SIM 1.86 0.54 1.59 0.53 DL-HMC 0.65 0.41 0.80 0.61 Du SFE 0.44 0.23 0.76 0.72 DL-HMC++ 0.39 0.11 0.65 0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. mm Rot. deg Transverse Coronal Sagittal 18F-FDG NMC 6.29 5.79 3.12 1.42 11C-UCB-J to 18F-FDG 1.50 0.37 1.38 1.52 DL-HMC++ on 18F-FDG 1.27 0.46 1.16 1.20 Fig. 5. 3D PET Cloud Image PCI Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions top 323 voxels middle 643 voxels and bottom 963 voxels. 18F-FPEB NMC 2.42 1.43 1.36 0.48 11C-LSN3172176 to 18F-FPEB 0.74 0.02 0.55 0.00 DL-HMC++ on 18F-FPEB 0.54 0.00 0.40 0.00 IV. DISCUSSION DL-HMC++ a novel supervised deep learning model for PET head motion estimation with a cross-attention module demonstrates effective motion estimation capabilities with-out the need for external hardware-based motion tracking HMT on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua-tion on two different PET scanners HRRT and m CT using four different tracers 18F-FDG 18F-FPEB 11C-UCB-J and 11C-LSN3172176 shows that DL-HMC++ outperforms other benchmark SOTA methods yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita-tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism our model establishes spatial cor-respondences between the reference and moving PCIs which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation the cross-attention mechanism guides the network to focus on motion-relevant information diminishing the influence of irrelevant features. This process not only enhances the precision of the motion estimation but also improves robustness across the scan duration. Remarkably despite extremely blurry images Fig. 5 DL-HMC++ demonstrates anatomical motion errors of magnitude 1 mm Tab. VI that are far below the input PCI voxel size CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.977 NMSE 0.009 SSIM 0.960 NMSE 0.024 SSIM 0.942 NMSE 0.034 SSIM 0.844 NMSE 0.131 SSIM 0.956 NMSE 0.023 1.00 0 40 Intensity k Bq/cm3 Caudate 0.00 Thalamus -1.00 SSIM 0.965 NMSE 0.013 SSIM 0.944 NMSE 0.028 SSIM 0.878 NMSE 0.065 SSIM 0.889 NMSE 0.071 SSIM 0.938 NMSE 0.027 1.00 0 40 Intensity k Bq/cm3 0.00 Parietal -1.00 SSIM 0.966 NMSE 0.013 SSIM 0.923 NMSE 0.040 SSIM 0.884 NMSE 0.060 SSIM 0.801 NMSE 0.138 SSIM 0.942 NMSE 0.026 1.00 0 40 Intensity k Bq/cm3 Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method we implemented the IMR method following Spangler-Bickell s work on the m CT dataset. However the motion estimation result reveals that all DL methods especially DL-HMC++ outperform the IMR result. In addition we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F-FPEB dataset. Following optimization strategies in mo-tion estimation performance without 6-mm Gaussian filtering FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR where filtering and dynamic reference frame did not affect the performance. Notably compared with DL-HMC++ a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames alongside the requirement for fine-tuning for different tracers. In our studies due to the patient s posture for the PET scan movements in the rotation along the Y-axis vertical direction TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. mm Rot. deg IMR 1.64 0.49 0.78 0.34 w/o filter 1.55 0.54 0.77 0.35 w/o FRI 4.30 6.31 1.43 0.46 w/o dynamic reference 1.53 0.40 0.76 0.34 of 10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset e.g. the IMR result on 11C-LSN3172176 dataset mean translation error 2.32 mm compared to the 18F-FPEB dataset mean translation error 1.38 are expected. This is due to the intensity variations and noise in the dynamic input data especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING VOL. XX NO. XX XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM 0.989 NMSE 0.019 SSIM 0.986 NMSE 0.023 SSIM 0.861 NMSE 0.343 SSIM 0.852 NMSE 0.369 SSIM 0.988 NMSE 0.021 1.00 0 20 Intensity k Bq/cm3 Caudate 0.00 -1.00 SSIM 0.970 NMSE 0.027 SSIM 0.965 NMSE 0.034 SSIM 0.746 NMSE 0.351 SSIM 0.739 NMSE 0.353 SSIM 0.969 NMSE 0.028 1.00 Intensity k Bq/cm3 0 20 0.00 -1.00 SSIM 0.960 NMSE 0.030 SSIM 0.956 NMSE 0.037 SSIM 0.758 NMSE 0.296 SSIM 0.755 NMSE 0.288 SSIM 0.956 NMSE 0.034 1.00 Intensity k Bq/cm3 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small making it challenging for the model to capture. One reason is that Y rotation is less frequent than X horizontal direction rotation and Z patient bed movement direction rotation resulting in less variability in Y rotation for the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore the cross-tracer generalization experiment highlights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding our findings indicate that it introduces redundancy the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information resulting in overfitting on the training data. In the ablation study we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal-to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points e.g. 18F we demonstrated that DL-HMC++ exhibits gener-alizability across different tracers. Less intuitively performing the cross-tracer experiment in the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data. Theoretically motion parameters can also be directly estimated from sinograms and it is feasible to employ deep learning algorithms for this purpose. However part of our dataset includes TOF information which causes the sinogram size to be much larger than the image size. In the future we will explore the possibility of applying DL-HMC++ to other domains such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limitations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes such as the first 10 minutes post-injection. Moreover Vicra failure and inaccuracy may have a negative effect on the proposed supervised model. In the future we aim to develop a generalized model to various tracers and scanners, including an ultra-high-performance human brain PET/CT scanner with a spatial resolution of less than 2.0 mm, which is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised and unsupervised learning approaches for PET head motion estimation. In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention enhances the model's ability to track motion by establishing spatial correspondences between the two images to be registered and by focusing network learning on the most informative regions for head motion. We validated DL-HMC++ in a large cohort PET study using four different tracers across more than 280 subjects. The results showed significant improvements in motion estimation performance both qualitatively and quantitatively compared to state-of-the-art data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of DL-HMC++ for addressing PET head motion estimation without requiring hardware-based motion tracking. Additionally, the cross-tracer generalization experiment highlights the potential of the proposed network to generalize effectively across different tracers.""}]",Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset.
Why is preprocessing important for Ethic-BERT's performance?,2510.08770v1,2510.12850v1,False,"['2510.08770v1', '2510.14855v1', '2510.12850v1', '2510.13937v1', '2510.08116v1']","[5.0, 4.0, 4.0, 4.0, 4.0]","['Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation']","[{'rank': 1, 'score': 5.0, 'id': '2510.08770v1', 'title': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'text': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform. This paper presents a real-time spill detection system that utilizes pretrained deep learning models with RGB and thermal imaging to classify spill vs. no-spill scenarios across varied environments. Using a balanced binary dataset 4 000 images our experiments demonstrate the advantages of thermal imaging in inference speed accuracy and model size. We achieve up to 100% accuracy using lightweight models like VGG19 and Nas Net Mobile with thermal models performing faster and more robustly across different lighting conditions. Our system runs on consumer-grade hardware RTX 4080 and achieves inference times as low as 44 ms with model sizes under 350 MB highlighting its deployability in safety-critical contexts. Results from experiments with a real robot and test datasets indicate that a VGG19 model trained on thermal imaging performs best. Additionally when a warm liquid contacts a cooler floor evaporation and heat transfer create characteristic gradients at the spill boundary. These gradients increase the effective contrast at edges and make the spatial pattern of the spill more pronounced in thermal imagery than in RGB. Liquids and common flooring materials e.g. tile wood often have different emissivities as well. Even when temperature differences are small differences in emissivity can change the intensity measured by the thermal camera helping contrast liquid spills from the floor. Since thermal cameras sense emitted IR rather than reflected visible light thermal-based systems remain effective under low light glare or complex reflections where RGB-based detectors commonly fail the potential impact of variable lighting conditions is decreased. Spill detection is essential for public safety in dynamic environments such as cafes restaurants and retail spaces where unnoticed spills frequently lead to slips and injuries. Traditional detection methods often depend on intrusive hardware or suffer from delayed response times highlighting the need for fast vision-based solutions. However RGB-based computer vision systems are vulnerable to lighting changes and surface reflections. To address these limitations we integrate thermal imaging and evaluate its effectiveness using pretrained convolutional neural networks CNNs for real-time accurate and lightweight spill detection. There are certain motivations for exploring the use of thermal imaging in this task. Overall these properties help explain why thermal input often contains a cleaner less ambiguous signal that a CNN can exploit with fewer parameters and less preprocessing. Despite these advantages thermal imaging is not universally superior. There are some important failure cases. For example if a spilled liquid has the same temperature as the surrounding floor and similar emissivity e.g. a beverage left long enough to reach thermal equilibrium with the floor thermal contrast vanishes and the thermal image can become indistinguishable from the background causing missed detections. Thin films of liquid also exchange heat rapidly with the floor and may equilibrate faster than bulk spills. Thermal imaging offers physical advantages that explain potential strong performance. Thermal infrared/IR cameras measure emitted long-wave IR radiation from surfaces rather than visible light the recorded signal is proportional to surface temperature and emissivity. Spills produce detectable thermal signatures for several reasons. Environmental heat sources may also impact results. Nearby heaters sunlight patches or warm machinery create thermal clutter and false positives. Reflections of thermal radiation on Many spills hot or cold beverages have temperatures differing from the ambient floor and therefore show up as highly reflective surfaces and heat conduction through layered floors can produce artifacts that mimic spill-like signatures. Multi-Modal Hybrid Systems Several works fuse RGB with other modalities to improve spill detection. As noted Yang et al. s p RGB-D sensor integrates polarization and depth cues with RGB to detect floor water. Bao et al. explicitly fuse infrared thermal and visible cameras first the IR channel isolates the leak region then the visible channel refines it. Similarly Zhang Zhang combine thermal imaging with gas sensor data in a deep-learning architecture for pipeline leaks. These hybrid systems leverage complementary modalities for instance IR can distinguish cold liquid pools while RGB provides high-resolution spatial detail. However many existing hybrid methods use hand-engineered fusion or multi-stage pipelines. In contrast our approach tests the efficacy of end-toend CNNs on the combined data e.g. stacking RGB+thermal channels so that a single pretrained network learns from both modalities. That is rather than separate IR processing we feed multi-modal imagery into one model. RGB and RGB-D Vision for Spills Prior computer-vision research on spill or wet-surface detection has largely focused on RGB imagery and occasionally RGB-D data. For example Bhutad and Patil released a dataset of 1 976 labeled images of stagnant water and wet surfaces enabling supervised learning for water/vs-dry classification. Using such RGB datasets Gawdzik and Orłowski applied Mask R-CNN to detect and segment spilled liquids in an industrial setting. Their approach trained a two-class deep detector on RGB images of known spills achieving high accuracy in identifying liquid regions. While these methods demonstrate that CNNs can learn visual features of liquids on floors they rely solely on appearance cues and can struggle with low contrast or reflections. Related work in robotics has used RGB-D sensors to aid water detection Yang et al. propose a polarized RGB-D p RGB-D framework that fuses color polarization and depth to detect water hazards puddles for visually impaired navigation. By combining polarization cues with depth data the p RGB-D system can disambiguate specular reflections. Such multi-cue approaches improve robustness but add hardware complexity. In summary pure-RGB CNN methods e.g. Mask R-CNN can learn to detect spills when ample labeled data is available but have limited reliability under challenging lighting or surface conditions e.g. dark floors low contrast. Lightweight Real-Time Models For robot or mobile deployment efficiency is critical. Lightweight CNN architectures e.g. Mobile Net Squeeze Net Tiny-YOLO have become popular for on-device vision. For example Bouguettaya et al. survey mobile CNNs and note that Mobile Net with width 0.25 resolution 0.714 can run at 28 FPS on an NVIDIA Jetson TX2. They also note Mobile Net-SSD an object detector uses depthwise convolutions and achieves strong COCO accuracy and that SSDLite Mobile Net V2 backbone outperforms YOLOv2 on COCO dataset with 20x more efficiency and 10x smaller model size. Other works similarly demonstrate that reduced-channel or quantized networks can meet real-time constraints while retaining acceptable accuracy. In the context of spills few papers explicitly focus on model size but the insight is clear one can deploy CNNs on robots by using such lightweight variants. Our work differs in that we use pretrained networks like full-size Res Net or Efficient Net for accuracy and then evaluate their feasibility on consumer hardware and a robotic platform. By comparing performance across standard pretrained models and modalities we assess the tradeoffs directly. Thermal Imaging for Liquids and Leaks Thermal infrared cameras have been studied for liquid and leak detection leveraging the temperature contrast between liquid and surroundings. Appuhamy et al. developed a thermal-camerabased method to map surface moisture. They spray liquid onto a surface and use an IR camera plus image processing to highlight areas with large temperature gradients detecting wet coverage noninvasively. Their experiments show that liquids with 5 C difference from the background are reliably detected. In industrial maintenance Bao et al. designed a dual-camera system combining IR and visible light to find colorless fluid leaks e.g. water on hot pipework. The system uses IR images to locate cooler effusion regions and then uses visible-light images to refine the contour of the leak. As Bao et al. explain the overall process first uses infrared imaging to locate a potential effusion and then conducts a secondary assessment for the final effusion characterization. This pipeline approach isolates candidate leaks via thermal contrast and then confirms them via visual processing. More broadly thermal imaging is widely used in gas and pipeline leak detection. For example Zhang Zhang note that thermal images often from optical gas imaging are less prone to be affected by weather and provide an orthogonal data source for leak monitoring they feed thermal images into CNNs Alex Net Res Net Mobile Net etc. as part of a multi-sensor pipeline leak detection system. In sum these studies show that IR cameras can reveal spills or leaks invisible in RGB especially transparent fluids and that combining IR with image analysis sometimes with additional sensors yields high detection accuracy. Comparison to Our Approach In summary prior methods use specialized cues or multi-stage pipelines e.g. polarization thermal+visible etc. or custom CNNs. In contrast our approach evaluates general CNN classifiers pretrained on large datasets and fine-tuned for spill detection. We compare their performance on RGB thermal and fused input explicitly measuring inference speed and accuracy. Unlike which engineer modality-specific preprocessing we investigate to what extent off-the-shelf deep models can handle spill detection under different modalities. This yields insight into whether a unified learned model can replace complex handtuned pipelines. To be able to create a model to detect spills it is first necessary to collect sample images to train the model on. Since standard RGB cameras struggle to distinguish the floor from water spills similar to human vision data was collected with a combination of a thermal camera and an RGB camera. A Topdon TC001 was used as the thermal camera while a Genius Wide Cam F100 was used as the RGB camera. A Python script was used to save images from the thermal camera and RGB camera at the same time which can be found in the following Git Hub repository https //github.com/Aeolus96/Thermal Camera Capture. Spill images were put into the spill folder and images without spills were put into the no-spill folder. By the end of the research 4000 total images were collected evenly split across the image types RGB and thermal and evenly split across the classes spill and no-spill. Four liquids water Coke red juice and yellow juice were used and two rooms were used in data collection Atrium and J234. This resulted in 8 combinations Room x Liquid x Modality. A variety of spill sizes were collected forming a diverse spill dataset. Within these sizes a typical small spill would have a diameter of 2-4 inches with the regions being approximately circular. A typical large spill would have a diameter of up to 12 inches assuming an approximately circular region. Over time these large spill regions deformed as the liquid flowed across the floor which led to an increase in certain dimensions. The atrium had porcelain tiled floors while J234 had polished concrete flooring. Foot traffic was not included in pictures in the dataset. In the atrium lighting conditions were kept consistent while lighting conditions in J234 were dynamic due to the high presence of natural lighting and variability of sunlight. Training In order to determine whether RGB thermal or combined images would yield the best accuracy and efficiency a VGG19 model was trained on each type and the resulting model was tested on a test set and real-time input. The same hardware was used to train the model as was used to collect data A Lenovo Legion Pro 7i with NVIDIA RTX 4080. Data used and real-time testing area were homogeneous across room and liquid types for this comparison For the training strategy the last 5 layers were fine-tuned an RMSprop optimizer was used lr 1e-5 a binary crossentropy loss function was used and early stopping was used with patience 5. A batch size of 8 was used for training and validation while a batch size of 2 was used for test. The model was trained with up to 50 epochs however this was never reached due to the application of the early stopping mechanism. For data augmentation images were randomly flipped horizontally slightly rotated and had small variations in contrast during training The random rotation and contrast had factor 0.01. The standard VGG preprocessing pipeline was then used. The results are found in Table I. Testing The results show that Combined although accurate on the test dataset is not effective in real-time testing. RGB although effective in real-time testing and near-perfect in the test set is slower for inferences and has a model size three times that of thermal. Thermal performs with 100% accuracy on the test set and real-time testing along with offering the shortest inference times and the smallest model size. Our results align with recent findings that show the effectiveness of deep convolutional architectures in leak detection tasks using thermal imaging. Data collection was performed using a robot with a thermal and RGB camera mounted to it. A Lenovo Legion Pro 7i with NVIDIA RTX 4080 was used for control and image collection. The cameras were plugged directly into the laptop and the robot was controlled using a USB joystick plugged in to the laptop with inputs being relayed to the robot through serial communication. Images were collected from a variety of positions and angles and in isolation from environmental heat sources. Robotic inspection using thermal vision has proven effective in industrial environments for leak detection and has been implemented in pipeline inspection systems. We proceed by probing the efficacy of the thermal-VGG19 configuration by creating individual models for each roomliquid combination and noting accuracy on test datasets and realtime testing. The same training strategy was employed. The results are found in Table II. To split the data another script was used on collected images. The dataset was split 70-20-10 training validation test. Another script was used to match the view of RGB images and thermal images through cropping and a perspective transformation. Additionally for the purpose of experimentation a third modality was introduced side-by-side combined images. Transformed RGB images and thermal images were combined through side-by-side concatenation with thermal on the left and RGB on the right. The resolutions were 256x192 for thermal 640x360 for RGB and 512x192 for sideby-side RGB was downscaled to 256x192 prior to concatenation. Based on the high accuracy consistently indicated throughout these results we note that a thermal-VGG19 configuration has high efficacy in a variety of situations given isolation from environmental heat sources. This is likely due to the homogeneity of no-spill images when environmental heat sources are not introduced. Differentiation between the two classes may potentially be reduced to identifying changes in the homogeneity of thermal input. Training In order to identify pretrained models with higher time efficiency without sacrificing accuracy a collection of various models were trained on the comprehensive data set and their accuracy over the test data set was logged. The results can be found in the first two columns of Table III. Real-time Testing Although all model choices performed with high accuracy Nasnet Mobile seems promising seeing that it was the only model that could match VGG19 s 100% accuracy. In order to determine which model would be better for a real life scenario real-time testing was performed to compare inference times with the goal of selecting the most efficient and Fig. 1. A diagram of the data collection and testing setup TABLE I. RGB VS THERMAL VS COMBINED Image Type % Test Demo Accuracy Model Inference Size Time Thermal 100% 100% 324.6 MB 44 ms RGB 98.84% 100% 1.0 GB 55 ms Combined 100% 60% 525.9 MB 47 ms TABLE II. THERMAL-VGG19 ACCURACY Fig. 2. Side-by-side comparison of no-spill and spill thermal images Room Liquid % Test Demo Accuracy Model Inference Size Time It is found that a model using Nasnet Mobile yields greater inference times by 9 ms on average in the trials and a greater model size by 115.7 MB as well. VGG19 performs with quicker inferences and smaller model size while achieving the same accuracy. It is determined that VGG19 remains a more appropriate model than Nasnet Mobile for this application. Atrium Water 100% 100% 324.6 MB 44 ms Atrium Coke 100% 100% 324.6 MB 45 ms Atrium Red Juice 100% 100% 324.6 MB 45 ms Atrium Yellow Juice 100% 100% 324.6 MB 45 ms V. J234 Water 100% 100% 324.6 MB 45 ms Through our experimentation it is found that in environments with diverse lighting conditions and isolation from environmental heat sources a VGG19 image classification model trained on thermal imaging offers the best performance as measured by inference time test accuracy and accuracy in real-time deployment. J234 Coke 100% 100% 324.6 MB 45 ms J234 Red Juice 100% 100% 324.6 MB 45 ms J234 Yellow Juice 100% 100% 324.6 MB 45 ms Both All 100% 100% Atrium 100% J234 324.6 MB 44 ms VI. FUTURE WORK In future work it may be useful to perform experiments without isolation from environmental heat sources such as foot traffic. Future directions include exploring ensemble methods that fuse RGB and thermal features as motivated by recent surveys on multi-sensor data fusion in leak detection systems. This could potentially allow for misclassifications of environmental heat sources on thermal imaging to be corrected through distinction in RGB imaging. lightweight model. The results can be found in the last three columns of Table III. Demo accuracy model size and inference time are omitted for models other than VGG19 and Nasnet Mobile in Table III since a comparison is made only between VGG19 and Nasnet Mobile due to accuracy on the test dataset being lower for other models. It is found that a model using NasNet Mobile yields greater inference times by approximately 9 milliseconds on average in the trials, as well as a larger model size by about 115.7 megabytes. VGG19 provides faster inference and a smaller model size while achieving the same accuracy. Based on these findings, VGG19 remains the more appropriate model than NasNet Mobile for this application. Through our experimentation, we observe that in environments with diverse lighting conditions and minimal influence from environmental heat sources, a VGG19 image classification model trained on thermal imaging offers the best performance. This conclusion is supported by inference time, test accuracy, and accuracy under real-time deployment conditions.'}, {'rank': 2, 'score': 4.0, 'id': '2510.14855v1', 'title': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution', 'text': 'A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution. Early detection of melanoma has grown to be essential because it significantly improves survival rates but automated analysis of skin lesions still remains challenging. ABCDE which stands for Asymmetry Border irregularity Color variation Diameter and Evolving is a well-known classification method for skin lesions but most deep learning mechanisms treat it as a black box as most of the human interpretable features are not explained. In this work we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect opening more windows for future exploration. The A B C and D values are quantified particularly within this work. Moreover this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary the classification worked with an accuracy of around 89 percent with melanoma AUC being 0.96 while the feature evaluation performed well in predicting asymmetry color variation and diameter though border irregularity remains more difficult to model. Overall this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria thus improving our understanding of skin cancer progression. Melanoma an aggressive form of skin cancer is one of the leading causes of death due to skin cancer. Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. In order to differentiate between harmful and harmless lesions dermatologists utilize the ABCDE method. A stands for asymmetry as malignant skin lesions often appear to be uneven B stands for border irregularity as scientists search for jagged or notched edges C Preprint submitted to ar Xiv October 17 2025 stands for color variation D stands for diameter as larger lesions are more likely to be malignant and E stands for evolving as skin lesions evolve over time. If a lesion displays two or more of the attributes described above the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions. Many deep learning techniques are proficient at classifying skin lesion images as either benign malignant or one of several other clinically recognized categories found in datasets such as HAM10000. Convolutional neural networks are utilized most commonly as they are trained to classify images. However these CNN models lack clear interpretability because they only provide point predictions of lesion type without explanation. In the context of melanoma detection this lack of explanation can hinder clear interpretable diagnoses. The E feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes but this is difficult to enforce. However using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example J utte et al. 2024 utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma. As discussed above the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns. In this work we propose a deep-learning framework that combines classification ABCDE feature quantification and feature evolution simulation. First we design a CNN that learns to predict both the lesion s class and quantify continuous values representing each ABCDE criterion. Second we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpolation are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Researchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry border irregularity color variegation and lesion diameter. For example in 2001 Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition. Asymmetry was quantified via shape moments border irregularity via fractal dimension or edge abruptness color variegation via histogram analysis and diameter via lesion area in pixels. These pipelines showed that it is feasible to use computer vision for melanoma screening but they often need expert parameter tuning and struggle with variations in image quality. A recent survey by Celebi et al. 2022 summarizes many such efforts to automate parts of the ABCD rule. In general while these approaches brought interpretability each feature could be reported their accuracy 2 was typically lower than that of data-driven deep learning which can learn more complex representations. In addition the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into categories such as melanoma basal cell carcinoma nevus etc. For example on the HAM10000 dataset approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy often 85 90%+ in 7-class classification. Some research focuses on binary classification melanoma vs. benign as they report very high specificity and good sensitivity. Still as described above these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However these pixel-level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. 2024 proposed an ABC ensemble model that preprocesses images to emphasize asymmetry border and color regions as they feed them into specialized network branches these are then combined for classification. Still their model did not output quantitative values for each criterion even though they were able to use ABCD rule knowledge to improve classification performance. Notably they omitted the diameter feature because the scaling in the data was inconsistent. In summary this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion we first optionally preprocess it including lesion segmentation and color normalization. The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A B C and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change it basically gives a trajectory of the features. Additionally the model s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image followed by two heads output branches. These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks. Still the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net but we ultimately chose Res Net50 due to its balance of depth and efficiency. The classification head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to A B C D E is not supervised. Overall we use linear outputs with appropriate activation/normalization we do this to make sure that the feature values fall in a reasonable range 0 to 1 for A B C and a scaled range for D as discussed later. The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048-dimensional feature vector. This vector represents high-level information about the lesion. Also the network is trained to predict the ABCDE features so the vector encodes information relevant to asymmetry border color and others in addition to other features useful to classify lesions. After that we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000. These include nv mel bcc akiec bkl df and vasc and correspond to melanocytic nevus melanoma basal cell carcinoma actinic keratosis benign keratosis dermatofibroma and vascular lesion. During training we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing A B C D E. No activation linear output is applied for regression. However these values are constrained through the training data scaling and loss function this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0 but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable A. Border Irregularity B An irregular border is one that is ragged notched or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border. For shape irregularity we compute the lesion s convex hull and compare it to the actual border. After that we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0 but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity we take an approach similar to that of J utte et al. we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image calculate the gradient around the border and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to. For the overall border irregularity score we combine shape and gradient. In this equation the gradient B value is inverted because a low gradient fuzzy border should increase the irregularity score. Thus a lesion with a very jagged shape or a very blurred edge will have B near 1. Color Variation C We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown dark brown black blue-gray white and red. A melanoma often has different colors. To quantify this value we compute the dispersion of colors in the lesion. Specifically we apply a clustering in color space to the lesion pixels. The number of color clusters is determined and then the dispersion index is calculated this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally we count the number of distinct color clusters. If there are more clusters there is more color variety. We map the number of clusters to a 0 1 range as well. Our final color variation score C is a combination we used the dispersion index primarily and added a small increment for each distinct color beyond one. Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown black and red would score more than 0.7. Diameter D For the most part lesions with a diameter greater than 6 millimeters are deemed as suspicious. However these images lack a consistent physical scale as the zoom level varies. HAM10000 images come from different devices and magnifications. Unfortunately no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation we compute the maximum distance across the lesion in pixels this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score we make an assumption that an average nevus in the dataset say 30 pixels across in the image corresponds to about 5 mm this is based on typical dermoscope resolution. D clamp max diameter pixels p6 mm 0 1 This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali-bration one could include a reference scale or use the dataset s metadata some ISIC images have ruler markers but this is not the case in HAM10000. In our training labels we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a continuous variable because it influences risk in a graded way even though it is non-linear. Evolving E Because the dataset does not contain time-series images we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A B C and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example if a lesion image is very close-up the diameter estimate may falsely appear large if lighting causes part of the border to fade into the skin the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter using inpainting for dark hairs and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A B C and D value. We use these as the target outputs for the regression head. In this work E is not directly mentioned but only represents the evolution or progression of the other A B C and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset this dataset contains 10 015 der-moscopic images of pigmented lesions across 7 diagnostic categories. However images are not evenly distributed across diagnostic categories benign and melanoma. To handle this we perform data balancing. Specifically we use a combination of oversampling and class-balanced loss. During each training epoch we sample images such that each class is roughly equally represented this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner 70% of the images for training 10% for validation and 20% for testing. We also improve the training images with random horizontal or vertical flips small 7 rotations zoom-in and out and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image we resize it to 224x224 apply the hair removal filter normalize the color and segment the lesions. For the loss functions we have to combine the losses for both the heads of the model classification and regression. The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments we set this value to be 1. However it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting we apply L2 weight decay with a value of 0.00001. All of these experiments use PyTorch and are trained on an NVIDIA A100 GPU. For lesion classification we evaluate standard metrics including the overall accuracy per-class accuracy precision sensitivity and F1-score for each class and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression we measure the Pearson correlation between the predicted and ground-truth feature values on the test set we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features ABCD are approximate we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for E on static images so we cannot directly evaluate E predictions in the standard test set. To take care of the E we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally we propose a method to simulate how a lesion s image and thereby its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al. a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign-appearing lesions to the domain of malignant-appearing lesions. The Cycle GAN learns to produce an image that does indeed retain the structure of the input like the particular lesion s shape and significant features but it changes its appearance to resemble a melanoma. Evidently according to what was mentioned above with the ABCD rule higher asymmetry more colors and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN s cycle-consistency this is so 8 that the network doesn t simply always output a generic melanoma. To simulate a gradual evolution we use frame interpolation this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant. An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion say the 2048-d feature from the CNN and model how it would drift as the lesion evolves. The multi-task CNN s feature extractor would be a state encoder in this scenario. Then the time steps would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence we collect CNN features from each frame and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features and this helps it learn how features evolve as a lesion becomes more malignant. After training we can simulate progression by applying small steps in the predicted direction. For example starting with a benign lesion s features we can apply several steps and watch the ABCD scores increase this parallels malignant transformation. While this method does not generate images it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al. These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study the latter approach directly observing feature values is performed to visualize plausible ABCD trajectories and no image synthesis is used in the reported experiments. On the HAM10000 dataset the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89% it correctly classified 89% of all test samples. Also it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types even the less common ones. In the specific task of finding the difference between melanoma and all other lesion types the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve AUC is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives. The confusion matrix in Figure 3 shows how often each lesion type was correctly or incorrectly predicted. Common classes such as melanocytic nevus (nv) and basal cell carcinoma (bcc) achieved very high recall, meaning the model rarely missed them. However, for rarer classes such as dermatofibroma (df) and vascular lesions (vasc), recall was lower at around 50% and 82%, indicating greater difficulty in identifying these categories. The recall for melanoma was approximately 76%. Most errors occurred between melanoma and visually similar benign lesions such as benign keratosis or nevus, highlighting the challenge of distinguishing classes with similar dermoscopic appearance. The model demonstrated mixed accuracy for the ABCD feature predictions. For asymmetry (A), color variation (C), and diameter (D), the model performed well with low average error and strong correlations between predicted and true values, indicating that it learned meaningful clinical patterns. However, border irregularity (B) was significantly harder to predict. Its prediction error was higher, and the correlation between predicted and true B values was weak. This may be due to noisy labels or limitations in how border irregularity was quantified. When broken down by class, B remained difficult to estimate across most lesion types, while A, C, and D errors varied more by class. For instance, melanoma cases had relatively accurate color predictions, while vascular lesions had larger diameter errors. The correlation matrix in Figure 5 shows the relationships between the predicted ABCD features across all test samples. Values closer to 1 indicate stronger positive correlations. A strong correlation of 0.78 between border irregularity and diameter is observed, which makes sense clinically because larger lesions often have more irregular borders. Moderate correlations were found between other pairs of features, such as asymmetry with color variation and diameter (around 0.38). Although the ABCD features are conceptually distinct, some are meaningfully related, reflecting how certain visual traits of skin lesions commonly co-occur. Importantly, the features are not so strongly correlated that they become redundant; this shows that the model predicts separate, clinically useful characteristics rather than duplicating the same information. Figure 6 illustrates how the lesion representation evolves in the model’s internal feature space using latent-space interpolation. Each step from 0 to 5 represents a gradual simulated evolution toward a more malignant lesion. PC1 and PC2 are the top two principal components from PCA, reducing the high-dimensional feature vectors to a 2D space while retaining most of the variation. The trajectory progresses smoothly, indicating that the model has learned a consistent and directional concept of lesion progression even without explicit temporal training data. The slight zigzag motion on PC2 suggests subtle nonlinear shifts in color or shape, while the monotonic increase along PC1 reflects a gradual trend toward higher malignancy. This supports the idea that the latent space encodes clinically meaningful information. Figure 7 shows how ABCD scores evolve across six simulated steps in the latent space. Each line represents one of the ABCD criteria. Asymmetry (A), color variation (C), and diameter (D) increase smoothly across steps, consistent with clinical expectations for malignant progression. These upward trends show that the model captures expected progression patterns without being trained on temporal lesion data. In contrast, the border irregularity (B) score remains flat near zero, confirming earlier findings that B is difficult for the model to predict accurately, likely due to noisy or insufficient training labels. The lack of progression in B reveals a current limitation in modeling that specific clinical feature. Overall, the multi-task CNN performs well by combining accurate lesion classification with interpretable ABCD feature predictions. For melanoma detection, it achieves an AUC of 0.96. The model accurately predicts asymmetry, color variation, and diameter and embeds these features meaningfully in the latent space, as evidenced by smooth trajectories and increasing malignancy indicators during evolution simulations. One limitation is the poor performance on border irregularity prediction, likely caused by simplistic segmentation heuristics used during labeling. Class imbalance also affected classification and regression accuracy, particularly for rare lesion types. Another limitation is that the evolution simulation occurs in latent space rather than directly on images, making visual interpretation less intuitive. This system can assist clinicians not only with diagnosis but also with interpretation through ABCD feature outputs. The evolution simulation provides potential what-if visualizations of lesion progression, supporting patient education and monitoring. Improving border irregularity prediction will require better labeling methods, such as expert annotation or advanced segmentation. Addressing class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using ABCD features, and simulate possible lesion progression. The model provides robust predictions of asymmetry, color, and diameter but struggles with border irregularity. Nonetheless, it offers an interpretable and clinically meaningful framework for analyzing lesions and anticipating potential changes. Future work will focus on improving border score prediction, generating realistic image evolutions, integrating expert-labeled data for the E feature, and testing on more diverse datasets.'}, {'rank': 3, 'score': 4.0, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to bridge the gap between human values and computational frameworks. Ethical morality involves the principles of right and wrong that guide human behavior encompassing dimensions such as justice fairness well-being duties and virtues. These principles are deeply interconnected often leading to conflicts that require nuanced decision-making. Humans rely on cultural social and personal contexts to navigate moral ambiguities but replicating this capacity in AI systems demands sophisticated techniques. The integration of ethical reasoning into AI is particularly important because of its potential societal impact. AI systems if left unchecked can amplify biases produce harmful outputs or make decisions that conflict with shared human values. To address these issues researchers have turned to text-based scenarios as a means of evaluating AI systems ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP particularly the development of transformer architectures have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements presenting scenarios derived from philosophical theories including justice deontology virtue ethics utilitarianism and commonsense morality. These benchmarks challenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements progress in embedding ethical reasoning into AI has been limited. Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include Lack of high-quality datasets that reduce ambiguity and enhance representativeness. Existing models struggle with nuanced ethical reasoning limiting accuracy in moral decision-making. AI models rely on spurious correlations rather than deep moral reasoning leading to misclassifications in complex ethical scenarios. The dataset primarily reflects Western moral perspectives reducing its applicability to diverse cultural and ethical viewpoints. In this research we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and inappropriate material. Recent research efforts have focused on developing robust detection techniques using machine learning ML and deep learning DL models to improve accuracy efficiency and adaptability. At the same time the ethical and moral implications of content have also become crucial requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores significant contributions in this field focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection Bhatti et al. proposed an Explicit Content Detection ECD system targeting NSFW content using a residual network-based deep learning model. Their approach integrating YCb Cr color space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the input text was tokenized dynamically using BERT s Word Piece algorithm. Each tokenized word w was decomposed into subword tokens. In Equation 2 V is BERT s fixed vocabulary. Instead of relying on standard segmentation we employed frequency-aware tokenization ensuring sub-words were split efficiently based on their corpus occurrence. In Equation 3 P T w denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training this adjustment helped the model generalize better to unseen words. Tw t1 t2... tn ti V 2 T w arg max T P T w 3 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4 a sequence S is shorter than L padding tokens PAD are appended. The exponent notation L S represents the number of padding tokens added to match the fixed length L. For example if S has 8 tokens but L 12 then 4 PAD tokens are appended. To prevent overfitting due to excessive padding we implemented batch-wise dynamic padding which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant PAD tokens leading to faster training and reduced computational overhead. S S + PAD L S 4 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. is particularly effective due to its ability to preserve case distinctions which are often vital in formal and ethical text analysis. This ensures that proper nouns legal terms and acronyms retain their intended meanings reducing ambiguity in ethical and policy analysis. Research highlights the importance of case sensitivity in legal and ethical texts as it helps differentiate between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model resilient to noisy or imbalanced data leading to more reliable and ethically sound decisions. H task D Htask 12 Hinference task 1 p Htask 13 3.6 Evaluation Matrix The model s performance was evaluated using accuracy precision recall F1-score and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense Justice Virtue domains and Deontology reaching 86.46% 78.22% 83.40% and 81.23% respectively. These results highlight 10 the model s ability to effectively adapt to the task in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 4, 'score': 4.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 5, 'score': 4.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}]",Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma but drops below 20% for advanced stages. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement this is pointed out by J utte et al.
What are the main strengths of using an LSTM model for real-time sign language translation?,2510.13137v1,2510.13137v1,True,"['2510.13137v1', '2510.05163v1', '2509.20913v1', '2510.13050v1', '2510.08116v1']","[14.0, 11.0, 11.0, 10.0, 10.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation']","[{'rank': 1, 'score': 14.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 11.0, 'id': '2510.05163v1', 'title': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'text': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems enabling higher accuracy resilience to spoofing and seamless integration with hardware-based solutions. At the same time smart card technologies have evolved to include on-chip biometric verification cryptographic processing and secure storage thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work 2019 2025 at the intersection of deep learning biometrics and smart card technologies for MFA. We analyze biometric modalities face fingerprint iris voice review hardware-based approaches smart cards NFC TPMs secure enclaves and highlight integration strategies for real-world applications such as digital banking healthcare IoT and critical infrastructure. Furthermore we discuss the major challenges that remain open including usability security tradeoffs adversarial attacks on deep learning models privacy concerns surrounding biometric data and the need for standardization in MFA deployment. By consolidating current advancements limitations and research opportunities this survey provides a roadmap for designing secure scalable and user-friendly authentication frameworks. In today s hyperconnected digital environment safeguarding user identity has become more critical than ever. With increasing reports of data breaches phishing scams and account hijacking traditional password-based authentication has proven insufficient. Passwords are not only prone to being forgotten or reused but are also vulnerable to brute-force attacks phishing and database leaks. These vulnerabilities have driven the widespread adoption of Multi-Factor Authentication MFA as a more robust alternative. MFA relies on the combination of independent identity factors something the user knows e.g. a password something they have e.g. a smart card and something they are e.g. a biometric trait. This layered approach significantly improves security by ensuring that compromising one factor does not grant access to protected systems. According to recent surveys modern digital services including financial platforms compliant with regulations such as PSD2 increasingly require MFA to mitigate risks associated with single-factor systems. Privacy-preserving MFA approaches leveraging deep learning and biometrics have also been proposed. 1 Biometric authentication including modalities such as face fingerprint iris and voice recognition has emerged as a popular inherence factor in MFA systems. Biometrics offer nontransferable user-specific traits improving usability and reducing reliance on memory. Yet they raise unique challenges in terms of spoofing privacy and demographic bias. Recent advances in deep learning DL have significantly improved the accuracy and reliability of biometric systems. Convolutional Neural Networks CNNs and other deep architectures have shown strong performance in extracting robust features from noisy or occluded biometric data enabling real-time and multimodal biometric authentication. DL techniques also power liveness detection domain adaptation and template security enhancing resilience against spoofing and adversarial attacks. In parallel possession-based factors have evolved. Smart cards Trusted Platform Modules TPMs and secure enclaves now provide cryptographic computation biometric match-on-card and tamper-resistant credential storage. Recent advances in biometric payment cards and NFCenabled devices illustrate how hardware tokens can securely integrate with DL-based biometric authentication to form compact and user-friendly MFA schemes. These solutions reduce fraud while ensuring compliance with data protection requirements. Despite this progress several open challenges remain ensuring usability without degrading security defending against presentation attacks preserving biometric privacy and ensuring interoperability across vendors and regulatory frameworks. Objectives. This survey provides a comprehensive overview of recent research 2019 2025 on DL-based MFA systems integrating biometrics and smart cards. Specifically it Reviews deep learning methods applied to biometric authentication within MFA frameworks. Examines smart card and hardware-based approaches for secure factor integration. Compares architectures fusion strategies datasets and benchmarks employed in state-ofthe-art systems. Analyzes threat models and countermeasures against adversarial and spoofing attacks. Identifies open research questions and outlines future directions for scalable privacypreserving and user-friendly authentication. 2 Background MFA Biometrics and Smart Cards Multi-Factor Authentication MFA is a security mechanism that requires users to verify their identity using a combination of independent factors typically categorized as knowledge something the user knows possession something the user has and inherence something the user is. The rationale behind MFA is that compromising multiple independent factors is significantly harder for attackers thus enhancing overall system security. Traditionally authentication relied heavily on passwords. However due to their susceptibility to guessing phishing and large-scale leaks passwords alone have become increasingly risky. In response MFA has been adopted across domains handling sensitive data particularly digital banking e-government and healthcare Io T. Biometric authentication represents the inherence factor leveraging physiological traits e.g. face fingerprint iris or behavioral patterns e.g. gait keystroke dynamics. Unlike passwords or tokens biometric traits are intrinsic to individuals and difficult to replicate. However they raise challenges such as privacy concerns irrevocability and vulnerability to spoofing attacks. To mitigate these risks standardization bodies e.g. ISO/IEC 30107 have proposed presentation attack detection PAD guidelines and researchers are increasingly focused on fairness and robustness across demographics. 2 Smart cards typically associated with the possession factor are tamper-resistant hardware devices capable of securely storing credentials cryptographic keys and even performing biometric matching. In the banking sector EMV-compliant cards enable secure offline authentication through digital signatures. When combined with biometrics smart cards can implement matchon-card verification ensuring that sensitive templates never leave the card s secure chip. Beyond traditional smart cards Trusted Platform Modules TPMs and Secure Enclaves extend these guarantees to general-purpose devices such as smartphones and laptops. These components isolate sensitive data and computations enabling secure biometric enrollment inference and key storage and underpin modern standards such as FIDO2 and Web Authn. In practice effective MFA requires balancing usability cost and risk. A typical modern system may combine a fingerprint scan inherence a smartphone secure enclave or biometric smart card possession and a PIN or behavioral pattern knowledge/behavior. The integration of deep learning into biometric systems coupled with trusted hardware marks the next evolution in MFA explored in detail in the following sections. Deep learning DL has fundamentally transformed biometric authentication by enabling endto-end learning robust feature extraction and scalability across diverse modalities. Traditional biometric systems relied on handcrafted features which often lacked generalizability across populations or environmental conditions. DL models particularly Convolutional Neural Networks CNNs Recurrent Neural Networks RNNs and Transformers now power state-of-the-art systems for face fingerprint iris voice and behavioral biometrics. 3.1 Facial Recognition and Anti-Spoofing Facial recognition has rapidly advanced with architectures such as Face Net Arc Face and Cos Face which learn highly discriminative embeddings from images. Wang and Deng survey modern DL-based face recognition. However face systems remain vulnerable to spoofing 2D photos replay videos 3D masks. Liveness detection networks address this by analyzing texture motion cues or physiological signals e.g. eye blinking r PPG. Guo et al. proposed CNN-based liveness detection while recent works integrate anti-deepfake detection. 3.2 Fingerprint and Iris Recognition DL enhances fingerprint authentication by improving minutiae detection ridge classification and partial print matching. Zahid et al. show CNNs outperform traditional Gabor-based methods under noisy conditions. Similarly iris recognition benefits from CNNs and attentionbased models trained on datasets such as CASIA-Iris and ND-Iris robust to illumination and pupil dilation. 3.3 Voice and Behavioral Biometrics DL also advances speaker verification via spectrogram-based CNNs and LSTM embeddings. Combining voice with face audiovisual biometrics strengthens robustness for remote banking authentication. Behavioral biometrics keystroke gait touchscreen mouse movement enable continuous MFA. Verma et al. demonstrate smartphone motion-based DL models for adaptive MFA. 3 algorithms. The synergy between biometric recognition and possession-based hardware is foundational to secure MFA. 4.1 Smart Cards in MFA Systems Smart cards have long been used in authentication due to their ability to securely store user credentials and perform local computations. In modern MFA biometric smart cards BSCs integrate fingerprint or facial recognition sensors directly into the card or terminal. There are two main architectures Match-on-Card Mo C Biometric matching is performed entirely on the card s chip and the template never leaves the card. This ensures maximum privacy. Match-off-Card The biometric is matched externally with the template read from the card. This mode is more flexible but less private. Mo C provides superior privacy and is increasingly supported by commercial products such as Idemia s biometric payment cards and Gemalto s biometric EMV solutions with technical standards maintained by EMVCo. 4.4 Smart Card + DL System Architectures Recent architectures combine deep learning with secure hardware to enhance biometric verification DL on-chip Miniaturized CNNs embedded in smart cards or tokens. Secure template fusion Combining multiple traits e.g. fingerprint + iris with fused templates stored in secure elements. On-device adaptation DL models fine-tuned per user during enrollment stored within TEEs. Tani et al. 2025 validated the feasibility of such architectures for real-world banking authentication. 4.5 Challenges in Hardware-Based MFA Despite their advantages hardware-integrated MFA systems face challenges Cost and Scalability Biometric smart cards are more expensive than traditional tokens. Hardware Standardization Fragmentation across vendors complicates integration. Energy Constraints DL inference on low-power chips requires lightweight models and quantization. Ongoing research explores energy-efficient DL models and secure co-processors to address these limitations. The integration of deep learning DL based biometric authentication with traditional MFA systems introduces several design options depending on modality fusion user experience UX requirements and environmental constraints. This section discusses core system-level integration strategies such as fusion techniques adaptive MFA policies and UX considerations like latency. 5.1 Fusion Techniques in DL-Based MFA To leverage the strengths of multiple authentication factors especially in multimodal biometrics systems often use fusion strategies to combine different sources of evidence. Fusion can occur at various levels Sensor-Level Fusion Raw biometric signals e.g. fingerprint + face are captured and preprocessed jointly. Feature-Level Fusion Deep embeddings from CNN/RNN models are concatenated before classification. Score-Level Fusion Independent DL models output match scores that are weighted and combined. Decision-Level Fusion Each modality votes independently a decision is made based on predefined logic e.g. majority voting. Score-level fusion offers a balance between flexibility and performance and is widely used in commercial systems. Decision-level fusion is preferred in scenarios with hardware heterogeneity or legacy compatibility. 7 6.5 Summary DL-based MFA improves identity assurance but still faces challenges in adversarial robustness deepfake resistance privacy and bias mitigation. The future of trustworthy MFA depends on standardized testing open datasets and secure hardware-software co-design. 6.6 Standardization and Interoperability DL-MFA systems often combine heterogeneous sensors inference engines and secure hardware. Without standards integration is fragile and error-prone. Key standards include FIDO2/Web Authn ISO/IEC 30107 PAD and EMVCo for biometric payment cards. Future work must emphasize pluggable frameworks formal verification of workflows and government-led certification e.g. e IDAS NIST 800-63. 6.7 Summary While DL-based MFA significantly strengthens digital identity protection its adoption introduces challenges in robustness privacy fairness and interoperability. Addressing these threats requires cross-disciplinary efforts spanning ML research hardware security regulatory compliance and usability studies. 7 Datasets Benchmarks and Metrics Robust evaluation of DL-based MFA systems requires standardized biometric datasets and consistent performance metrics. This section presents widely used benchmarks for facial fingerprint iris and multimodal biometrics and outlines key evaluation criteria such as FAR FRR and EER. 7.1 Benchmark Datasets for Biometric MFA Research in DL-powered biometric authentication relies on curated datasets that represent different modalities under diverse conditions. The quality and bias of these datasets significantly influence model generalizability. Well-established datasets underpin biometric research. Examples include LFW VGGFace2 Age DB CASIA-Iris V4 and FVC2004 which have become de facto benchmarks for DL-based evaluation. Many of these datasets include variations in lighting pose aging and acquisition devices to simulate real-world conditions. Public availability supports benchmarking and fair comparison across DL architectures. 7.2 Performance Metrics in MFA Systems Accurate evaluation of DL-based MFA systems requires standardized biometric metrics and protocols as defined in ISO/IEC 19795 and NIST SP 800-63B. These standards ensure comparability across algorithms datasets and deployment environments. False Acceptance Rate FAR Probability that an impostor is incorrectly accepted as a genuine user. FAR is critical for measuring system security and is often reported at operating points such as FAR 10 3 or 10 4. False Rejection Rate FRR Probability that a legitimate user is incorrectly rejected. FRR reflects system usability and user experience. Equal Error Rate (EER): The rate at which FAR and FRR are equal, often used as a single scalar indicator of performance. Lower EER implies a more accurate system. Receiver Operating Characteristic (ROC): A curve plotting the trade-off between FAR and True Positive Rate (1–FRR). Widely used to visualize model discriminability. Detection Error Tradeoff (DET): A log-scaled version of the ROC curve emphasizing low-error regions, recommended by ISO/IEC 19795 for biometric evaluations. Failure to Enroll (FTE) / Failure to Acquire (FTA): Rates at which biometric data cannot be captured or enrolled successfully, critical for deployment evaluation. Authentication Latency: The time required to complete an MFA process (biometric + token verification). Recommended latency for practical systems is below 1.5 seconds. In multimodal DL-MFA, researchers also evaluate: Fusion Gain: Improvement in EER or accuracy when combining multiple modalities compared to single-modality baselines. Robustness to Noise and Spoofing: Performance degradation under environmental noise, aging, or adversarial conditions. Template Security Impact: Trade-offs between encryption, privacy-preserving operations, and system accuracy. Public benchmarks such as FVC2004, CASIA-IrisV4, and AgeDB are typically evaluated using these metrics under standard protocols. As digital ecosystems expand and cyber threats evolve, MFA has become a central pillar of secure access control. This survey reviewed how deep learning, biometric modalities, and hardware tokens such as smart cards and secure enclaves can converge to build the next generation of MFA systems. Improvements include accuracy, liveness detection, and multimodal fusion, while challenges include robustness, privacy, fairness, and interoperability.'}, {'rank': 3, 'score': 11.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 4, 'score': 10.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}, {'rank': 5, 'score': 10.0, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation. Contrast-enhanced Computed Tomography CT is important for diagnosis and treatment planning for various medical conditions. Deep learning DL based segmentation models may enable automated medical image analysis for detecting and delineating tumors in CT images thereby reducing clinicians workload. Achieving generalization capabilities in limited data domains such as radiology requires modern DL models to be trained with image augmentation. However naively applying augmentation methods developed for natural images to CT scans often disregards the nature of the CT modality where the intensities measure Hounsfield Units HU and have important physical meaning. This paper challenges the use of such intensity augmentations for CT imaging and shows that they may lead to artifacts and poor generalization. To mitigate this we propose a CT-specific augmentation technique called Random windowing that exploits the available HU distribution of intensities in CT images. Random windowing encourages robustness to contrastenhancement and significantly increases model performance on challenging images with poor contrast or timing. We perform ablations and analysis of our method on multiple datasets and compare to and outperform state-of-the-art alternatives while focusing on the challenge of liver tumor segmentation. Computed Tomography CT is a cornerstone in the diagnosis and treatment planning of various health conditions. In liver applications contrast-enhanced CT imaging enables precise imaging for detection and delineation of tumors facilitating effective intervention strategies. With the rapid advancement of Deep Learning DL the utilization of computer vision CV models has become increasingly prevalent for automating tasks in radiology. With novel techniques and improved accuracy of recent DL based segmentation models the potential for impactful clinical applications emerges. Limited data has been a longstanding challenge in DL and liver tumor applications and techniques such as image augmentation have proven to be indispensable in enhancing the generalization capabilities of A. Contributions We summarize the main contributions of this paper We introduce Random windowing a CT-specific augmentation scheme that encourages robustness and can be targeted to specific regions. We thoroughly analyze and ablate the effects of Random windowing its components and alternatives on contrastenhanced CT images for liver tumor segmentation. Random windowing is compared to state-of-the-art alternatives and is found to yield models with stronger performance on challenging CT images that suffer from poor intravenous contrast or poor contrast timing. B. Outline In Section II we present related work that our methods complement and build upon. Section III introduces Random 2 phase 20 30 s showing liver arteries and the portal venous phase 50 70 s enhancing liver parenchyma by 50 HU. Due to the sensitive timing of contrast-enhancement the variation in ROI appearance and HU of the same phase can be great across patients and scans. DL-based CT applications often rely on image augmentation to learn robustness to these variations. Preprocessed Artifact free Additional context Intensity augmentation standard Windowing B. Augmenting CT images Artifact free Additional context Data augmentation involves applying various transformations to existing training data to create slightly altered instances of the data which enrich the dataset to enhance the model s robustness and generalization. For medical images two main types of augmentations are especially relevant geometric augmentations and intensity augmentations. Geometric augmentations preserve the pixel intensities by only altering the spatial appearance using geometric transformations like rotation flipping translation resizing and cropping. Intensity augmentations transform the pixel values of the image without changing the spatial aspects of the image. Certain augmentations such as saturation and hue transformation operate in the RGB space of natural images and require three color channels making them unsuitable for CT images which have HU in only one channel grayscale. Intensity augmentations like contrast brightness and gamma corrections however can be applied to CT intensity values to change the visual appearance of the image. Geometric augmentations are commonly used in DL applications for CT images as well as in liver and tumor applications. Applying geometric augmentations like flip rotation translation crop and resize for CT can accommodate for lack in variation of orientation shape and sizes of tumors and other anatomical structures. Patch-based training inherently provides translation variability by exposing the model to structures at different spatial positions while also enabling computational memory benefits. Intensity augmentations for DL in CT applications are not always required for good performance as many wellperforming methods manage fine without them. However many top-performing methods leverage some forms of intensity augmentations to increase variability in limited data domains. The most popular intensity augmentations are intensity shifting and scaling methods closely connected to contrast and brightness augmentations for natural images. Random windowing proposed Raw CT inputs Intensity based HU based Fig. 1 Standard intensity augmentation of CT images often operates on the clipped intensities of the image. This limits the augmentation potential and available context and may create artifacts in the image like unnatural values for background bone or air pockets. We propose Random window augmentations for CT that operate on the raw HU using the viewing window which resolves the aforementioned challenges. windowing with its effects analyzed in Section IV. Results and ablations that validate our method are presented in Section V followed by discussion and a future outlook in Section VI. A. Preprocessing of CT images In a CT image the measured volumetric linear attenuation μ of scattered X-rays are calibrated against the attenuation of water μwater and air μair resulting in intensity units measured in Hounsfield units HU given by HU 1000 μ μwater μair μwater. 1 Before CT images are visualized they are often preprocessed to a viewing window by clipping the intensities to a given range resulting in increased contrast of the region of interest ROI. Although DL models can take unprocessed HU as inputs they often benefit from clipping the intensity values to a narrower range. The benefit comes from increased relative HU differences within the ROI at the cost of removing certain intensities assumed to be irrelevant. For CT in general and liver tumor segmentation specifically there is much variation in the chosen clipping range which may suggest that a suboptimal window is common. The clipping boundaries in DL applications are often determined from radiology domain knowledge computed from intensity statistics of the dataset or determined dynamically during training. In our experiments we show that choosing a narrow task-specific clipping range is beneficial for segmentation performance. In contrast-enhanced CT contrast injected into an upper extremity vein highlights abdominal tissues with the arterial C. Questionable augmentation practices Shifting and scaling raw CT intensity values is not problematic in a DL setting but could simulate variations in measurements that could naturally occur across scans protocols and patients. We argue that the problem arises when such intensity augmentations are applied to clipped intensity values. When HU are clipped to a viewing window relevant for the application the information outside the viewing window is removed and is not possible to recover. Subsequent scaling and shifting during brightness and contrast transformations will risk introducing artifacts in the form of empty values near the Int. scale Gamma Inv. gamma Int. shift W. shift ours W. scale ours Fig. 2 On certain contrast-enhanced CT images standard preprocessing removes important information about liver and tumor intensities. Standard image transformation applied to such preprocessed images fails to reintroduce useful variation into the image. Our proposed windowing augmentations are applied before any preprocessing and have the potential to yield better visualizations of such difficult images. edges of the interval instead of simulating natural variation Figure 2. While we acknowledge that many CT applications might already apply intensity augmentations with care we consider the importance of this to be understated. The nn U-Net augmentation pipeline leverages a combination of brightness contrast and gamma augmentation from Batchgenerators and has been reused in multiple CT applications. The Unetr and Swin-Unetr apply intensity shifting and scaling from the MONAI framework. These top-performing segmentation frameworks all apply intensity augmentation after HU clipping which we find concerning. Although these augmentations seemingly increase performance we hypothesize that augmentation strategies that are tailored towards CT and treat the HU distribution of CT with care are more advantageous. also been explored in segmentation and self-supervised learning. While these methods avoid artifacts they do not provide the continuous properties comparable to traditional augmentation techniques. They also do not address the issue of patient contrast or timing variations introduced by the contrastenhancement in diagnostic CT scans. We propose to continuously vary the viewing window used for preprocessing by sampling the window width and level randomly. The augmentation strength can be tailored for the relevant task by controlling the allowed range of viewing windows. Our method entitled Random windowing creates training images that can simulate difficult cases and make difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and inference in liver lesion segmentation and found that window selection is important for segmentation performance. Tangential to augmentation works exploiting multiple inputs with images of different viewing windows during training have III. In this section we introduce our new CT augmentation technique Random windowing as well as the core components of the technique. Specifically the windowing operation used for preprocessing Window shifting and Window scaling. These operations together make up our CT augmentation method Random windowing. A. Windowing operation Windowing is a preprocessing scheme for CT images and is an essential step performed by radiologists upon CT inspection and in CT DL applications. It removes irrelevant information by limiting the range of HU to display. the values to a minimum and maximum value. The viewing window is defined by the window width W and the window level L. The width W determines how much of the HU range to include and the level L is the center of the range. For each application or task a base viewing window comprising a base width Wbase and base level Lbase is typically selected to optimize visualization. The included HU intensities x are then given by The included HU intensities x in the preprocessed image are given by effect. Specifically the CT images are clipped with a randomly sampled width W from a uniform distribution W Uniform Wmin Wmax 4 where Wmin and Wmax are the minimum and maximum widths for the augmentation strength. We sample W from a range around the base width. Hence Wmin Wbase Wmax. This allows the Window scaling to yield continuous variations around the base width. This makes it natural to use the base window during inference. The resulting augmentation effect is in some settings similar to standard intensity scaling and contrast enhancement. However as the augmentation happens before clipping similar to Window shifting the output is not limited by the initial preprocessing setting which may cause artifacts. x L W 2 L + W 2. 2 After windowing the range of intensity values to display is smaller and thus fewer values are mapped to each grayscale level in the display. The contrast of the image is therefore increased so details are more prominent to both radiologists and DL models Figure 1 Windowing. For liver tumor segmentation we find in Section V-D that a narrow tumorspecific window is beneficial for performance. D. Random windowing Window shifting and Window scaling both work on independent parameters of the viewing window allowing them to be combined without overhead. We refer to the combined transformation of Window shifting and scaling as Random windowing due to the randomness introduced in the selection of both window level and width. The computational cost is negligible as it is performed in place of standard windowing. Following common augmentation practices we sample L and W independently with probability p L and p W from uniform distributions but acknowledge the potential for more data driven approaches. Our preliminary exploration in this direction did not lead to significant improvements but we encourage further investigation in future work. We present the combined preprocessing and augmentation technique of Random windowing using both Window shifting and Window scaling in Algorithm 12. B. Window shifting When a narrow viewing window is selected the CT images are more affected by varying contrast-enhancement from timing of the IV contrast and the patient s response to it. To mitigate this problem Window shifting1 adjusts which parts of the image distribution are visualized during training and thus introduces useful variation into the training of DL models. Window shifting stochastically adjusts the window level L during preprocessing of training images resulting in an augmentation effect after clipping. This is achieved by sampling a new window level L from a uniform distribution defined by Lmin and Lmax Algorithm 1 Random windowing algorithm x ct image In Hounsfield units W base width L base level if uniform 0 1 p W then L Uniform Lmin Lmax. 3 The boundaries of Window shifting Lmin and Lmax can be set as hyperparameters or be determined from the distribution of foreground intensities in the CT dataset tailored to the task at hand. W uniform W min W max Window scaling end if if uniform 0 1 p L then L uniform L min L max Window shifting end if lower L W/2 upper L + W/2 x clip x lower upper Windowing x x lower /W Normalize to zero-one C. Window scaling Window shifting exploits the variation of HU shifts from contrast-enhancement in the dataset to augment the images. However it does not account for uneven distribution of contrast agent within a foreground region which may result in a tight or wide spread of HU for an image. To account for this and exploit the effect during training we introduce Window scaling. Window scaling scales the window width before clipping to vary how much of the image distribution is included during training resulting in an augmentation IV. ANALYSIS OF RANDOM WINDOWING The following sections explore how Random windowing improves and intentionally distorts images avoids augmentation artifacts and creates realistic yet challenging training samples. We also examine its impact on HU measurements and intensity distributions highlighting its role in enhancing model performance and generalization. 1Window shifting was first introduced in the conference version of this paper. In this work we extend the original study by introducing Window scaling and Random windowing and by substantially expanding the analysis with additional experiments ablations metrics and datasets. 2Code at https //github.com/agnalt/random-windowing. 5 A. Image correction get strong clues from specific values. In the following paragraphs we analyze the effect of Random windowing on the HU measurements and distribution of a CT scan. 1 Adjusted Hounsfield units For the CT modality a unified global preprocessing scheme is beneficial during training to preserve information in the HU pixel measurements. However during augmentation the HU are deliberately distorted to simulate useful variation and prevent overfitting. Standard intensity augmentations do this by default on the input while Random windowing obtains a similar effect through min-max normalization after clipping. Doing this resets the intensities to the zero-one range ensuring that the HU are stochastically adjusted by the randomly sampled window width and level. In Section V-C we verify that this step is key when working with tumor segmentation in contrast-enhanced CT images. However skipping this step will allow Random windowing to preserve the absolute HU measurement in the scan while augmenting the image through added or removed context of the pixel distribution. In applications for CT without IV contrast this might be beneficial as the original HU is intact. 2 Additional context and characteristic distribution Regardless of whether HU are preserved or not Random windowing can stochastically provide additional context compared to the clipped image view. Intensity augmentations are shown to be effective for certain DL applications as they prevent models from picking up on the characteristic distribution of the inputs. When linear augmentation transformations like intensity shifting or scaling are applied to the clipped intensity distribution the absolute intensities are altered but the relative shape of the distribution remains largely unchanged Figure 4. Although Random windowing is parameterized by linear transformations in HU space its effect on the final distribution can be non-linear. This is because the transformation of the window may expand the distribution by incorporating additional HU values thereby reshaping the distribution rather than simply shifting or scaling it. This effect is further investigated in Section V-C. In the special case where Window scaling is performed with W Uniform Wmin Wbase no additional context is included and its effect is comparable to contrast augmentation with a scaling factor α 1 Wbase Although CT scans are obtained with similar protocols variations due to contrast-enhancement are expected. In Figure 3a Windowed and Normal ref. display how the same clipping setting can result in different liver brightness in CT images due to contrast-enhancement. As Random windowing introduces variation to the CT clipping during training it enables scans to be visualized in multiple ways which can result in better visualizations. Intensity augmentations that transform clipped HU distributions will struggle to create the same variation. In Figure 3a we aim to remedy the poorly timed contrastenhancement using standard intensity augmentations and Random windowing. Standard augmentations cannot correct the loss of detail in the image while the Random windowing settings yield a much better result. Additionally standard intensity augmentations transform all values equally and the background and bone structures like the spine outside the soft tissue range are artificially darkened/brightened and can be considered artifacts in the final image. B. Image distortion An important task of data augmentation is to expose the model to images that resemble challenging training cases so it can learn to generalize to difficult cases. Similar to how Random windowing can yield better visualizations of challenging images Section IV-A it can make normal training images look like the challenging ones without introducing artifacts. In Figure 3b a CT slice where the liver has a normal response to contrast-enhancement is augmented to produce a training sample that resembles dark and bright training cases from the dataset. Standard intensity augmentations may fail to make realistic augmented images as they are prone to introducing artifacts in the background and bone structures. C. Avoiding artifacts Artifacts from intensity augmentations in CT images occur when the pixel distribution is transformed after clipping. Particularly prone to causing such artifacts are intensity augmentations such as contrast augmentation intensity scaling i.e. brightness and intensity shifting i.e. additive brightness. Artifacts occur when the edges of the intensity distribution are transformed such that they end up inside the original interval of x Equation 2. In other words the transformation t moves xmin or xmax so Wmin followed by clipping to the original range. V. In this section we empirically validate the effects of Random windowing in controlled experiments against traditional intensity-based augmentations from established baselines. Subsequently we scrutinize the mechanisms at play in window augmentations and analyze the effect of base windows augmentation components and strengths. t xmin xmin or t xmax xmax. 5 As Random windowing performs augmentation through the window operation itself it solves the problem of artifacts in Equation 5. A. Stronger intensity augmentation pipeline D. Effect on HU measurements and intensity distribution We compare the proposed Random windowing augmentation against the intensity augmentation pipelines of two strong baselines namely the nn U-Net and the Unetr. The intensity augmentations of the nn U-Net consist of contrast multiplicative brightness gamma and inverse Until this point the effect of Random windowing is mainly considered from an image perspective where the pixel intensities are visualized as viewed by an observer. However DL models process pixel values of the input and can in principle Int. corrected RW corrected Normal ref. Windowed Int. augmented RW augmented Hard ref. Darken Brighten Dark Bright a Improving visualization of difficult scans. b Simulating scans with non-standard contrast-enhancement. Fig. 3 Comparison of Random windowing and intensity augmentations. Random windowing samples beyond default window boundaries improving visualizations during training and recovering information lost with standard augmentations. It also produces realistic challenging samples without the artifacts introduced by standard intensity transformations. Raw image Windowed Intensity shifting Intensity scaling Window shifting ours Window scaling ours liver tumor other Fig. 4 Augmentation effect on intensity distribution. Augmentation through intensity shifting and scaling affects the appearance of the image but not the distribution shape. Shifting and scaling the viewing window can include more data near the edges of the base viewing window so the shape of the distribution changes more. gamma augmentations applied in sequence on clipped and centered intensities. The Unetr applies intensity shifting and scaling of the clipped and zero-one-normalized intensities. We apply Random windowing with Window shifting and scaling independently on the raw CT intensities. In subsequent experiments we standardize augmentation probabilities and strengths but resort to recommended settings for each baseline here. Details in Appendix A. Each augmentation pipeline is used for training identical 3D-U-net segmentation models to perform liver tumor segmentation with 4-fold cross-validation on the Liver tumor segmentation Li TS dataset. For robust evaluation we consider the entire Hepatic Vessel HV dataset 303 cases Colorectal Liver Metastases CRLM dataset 197 cases and HCC-TACE dataset 104 cases as disjoint test sets for liver tumor segmentation. With regards to tumor characteristics HV and CRLM are more similar to the Li TS traning set than HCC-TACE. HCC-TACE comprises only patients with Hepatocellular carcinoma HCC where tumors show heterogeneous appearance due to variable tumor attenuation and portal venous washout. Due to the limited support in Li TS for HCC HCC-TACE is especially difficult and in some degree out of domain. For each prediction we report the Dice similarity coefficient DSC measured with the original tumor mask and report the mean performance in Table I with the top performing method highlighted in bold. We measure the significance of the results with the Wilcoxon signed rank test at p 0.05. The results show that Random windowing leads to a statistically significant higher performance across all datasets. B. Generalization to difficult tumor cases For an extended analysis of the augmentation pipeline results we also measure the performance on what are considered difficult cases. The difficult cases are identified by as images with low contrast between tumor and liver regions with mean tissue difference 20 HU HU contrast in total 171 42 and 68 cases for HV CRLM and HCC-TACE respectively. Additionally we identify that scans where the contrast-enhancement is poorly timed are difficult. Poor IV contrast timing can be identified by particularly high or low HU in the liver. By visual inspection we consider the top and bottom 10 % of scans with the highest and lowest median liver HU to be difficult corresponding to HU 89 and HU 137 respectively CE timing in total 64 39 and 16 cases for HV CRLM and HCC-TACE respectively. In Table I we report the mean DSC on these cases specifically and find that models trained with Random windowing perform significantly better also on these subsets p 0.05. To highlight the benefit of augmentation we plot the relative improvement of DSC compared to not applying any intensity augmentations for the HV and CRLM datasets in 7 TABLE I The mean DSC of the HV CRLM and HCC-TACE test sets. Random windowing significantly outperforms the intensity augmentation pipelines of the nn U-Net and Unetr. These results are consistent across whole datasets as well as the difficult cases with low liver-tumor HU contrast and poor CE timing. denotes significance at p 0.05. Hepatic Vessel CRLM HCC-TACE Intensity augmentation All HU contrast CE timing All HU contrast CE timing All HU contrast CE timing None 0.507 0.019 0.419 0.027 0.365 0.033 0.600 0.006 0.449 0.008 0.501 0.006 0.305 0.027 0.255 0.023 0.144 0.043 Unetr baseline 0.527 0.009 0.451 0.010 0.395 0.024 0.588 0.021 0.438 0.006 0.496 0.031 0.329 0.059 0.280 0.060 0.196 0.086 nn U-Net baseline 0.544 0.026 0.476 0.039 0.431 0.028 0.606 0.007 0.448 0.014 0.528 0.014 0.373 0.070 0.313 0.086 0.303 0.071 Random windowing ours 0.566 0.015 0.499 0.017 0.450 0.035 0.617 0.003 0.471 0.005 0.546 0.023 0.393 0.049 0.338 0.054 0.333 0.046 TABLE II Ablation of augmentation mechanisms in Random windowing. The experiment displays the additional benefit of adjusting Hounsfield units Adj. HU and providing additional data context Add. cont. during training augmentations. All other variables are unchanged. indicates that the result is significantly larger than the next best alternative at p 0.05. Effect of augmentation in tumor segmentation DSC % 20 0 Adj. Add. AugInstance-metrics HU cont. mented Tumor DSC F1 Recall Precision CRLM × × × 0.507 0.019 0.592 0.019 0.735 0.032 0.624 0.011 RW shift-scale × 0.527 0.008 0.582 0.018 0.756 0.011 0.586 0.029 Int. shift-scale × 0.542 0.024 0.576 0.025 0.778 0.024 0.559 0.031 Random window 0.565 0.017 0.604 0.018 0.785 0.019 0.597 0.034 DSC % 0 Adj. HU × Adj. HU Add. context × Normal Poor contrast Poor timing Window Int. ss. Fig. 5 Relative DSC improvement by augmentation schemes measured for scans with normal contrast-enhancement poor liver-tumor contrast and poor contrast timing. The improvement is over not applying any intensity augmentations measured on the Hepatic Vessel and CRLM dataset. 0 100 200 0.5 1.0 RW. ss. RW Add. context Random windowing gives a larger improvement across all settings and is especially beneficial for difficult tumor cases where the HU contrast is low or the timing is off. For HCCTACE we observe that augmentation and Random windowing are key due to the very limited support for HCC in the training set. Interestingly Random windowing also benefits the normal cases across all datasets more than the baseline alternatives. We hypothesize that this is due to its potential to use difficult cases to simulate normal cases as described in Section IV-A. 100 0 100 0.0 0.5 1.0 Fig. 6 Illustration of the experiment settings used in the ablation of Table II. In each row the overall shape of the distribution and the included HU values are the same. In each column the HU are either preserved or not scaled to. C. Augmentation through context and HU adjustment Figure 6 illustrates the effects we are ablating with the distribution of one example scan. The initial row shows the distribution before and after augmentation when windowing is performed during preprocessing. In the second row we augment the image while allowing additional context. For all settings transformations are applied with p 0.5 and equal strengths on the z-score normalized to mean of 0 and standard deviation of 1 using the global dataset statistics. On the external test set we measure the tumor DSC and the instance-wise lesion F1 recall and precision after a connected component analysis where 10% pixel overlap counts as a detected lesion. We present the results in Table II. We observe that adjusting the HU has a larger impact than additional context while both contribute constructively in Random windowing. We hypothesize that HU perturbations are important to guide the models away from HU reliance alone Compared to augmentation on clipped intensities window augmentations can produce training samples with additional context from the raw data. By context we specifically refer to the parts of the CT intensity distribution that are near and outside the edges of the interval of the base window. Although Random windowing does not preserve absolute HU by default we hypothesize that context variation alone opens a new opportunity to augment CT intensities while preserving the HU of the image. We refer to this setting as Random windowing shift-scale RW ss. and is to the best of our knowledge also novel and unexplored in CT augmentation. To investigate this further we ablate the effect of augmentation through additional context as well as HU adjustments in Random windowing. HU adjustments are achieved through normalization e.g. to of the clipped and transformed intensities and is common in standard intensity augmentations. TABLE III Ablation study on the Li TS dataset reporting 2D validation tumor DSC 3 × repeated 4-fold CV. We observe that narrow region-specific viewing windows improve tumor segmentation and Window shifting further enhances performance especially with focused windows. 0.60 Tumor DSC 0.55 Viewing window Width Level Baseline Window shifting None raw 2000 0 0.552 0.081 0.580 0.099 Generic abdomen 500 150 0.628 0.078 0.636 0.080 Liver window 196 91 0.629 0.091 0.637 0.079 Tumor window 169 65 0.634 0.081 0.648 0.084 W. shift W. scale 0.50 0 20 40 60 80 as it increases tumor sensitivity. Meanwhile augmentation in general decreases tumor precision due to more false positives. These results shed light on the mechanisms at play in Random windowing while proving that the HU-preserving version of Random windowing is beneficial alone and perhaps the only option in certain settings. We leave further exploration in this direction to future work. Fig. 7 Window shifting and scaling improve tumor DSC at various strengths with peaks at L 60 and W 80 HU. L range W range 100 Level HU D. Importance of base viewing window ences between liver tumors and surrounding parenchyma but at the cost of reduced distribution context. The liver-tumor HU differences are emphasized by the HU shift of contrastenhancement which is exploited by Window shifting. We hypothesize that using a region-specific narrow base window improves tumor segmentation by emphasizing the relevant HU differences. Furthermore we expect Window shifting to benefit most when used with such focused windows. To test this we measure the impact of tumor and liver windows covering 99 % of foregrounds as well as a window of raw HU and one characteristic of the general abdomen. We measure the impact of each window and its interaction with Window shifting in all settings. We report the window settings and tumor segmentation DSC in table Table III and observe that both the baseline static windowing and Window shifting increase performance with narrower more region-specific base windows. The performance gain is greatest when going from raw HU to a more focused window even if only a generic soft tissue window. From Table III we observe that regardless of the base viewing window Window shifting augmentation is advantageous. The results suggest that a sufficiently narrow window benefits Window shifting and that the generic liver and tumor windows all are significantly better for Window shifting than the raw window with p 0.05 using Wilcoxon s signed rank test between folds. 0 100 150 200 250 Width HU Fig. 8 Per-case estimate of viewing windows covering 99 % of tumor HU in the Li TS train set and base window. L W range show best shift/scale ranges from 9 and gamma adjustment of inverse intensity values gamma inverse. These augmentation methods are compared against the individual components of Random windowing augmentation namely Window shifting and Window scaling. All individual intensity augmentations are applied with the same probability. The mean liver tumor DSC and standard deviations of 3 times repeated 4-fold cross validation are reported in Table IV. The results show that the individual components of our method are indeed potent and surpass their intensity-based counterparts. Interestingly applying no intensity augmentations geometric only outperforms individual intensity-based CT augmentations in certain settings suggesting that some intensity augmentations may hurt performance. architectures and metrics. We attribute its generalization capabilities to the additional contextual information preserved from raw CT data combined with HU adjustments that simulate natural variations in contrast-enhancement allowing our method to utilize limited data efficiently. Overall Random windowing emerges as a powerful augmentation strategy for CT images offering significant gains in segmentation performance under difficult imaging conditions. Future work could explore its extension to new applications organs and modalities as well as its potential role in improving model robustness in clinical scenarios.'}]",This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features.
How does model selection affect responsiveness in real-time applications?,2510.13137v1,2510.13137v1,True,"['2510.13137v1', '2509.20913v1', '2510.13050v1', '2510.13937v1', '2510.05163v1']","[6.0, 6.0, 5.0, 5.0, 5.0]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches']","[{'rank': 1, 'score': 6.0, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently while also highlighting potential future directions such as hybrid models to enhance real-time performance. In their study X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features followed by an attention-LSTM to dynamically weight salient frames achieving state-of-the-art accuracy on benchmark datasets e.g. WLASL. However the authors highlight the increased computational cost of attention mechanisms necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language ISL recognition systematically comparing architectures like CNNs LSTMs and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight region-specific models a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages data acquisition preprocessing model training and performance evaluation. We have developed a LSTM Long Short-Term Memory Model and compared it with 3D CNN Model architecture. Data Acquisition and Preprocessing F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. This work utilizes publicly available datasets including the Indian Sign Language ISL dataset and an American Sign Language ASL dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps e.g. 36 for alphabets A Z and digits 0 9. This produces a probability distribution over all gesture classes At the end of the architecture we have the Output layer which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize for example 36 units to cover the alphabet A-Z and numbers 0-9. The softmax layer processes the Dense layer outputs and generates a probability distribution giving a confidence score for each possible gesture. This means that for any given hand movement the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match making it easier to trust and interpret its predictions. Advantages of LSTM Model Temporal Awareness Unlike traditional CNNs which only analyze spatial features LSTM models inherently understand the temporal evolution of gestures. Handles Variable-Length Inputs LSTM can process sequences of varying lengths making it robust to different gesture speeds and durations. Real-Time Capability The model s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU. Noise Tolerance Since the input is based on 3D hand landmarks rather than raw pixel data the model is less sensitive to background noise and lighting variations improving robustness in diverse environments Scalability The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network 3D CNN Model While LSTMs excel at learning temporal dependencies in sequential data Convolutional Neural Networks CNNs particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions height width and the temporal dimension time making it especially suitable for video classification tasks where both motion and appearance are important. Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets. Media Pipe was used to extract 3D hand landmarks 21 points per hand each with x y z coordinates resulting in 63 features per frame for single-hand tracking. These features were normalized and reshaped to prepare them for time-series or spatial analysis depending on the model. LSTM-Based Sign Language Recognition Model The Long Short-Term Memory LSTM network a type of Recurrent Neural Network RNN is particularly well-suited for sequence prediction problems especially when there are long-term dependencies across time steps. In the context of sign language recognition gestures are basically sequential-coming in sequences a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture The model is made of LSTM Long Short-Term Memory layers capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team the lower layers zero in on subtle details like slight changes in finger positioning or wrist angles while the higher layers build on this to understand the broader movement signature that defines a specific gesture such as the fluid motion of signing a letter or number. To ensure the model doesn t just memorize the training examples and can adapt to new unseen gestures we include a Dropout layer right after the LSTMs. During training this layer randomly deactivates a portion of neurons forcing the model to learn more flexible patterns. It s like training the network to stay sharp even when some of its tools are temporarily unavailable which helps it generalize better and perform reliably on fresh data. After the LSTM layers fully connected Dense layers are employed to transform the temporal features into high-dimensional representations for classification. The final output layer applies a softmax activation function with C units where C corresponds to the number of gesture classes In this research we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout what s happening in each frame and the temporal flow how things change over time. For instance a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. 3D CNN Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training. LSTM Limited relies only on coordinate data i.e. no texture color or visual details 3D CNN High spatial awareness due to access to pixel-level visual features in the input video. LSTM Excels in recognizing dynamic gestures involving motion over time. 3D CNN Performs better for static or shape-dominant gestures due to its strong spatial feature extraction. At the end of the network fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes indicating the model s prediction and its associated confidence level. Overall this architecture is designed to fully capture both the visual details and the motion dynamics of sign language all while staying efficient enough for practical use. LSTM Lightweight requires less memory and computational power. Suitable for real-time and edge applications. 3D CNN Computationally heavy needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs Spatiotemporal Feature Learning Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points. No Feature Engineering Required Unlike LSTM models which require pre-extracted landmarks using Media Pipe etc. 3D CNNs learn directly from video data. High Expressiveness Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real-time sign-to-text translator application. In this research we compare it with a 3D CNN architecture to evaluate its trade-offs in terms of accuracy speed and usability in real-world scenarios. LSTM vs 3D-CNN A comparison LSTM Can generalize well on smaller datasets due to fewer trainable parameters. 3D CNN Requires large amounts of labeled video data to avoid overfitting and learn robust features. LSTM Requires hand detection and landmark extraction here via Media Pipe but reduces input dimensionality significantly. 3D CNN Requires raw video clips often with cropping resizing normalization and augmentation. LSTM Easier to interpret as it works on landmarks errors can be traced to motion or key point misalignment. 3D CNN More complex to interpret difficult to pinpoint which pixel regions influence predictions. Comparison LSTM Uses pre-extracted hand landmarks x y z coordinates of 21 key points per frame. These are fed as sequences e.g. 30 frames × 63 features. 3D CNN Takes raw video frames as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a significantly higher computational cost and latency which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time LSTMs offer a better balance between performance efficiency and practicality for deployment in real-world assistive technologies. The system s GUI further enhances user interaction by displaying detected signs maintaining a dynamic sentence output and providing a reference module for individual ASL letters. Overall this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models attention mechanisms and multilingual sign language support.'}, {'rank': 2, 'score': 6.0, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}, {'rank': 3, 'score': 5.0, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting. Precipitation nowcasting which predicts rainfall up to a few hours ahead is a critical tool for vulnerable communities in the Global South that are frequently exposed to intense rapidly developing storms. For these regions timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction NWP methods often suffer from high latencies low spatial and temporal resolutions and significant gaps in accuracy across the world. Recent progress in machine learning-based nowcasting methods commonly used in the Global North cannot be extended to the Global South due to extremely sparse radar coverage. Here we present Global Met Net an operationally ready global machine learning nowcasting model. It primarily leverages the Global Precipitation Mission s GPM CORRA dataset and geostationary satellite data along with global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05 5km spatially and 15 minutes temporally. Global Met Net significantly outperforms industry-standard hourly forecasts and achieves a significantly higher skill making the forecasts useful in a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially our model operates under real-time conditions and generates forecasts in under a minute making it readily deployable for diverse applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse high-resolution satellite observations into weather forecasting. Nowcasting the ability to forecast detailed local weather conditions from the present up to a few hours ahead is crucial for a wide array of applications. From individuals planning their daily activities to farmers deciding whether to apply fertilizer to meteorologists issuing timely warnings for severe weather events accurate and timely nowcasts are essential. Inaccurate precipitation forecasts can hinder disaster preparedness and response efforts potentially leading to greater loss of life and property. In fact the WMO estimates that over the past 50 years 22% of deaths and 57% of economic losses caused by natural disasters were the result of extreme precipitation events. However nowcasting particularly precipitation nowcasting presents significant challenges especially in tropical regions. In general weather forecasting systems benefit greatly from availability of raw observations. Doppler weather radars serve as the foundational instrumentation for the monitoring and forecasting of precipitation. Their operational availability typically determines the precision and spatial resolution Corresponding author s shreyaa google.com 2025 Google. All rights reserved An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting of meteorological forecasts within any given region. However coverage of ground-based weather radars is highly uneven across the globe. While dense radar networks exist over North America Europe and parts of East Asia there is a severe lack of radar coverage in developing regions oceans and largely uninhabited areas. This further exacerbates the gaps in accuracy of precipitation forecasts between the Global North and the Global South see Figure 3. Traditional Numerical Weather Prediction NWP methods play a significant albeit evolving role in precipitation nowcasting. They serve as a cornerstone for understanding atmospheric dynamics and provide valuable context for shorter-term predictions. However they also have limitations when applied to the rapid timescales of nowcasting. Running NWP models can be computationally expensive and time consuming limiting their ability to produce frequent low-latency updates needed for effective nowcasting Sun et al. 2014. For example the High-Resolution Rapid Refresh HRRR model produced by National Oceanic and Atmospheric Administration NOAA first collects and processes large amounts of observational data that feeds into their data assimilation system which runs on high-performance computing systems. The initial conditions are then fed to the forecasting system also running on supercomputers to produce the forecasts. This entire process takes about an hour and is limited to the CONUS region. Besides being more actionable in the near future sub-hourly nowcasts are needed to capture the fine-scale details of convective precipitation which can develop and dissipate in under 30 minutes. AI models promise lower latency which could support forecasters in capturing these events in a way that is both accurate and timely. While NWP methods have improved in spatial and temporal resolutions over the past few years achieving a global forecast at a 0.05 × 0.05 spatial resolution and 15-minute temporal resolution with sub-hourly latency remains a significant challenge for current global NWP systems. The high-resolution forecast HRES from the European Centre for Medium-Range Weather Forecasts ECMWF while providing global coverage at a 9km resolution is a medium-range model with a latency of several hours making it unsuitable for the immediate sub-hourly updates required for nowcasting. Similarly HRRR is a 3km spatial resolution model but available within the US only. Additionally NWPs continue to suffer from the problem of unequal skill in different parts of the world. The application of machine learning to medium-range weather forecasting has seen significant progress with models like Graph Cast Lam et al. 2023 Gen Cast Price et al. 2025 Neural GCM Kochkov et al. 2024 Pangu-Weather Bi et al. 2023 and Fuxi Chen et al. 2023 for medium-range forecasting demonstrating promising results. This growing body of work however has not addressed the issue of accuracy gaps in different regions globally. Furthermore the spatial and temporal resolutions of these models remain similar to their NWP counterparts as these AI-based systems are built for an entirely different purpose than nowcasting. Radar-based nowcasting methods using machine learning are able to overcome limitations of the traditional methods and showing considerable improvements in accuracy Espeholt et al. 2022 Piran et al. 2024 Ravuri et al. 2021. Although extremely effective in radar-rich parts of the world they are inapplicable to most of the rest of the world due to radar-sparsity. Satellite-based methods offer a potential solution and some work has been done towards this leveraging techniques such as optical flow are beginning to be adopted in data sparse regions but have known limitations World Meteorological Organization 2023. Rain AI Pablos-Sarabia et al. 2023 offers a method using EUMETSAT data as input and training against the OPERA network however it is unclear whether that approach generalizes to regions without radar. Lebedev et al. 2019 propose a similar satellite-based approach training against the radars in Russia but mention the problem of overfitting to regions with radar and potentially risking coverage of other areas. This work presents a precipitation nowcasting model Global Met Net that is globally available but specifically designed to be highly performant in data sparse regions of the world. It bridges the accuracy gaps we see in the current state-of-the-art nowcasting models in most of the world where populations live see Figure 1. Extending our prior work on Met Net for regional nowcasting Figure 1 Critical Success Index CSI at a 1 resolution for the HRES and Global Met Net model at 1 hour lead time for 1.0 mm/hr of precipitation. Espeholt et al. 2022 this is a satellite observations based machine learning model with high spatial and temporal resolution that incorporates elements to make it easily operational. Since ground radar is not available globally our model leverages a global mesh of geostationary satellites as input and to the best of our knowledge is the first system to use the Global Precipitation Mission s Combined Radar-Radiometer Precipitation algorithm dataset as a training target. The CORRA dataset combines data from a space-based dual-frequency precipitation radar with a microwave radiometer to create highly accurate estimates of rainfall. It provides near-global coverage and serves as a unique proxy for ground truth. By leveraging this combination of observational data sources our model provides nowcasts at a 15-minute resolution for the next 12 hours. We evaluate our model against ground weather radar where available calibrated and quality controlled rain gauges and the CORRA dataset where none of the other ground observations are available. Our model outperforms industry-standard hourly forecasts globally demonstrating its effectiveness in both data-rich and data-sparse regions. We also show that an optimized HRES forecast post-processed using our own ML model is a stronger baseline than the raw HRES forecast itself. Our work is especially critical in the tropics where the lack of ground radar and other weather infrastructure limits the accuracy of the best-known current nowcasting methods. forecasts HRRR in the US and HRES globally. All results have been computed using the Weather Bench-X framework. We compute metrics over various regions of the world because the varying climatologies can significantly impact the numbers. We also show results for varying rates of precipitation from the category of light rain to heavy precipitation. The results highlight substantial enhancements in predicting precipitation events across various lead times and geographical areas. It is important to note that the results here take operational latencies into account. For example while HRES produces a nowcast for a 1-hour lead time due to the operational latency the forecast only becomes available after its valid time has already passed. Hence in the best-case scenario only the 7 hour lead time forecast of HRES is available as a 1 hour nowcast from any given initialization point see Figure 14 in the supplement to help demonstrate. The Global Met Net model architecture has been designed to be flexible in the set of training datasets and we show results here for three different versions of our model with the only difference being the input datasets for training. These model variations share the same model architecture but are trained independently allowing each one to optimize model parameters based on their respective inputs. The first model called Global Met Net Nowcasting contains geostationary datasets and HRES NWP analysis and forecasts only as input. To contrast this we train a second model that includes high quality ground radar observations called Global Met Net Nowcasting with radar input. Both of these models are trained with the following targets as separate output heads the GPM CORRA dataset ground radars from the US Europe and Japan and the GPM IMERG dataset more in Table 1 later. A baseline model called Global Met Net Post-processed HRES is trained such that it takes only NWP data as input and trained to optimize the GPM CORRA dataset as target only. This baseline model helps calibrate HRES against GPM CORRA dataset and makes for a much stronger baseline than the deterministic forecasts from HRES. The primary goal of this baseline model is to show the importance of additional inputs other than NWP along with the strength of our model architecture. We evaluate our forecasts against quality controlled ground radar datasets which are considered the gold standard for precipitation measurements and the GPM CORRA dataset to provide uniform global coverage. For all the following results our test dataset spans one full year from June 2023 to May 2024. As a spaceborne satellite the GPM CORRA dataset is not considered as high quality as ground radar Speirs et al. 2017 primarily because the GPM radar cannot see the precipitation all the way to the surface and that it does not provide consistent global snapshots with a revisit rate of 2.5 days however it makes for a uniform dataset to evaluate against globally providing consistent coverage even over oceans complex terrains or where radar is unavailable. Note here that this dataset only captures sparse measurements and therefore a large enough validation dataset is required to be able to get less noisy evaluation against all possible precipitation rates. Figure 2 Critical Success Index CSI globally and for several regions Brazil India Africa and the USA using the GPM CORRA dataset as ground truth at precipitation rates of 0.2 mm/hr drizzle 2.4 mm/hr light rain 7.0 mm/hr heavy and 25.0 mm/hr very heavy. Figure 2 shows results for our key metric Critical Success Index CSI. We see that globally and regionally for all lead times and precipitation rates Global Met Net continues to perform better than both the baselines HRES and post-processed HRES. At 0.2 mm/hr globally Met Net shows a performance improvement of 0.18 CSI points over HRES for the first forecasting hour and narrows the gap between the performance of post-processed HRES at about 12 hours. Even for higher precipitation rates of 25.0 mm/hr Met Net performs much better where HRES is largely unable to predict these extreme events whereas post-processed HRES at least performs better than HRES. At that higher rate of precipitation there is some visible noise in evaluation due to lack of sufficient observation data at these rates over any given region. Regionally we see that the performance of HRES in the US is much higher than that over other regions demonstrating the challenges with predicting chaotic precipitation in the tropics. Notably the Global Met Net model trained with radar as an additional input performs better only over regions where radar is included such as the USA. We do not see any influence of ground radar inputs in other places that do not have this data provided as an input to the model. Figure 3 Forecasting Accuracy Gap Critical Success Index CSI of Global Met Net vs. HRES in the Global South and Global North top and Tropics and Mid-Latitudes bottom validated against the GPM CORRA dataset at rates of 0.2 1.0 2.4 7.0 and 25.0 mm/hr. Global North includes areas covering USA Canada Europe Japan and Australia. Global South includes regions covering India South-east Asia Middle-east Africa Brazil Mexico Central America and South America a CSI for a precipitation rate of 1.0 mm/hr. b CSI for a precipitation rate of 2.4 mm/hr. Figure 4 Comparison of Critical Success Index CSI for HRES and Global Met Net nowcasts at different lead times 3 6 9 and 12 hours for light 1.0 mm/hr and moderate 2.4 mm/hr precipitation. Figure 3 shows forecasting accuracy gap between the Global South and Global North and also between the tropics and the mid-latitudes. In Figure 4 we plot the CSI scores for various regions on a map for better context in the improvements we see globally between HRES and Global Met Net. Remarkably Global Met Net elevates the forecast skill in the Tropics and Global South blue line to a level that is comparable to and for most lead times and precipitation rates exceeds the skill of the industry-standard HRES model in the data-rich Mid-latitudes and the Global North green line. At 2.4 mm/hr of precipitation Global Met Net is able to close this forecasting accuracy gap. Overall this doesn t just reduce the accuracy gap it effectively eliminates the gap for certain conditions representing a pivotal step toward global forecast equity. Figure 5 Critical Success Index CSI for Global Met Net models vs. NWP baselines in the US vs. MRMS Europe vs. Opera and Japan vs. JMA at precipitation rates of 0.2 2.4 7.0 and 25.0 mm/hr. Next in Figure 5 we present results evaluated against ground radar based precipitation estimates over the US from MRMS over Europe from the OPERA network Huuskonen et al. 2014 and over Japan from the Japan Meteorological Agency radars. We can see that the Global Met Net model even when trained without high quality ground radars outperforms global and regional NWP HRRR at all lead times up to 12 hours and at all rain rates. The performance of the model trained with the regional radars as an input is the highest up to 6 hours of lead time at all precipition rates. Note here that the prediction of Global Met Net models is optimized for the GPM CORRA dataset whereas we evaluate against radars in this figure and hence there is some loss inherently due to the discrepancy in observations between GPM CORRA and radar datasets. At higher rates such as 25 mm/hr some noise is visible due to lack of sufficient observation data at those points. These results demonstrate the high skill of the model against the best available ground truth even when the gold standard of ground-based radar networks are not available during training or inference. Achieving good skill despite the absence of radar inputs is particularly critical in the Global South where radars are not widely available. This indicates the model is learning meteorologically sound patterns rather than simply overfitting to the characteristics of a single sensor type. Figure 6 Frequency Bias Globally and by Region for Precipitation Rates of 0.2 2.4 and 25.0 mm/hr. When looking at the frequency bias of the Global Met Net models compared to HRES in Figure 6 we note that there is some variation in the bias at varying lead times rates of precipitation and regionally as well. For the 0.2 mm/hr precipitation rate we see that Global Met Net s bias stays close to 1 at all lead times both globally and regionally whereas raw HRES tends to overpredict these lower thresholds more than twice. As we get to the higher rates we can see that Global Met Net and post-processing HRES leads to an overprediction whereas HRES underpredicts globally. It should be noted that for more extreme precipitation it is better to over-predict and issue sufficient warning to end-users rather than leave them unprepared this is commonly known as wet bias. As uncertainty of the forecast increases with lead time for higher precipitation rates Global Met Net tends to overpredict accordingly. It is important to note here that the probabilistic inference from Global Met Net is categorized by applying probability thresholds optimizing for the CSI metric which results in sub-optimal frequency bias scores. However if one was interested in specifically optimizing frequency bias then it is possible to apply thresholds to optimize that instead and we noticed that it does not decrease the performance of CSI much at all. We also show results for a spatial verification metric fractions skill scores FSS Roberts and Lean 2008 for varying sizes of pixel neighborhoods from 0.05 to 1. In Figure 7 we show results of the Global Met Net models vs NWP models HRES and HRRR in the US using MRMS as the ground truth. Due to the narrow swaths of the GPM CORRA dataset it is not possible to apply spatial verification metrics such as FSS at much coarser resolutions therefore we provide results here against a dense ground truth like MRMS. The FSS quantifies the ability of a forecast to correctly identify precipitation patterns at different spatial scales with higher values indicating better skill. Fractions skill score is also an important metric to look at that avoids the double penalty problem Haiden and Lledo 2023 Figure 7 Fractions Skill Score FSS of Global Met Net vs. NWP Baselines in the US vs. MRMS for Various Precipitation Rates 0.2 2.4 7.0 and 25.0 mm/hr across a Range of Spatial Neighborhoods 0.05 FSS 1 to 1 FSS 21. that metrics like CSI may suffer from placing NWP models at a disadvantage. Overall Global Met Net has higher skill than both the other baselines at all of these neighborhood sizes precipitation rates and at all lead times. As expected looking at Figure 7 we note that the FSS generally decreases as the neighborhood size decreases from 1 to 0.05. This reflects the increasing difficulty of accurately predicting fine-scale precipitation features at higher resolution. Met Net is able to capture even the more chaotic heavier precipitation events also more skillfully than NWP models at earlier lead times and meets the HRRR model by hour 12 at finer resolutions. While HRRR shows higher skill at an extremely coarse 1 neighborhood this primarily reflects its ability to correctly place a large weather system within a very large general area. For the high-resolution scales that are most meaningful for nowcasting applications e.g. 0.05 to 0.25 Global Met Net consistently demonstrates superior skill in capturing the actual location and spatial structure of precipitation making it a more valuable tool for localized warnings. 3. Global Met Net 3.1. Datasets This section outlines the multi-modal datasets used by Global Met Net distinguishing between non-time-sensitive training targets and low-latency input features required for real-time inference. These datasets vary in spatial and temporal scales and real-time latencies collectively enabling global coverage and enhanced prediction capabilities. Further details on each dataset are available in the supplement. 3.1.1. Training Targets An ML model is optimized by taking in a set of inputs and corresponding targets to train against. Hence during inference when the model is operationalized the datasets used as model training targets do not need to be available with a low latency. This gives us an opportunity to use calibrated observations in our model as training targets. Ideally a global network of ground-based weather radars would provide the highest quality high-resolution precipitation data for training. However in reality this is a challenging task for a number of reasons. Radars can be expensive to install and maintain such as over the ocean or mountains or in places lacking relevant infrastructure and trained personnel. Many times even if radars exist they are owned by city governments or by different organisations even within a country and their data is not easily available for use by external organisations. Furthermore even if the raw radar data is readily available for use it can be noisy picking up false signals from flocks of birds wind farms and sun interference. A mountainous terrain or presence of tall buildings close to the station can further lead to inaccurate data. This raw radar data requires significant processing and cleanup before it can be used as a training target or for validation. To facilitate validation and training of the model on precipitation measurements from other parts of the world and especially the tropics we make use of NASA s Global Precipitation Measurement GPM mission s dual-frequency precipitation radar satellite. GPM provides a precipitation estimate using the CORRA algorithm which is sparse but provides global coverage see Figure 8 for a map of global coverage. Additionally we use the IMERG final precipitation estimate as another training target which is dense but has potential inaccuracies. Table 1 summarizes the features of the training targets used by the Global Met Net model where the target type shows that the GPM CORRA data is the main target which makes the actual predictions used in all of our evaluations and results. The other datasets serve as auxiliary training targets. Table 1 This table summarizes the training targets and their properties. Dataset Spatial Resolution Target Patch Size Coverage Target Type GPM CORRA 0.05 × 0.05 3600 × 7200 Sparsely global Main Ground Radars 0.05 × 0.05 3600 × 7200 Dense in US Europe Japan Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1 × 0.1 6 to 12 hours 63 1 last available timestamp HRES surface variables 0.1 × 0.1 6 to 12 hours 40 1 last available timestamp IMERG Early 0.1 × 0.1 5 to 6 hours 1 6 timestamps 30 mins apart Elevation 0.05 × 0.05 - 1 N / A Latitude - Longitude 0.05 × 0.05 - 2 N / A The geostationary satellite mosaics is a special dataset that we create through blending and calibration of multiple satellites and we go into the details of it next. Information on the rest of the inputs can be found in Supplement A.1. 3.1.3. Geostationary Mosaics We use a total of 7 geostationary satellites as inputs to our model that are combined into a mosaic to provide global coverage. Table 3 outlines the coverage provided by each of the satellites and the agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT Meteosat-12 Europe/North Africa EUMETSAT Himawari-9 East Asia Western Pacific Japan Meteorological Agency GOES-19 Eastern Americas Atlantic Ocean NOAA GOES-18 Western Americas Pacific Ocean NOAA GK-2A East Asia Western Pacific Korea Meteorological Administration 3.2. Model Setup This section details the data processing steps model architecture and the approach to generating probabilistic outputs. 3.2.1. Dataset Processing The datasets were split into separate partitions for model development and evaluation. The development dataset spans from 2018 to 2023 that we further split into a dataset for training the ML model and parameter optimization January 1 2018 to April 30 2022 and a smaller held-out set for fitting the probability thresholds May 15 2022 to May 15 2023. Finally the test dataset covering the period from June 1 2023 to May 31 2024 was designated for final model evaluation and performance assessment. Before training all datasets were preprocessed for consistency and quality. All the datasets except for the NWP data were resampled to a consistent 0.05 ×0.05 spatial resolution. All the 0.05 ×0.05 datasets undergo a space-to-depth Wang et al. 2020 operation with a block size of 2 which stacks each block of pixels to create more channels which allows the model to analyze spatial patterns at different scales more efficiently. The NWP data on the other hand was resampled to a 0.1 × 0.1 resolution and no space-to-depth operation is applied to it. Space-to-depth operation on higher resolution datasets was necessary firstly to fit the data into the memory constraints and secondly allowing concatenation of these higher resolution datasets with the lower resolution NWP data. This processing step brought all input datasets to a consistent effective grid size of 1800 × 3600 pixels before being fed into the model. We then normalize all of the input datasets to a zero mean and unit standard deviation values. The precipitation inputs from radar sources are normalized using log normalization due to the high skew of precipitation data. We then handle the missing or invalid data by replacing it with 0s. We also append each of the input datasets with timedeltas from the initialization time to inform the model. These timedeltas were effectively added as extra channels. All the time slices of the inputs are concatenated along the channel dimension then all the inputs are also concatenated together along the channel dimension to produce the final inputs to the model. Since the global data is represented through a rectangle we add a context of 18 degrees on each left and right edges of this rectangle to avoid any artificial border artifacts. This brings the entire input data to a spatial dimension of 2160 × 3600. Instead of using a recurrent layer like an LSTM to process the time sequence of inputs we concatenate the features from different input timesteps along the channel dimension. This creates a very wide tensor that the subsequent convolutional layers will process. This is a simpler but potentially effective way to provide temporal context. For the training data target patches containing only missing values for any given lead time were mostly excluded and only a small percentage of such samples were kept chosen at random. We had to do this as the GPM CORRA data is quite sparse and very many target lead times only contained missing values. This ensures the model learns from valid precipitation data and prevents it from being trained on patches with no information. By filtering out these entirely empty patches the model s training is focused on meaningful precipitation patterns and values. The targets are discretized by 30 different precipitation rates and any precipitation rate that is beyond a reasonable range of 2 meters/hour is replaced with a value of 0. 3.2.2. Model Architecture At its core Global Met Net like its predecessors Met Net and Met Net-2 use an encoder-decoder structure. The encoder processes the preprocessed input tensor learning a compressed representation of current and past weather conditions. The decoder takes this learned representation and generates forecasts at future lead times for various training targets configured as output heads. Here are some of the key architectural features Conditioning with Lead Time Similar to Met Net-2 we encode the lead time as a one-hot embedding with indices from 0 to 721 representing the range between 0 and 12 hours with a 15 min interval and map them into a continuous 32-dimensional representation. Instead of feeding the lead time embedding as an input the embedding is applied both as an additive and multiplicative factor Perez et al. 2018 to the model inputs and to hidden representations before each activation function. This ensures that the internal computation in the network depends directly on lead time. Initial Downsampling The concatenated input features are first passed through another space_to_depth operation. This further reduces spatial resolution and increases channel depth preparing the data for the main convolutional stack. Deep Residual Network The core of the encoder is a stack of residual blocks. Residual connections help in training very deep networks by allowing gradients to flow more easily. Multiple Stages The encoder has 4 stages of these residual blocks. Number of Blocks per Stage Each stage consists of 8 residual blocks. Channels per Stage The number of feature channels increases from 256 in the first stage to 384 in the subsequent stages. This allows the network to learn increasingly complex features. Cropping After each stage of residual blocks a cropping operation is applied. This progressively reduces the spatial extent of the feature maps. This is done because as network depth and neuron receptive fields increase border information becomes less relevant for predicting the central area. Upsampling and Final Convolution After the final residual blocks and cropping features are upsampled by repeating values to their initial resolution before passing through a final convolutional layer. Heads that require a higher output resolution than the encoder receive further upsampling and convolutional layers. 3.2.3. Training and Optimization Features Data Type The training casts all input data to bfloat16 for faster training and reduced memory usage with minimal precision loss on TPUs. Optimizer Uses the Adam optimizer with an initial learning rate of 3e-4 with a step change mid way through training at a lower rate of 1.5e-4. Polyak Averaging Averages model weights over training steps which can lead to better generalization. Memory Optimization Enables gradient checkpointing rematerialization for input preparation Res Net blocks and heads. This saves memory by recomputing activations during the backward pass instead of storing them all crucial for large models. Hardware Configuration The training job is executed on a 16x16 Dragonfish TPU pod which effectively has 256 TPU chips and 512 TPU cores in total. 3.2.4. Probabilistic Output Heads The model uses multiple output heads each optimized for a specific prediction target resolution and lead time. This allows each head to be optimized for the specific characteristics of its target variable while sharing the core of the encoder weights. In contrast to NWPs that model uncertainty with ensemble forecasts Global Met Net outputs a marginal probability distribution for precipitation at each location using a full categorical Softmax. Thus each output head is discretized into bins and the model outputs the probability of precipitation for each bin for each lead time. This probabilistic approach enables a more comprehensive assessment of forecast uncertainty and improves the practical utility of the nowcasts for decision-making. Once the model has finished training on the training split of the dataset we compute optimal probability thresholds for each discrete bin and each lead time. These thresholds are found by maximizing the CSI score on a held-out evaluation dataset. The probability thresholds a value between 0 and 1 that results in the highest CSI on aggregate on this evaluation dataset gets fixed for future inferences and final metrics computation on the testing dataset. To assess Global Met Net s effectiveness in real-world scenarios this section presents case studies focusing on high-impact precipitation events. A crucial aspect of this evaluation is accounting for the significant differences in operational latency between the models. HRES forecasts have a latency of approximately six hours whereas Global Met Net generates forecasts in under a minute. To ensure a fair and operationally relevant comparison our analysis visualizes the earliest available forecast from each model for a given point in time as illustrated in. For these comparative visualizations, HRES is represented by its direct, deterministic forecast value. Global MetNet’s visualization is derived from its probabilistic output. The model predicts probabilities for several precipitation rates (0.2, 1.0, 2.4, 5.0, 7.0, 10.0, 15.0, and 25.0 mm/hr). These probabilities are converted into a single deterministic forecast by applying thresholds optimized to maximize the Critical Success Index (CSI), as detailed in Section 3.2.4. The highest precipitation rate identified through this process is displayed. IMERG Final serves as an observational benchmark to estimate actual precipitation during the event. Figure 9 presents a side-by-side comparison of the HRES and Global MetNet forecasts against IMERG satellite precipitation estimates for a deep convective system that developed in West Africa on April 24, 2024. The forecasts visualize the models’ performance in capturing the thunderstorm’s development from 12:00 UTC to 19:00 UTC. HRES is initialized at 06:00 UTC and Global MetNet at 11:58 UTC, making forecasts from both models available for 12:00 UTC. The near-complete absence of the system in the HRES forecast produces a high number of misses, directly explaining the significantly higher recall scores for Global MetNet. Additionally, Global MetNet’s accurate prediction of the storm’s location and intensity, without generating widespread spurious precipitation, accounts for its large gains in precision and overall skill as measured by CSI. This case study illustrates an event where HRES exhibits virtually no predictive skill, while Global MetNet provides a highly accurate and actionable forecast. Both the statistical and case-study analyses demonstrate that Global MetNet represents a significant advancement over HRES for short-term quantitative precipitation forecasting. On April 24, 2024, a north–south oriented mesoscale convective system (MCS) developed in eastern Uganda, as shown in Figure 10. Within the MCS, multiple regions of moderate to strong convection were observed from 12–18 UTC. Throughout the day, the MCS moved westward and weakened in the evening due to the loss of diurnal heating. Convection along the Intertropical Convergence Zone (ITCZ) is particularly challenging for weather models because it is weakly forced and transient. This is reflected in HRES output, which shows widespread, scattered precipitation with low coherence between consecutive two-hourly forecasts. This makes the ITCZ an ideal setting for nowcasting methods that incorporate observational datasets. Statistical analysis again shows improvements in precision and CSI for Global MetNet due to improved prediction of precipitation location and intensity. Further analysis evaluates Global MetNet and HRES performance in a high-impact weather event: Tropical Cyclone Remal in the Bay of Bengal. Results reveal a key trade-off between the models’ forecast strategies. Global MetNet’s aggressive prediction of heavy rainfall yields superior overall skill despite reduced precision. IMERG data shows a well-defined tropical cyclone with strong circulation and curved rain bands containing embedded cores of intense precipitation (≥20 mm/hr). HRES captures the cyclone’s general location but severely underestimates rainfall intensity, producing a diffused precipitation field with almost no high-intensity cores, explaining its lower recall. Conversely, Global MetNet’s broader precipitation shield explains its lower precision. It correctly captures heavy rainfall where it exists (high recall) but also predicts heavy rain in gaps between actual rain bands (false alarms). HRES is initialized at 18:00 UTC on May 25, 2024, and Global MetNet shortly before 00:00 UTC on May 26, 2024. From a practical hazard-forecasting standpoint, Global MetNet’s behavior is more valuable: its high recall ensures that life-threatening extreme rainfall risks are not missed. HRES produces fewer false alarms but fails to reflect the true severity of the event. The work presented here introduces Global MetNet, an operational deep-learning-based system for high-resolution precipitation nowcasting that represents a major step forward in global forecast equity. By leveraging geostationary satellite imagery and the GPM CORRA dataset, Global MetNet circumvents key limitations of traditional models that rely heavily on ground-based radar infrastructure, which is sparse in the Global South. Results show that Global MetNet consistently outperforms industry-standard numerical weather prediction (NWP) models such as HRES and HRRR across all tested lead times and precipitation intensities. It significantly improves forecast skill in the tropics and other data-sparse regions, effectively narrowing the long-standing accuracy gap between the Global North and Global South. The model provides forecasts at approximately 0.05° spatial and 15-minute temporal resolution for up to 12 hours, with operational latency under one minute, making it highly suitable for real-world applications. Despite these advances, certain limitations remain. Training in data-sparse regions relies on GPM CORRA as a proxy for ground truth, but its satellite revisit times limit the amount of extreme rainfall data available. Additionally, the model tends to over-predict intense rainfall—a wet bias that is safer than under-prediction but lacks realistic spatial structures. This suggests a need to refine predictions to achieve sharper representations in accurate locations without sacrificing intensity. This research marks an important step toward democratizing access to accurate, life-saving weather information. Future work will address current limitations by refining probabilistic forecasts, reducing biases in extreme events, and incorporating additional observational sources such as lightning activity. We also aim to develop pathways for broader accessibility of this technology to meteorological agencies in developing nations. Through its deployment to millions of users on Google Search, Global MetNet already demonstrates operational readiness and real-world value, paving the way for AI-driven weather prediction that serves communities worldwide.'}, {'rank': 4, 'score': 5.0, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach. Automated rock classification from mineral composition presents a significant challenge in geological applications with critical implications for material recycling resource management and industrial processing. While existing methods using One-dimensional Convolutional Neural Network 1D-CNN excel at mineral identification through Raman spectroscopy the crucial step of determining rock types from mineral assemblages remains unsolved particularly because the same minerals can form different rock types depending on their proportions and formation conditions. This study presents a novel knowledge-enhanced deep learning approach that integrates geological domain expertise with spectral analysis. The performance of five machine learning methods were evaluated out of which the 1D-CNN and its uncertainty-aware variant demonstrated excellent mineral classification performance 98.37 0.006% and 97.75 0.010% respectively. The integrated system s evaluation on rock samples revealed variable performance across lithologies with optimal results for limestone classification but reduced accuracy for rocks sharing similar mineral assemblages. These findings not only show critical challenges in automated geological classification systems but also provide a methodological framework for advancing material characterization and sorting technologies. Rock classification and characterization are fundamental processes in the construction and mining industries particularly for material recycling and sustainable resource management. Accurate identification and analysis of rock types facilitate optimized resource utilization enhance operational efficiency and contribute to environmentally responsible practices. Traditional rock classification is primarily based on expert petrographic analysis which integrates macroscopic observations microscopic examinations and manual mineral identification often supplemented by chemical and/or mineralogical compositional data. However automated approaches based on mineral composition present unique opportunities in applications that require automated rapid and accurate classification. This study establishes a systematic framework for automated rock classification that focuses on three representative rock types that are both geologically significant and economically important granite sandstone and limestone. These rocks were selected based on three key criteria 1 their widespread occurrence in the earth s crust 2 their economic iye.ang unileoben.ac.at I.S. Ang martin.findl unileoben.ac.at M.J. Findl ORCID s Iye Szin Ang et al. Preprint submitted to Elsevier Page 1 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach significance in the construction energy and mineral industries and 3 their distinct mineralogical compositions that make them ideal candidates for automated classification systems. Although existing mineral identification methods achieve high accuracy 96% using Raman spectroscopy there is a fundamental methodological gap in automated classification of rock types from these mineral assemblages. Consequently the crucial step of automatically determining the rock types of these mineral assemblages remains an unresolved challenge. This study addresses the question How can an automated system to accurately classify various rock types based on mineral assemblages identified through Raman spectroscopy be developed To the best of our knowledge this work presents the first systematic approach to automatically classify rock types directly from Raman-identified mineral assemblages. A mineral-to-rock deduction classification system using Raman spectroscopic data was developed and evaluated to address this classification challenge. Raman spectroscopy was selected for its non-destructive analytical capabilities and its ability to provide distinct molecular fingerprints for different minerals making it particularly suitable for mineral-based rock classification. The system was initially validated with granite before being expanded to sandstone and limestone. These rocks present varying mineralogical complexities allowing for a systematic evaluation of the classification approach. By starting with this controlled scope the potential for automated classification can be systematically assessed before expanding to more complex scenarios. This approach combines a One-dimensional Convolutional Neural Network 1D-CNN with domain-expert rules in a baseline system leveraging both data-driven learning and established geological knowledge. An uncertainty-aware variant 1D-CNN-UNK was also explored designed to handle ambiguous cases and potential unknown mineral assemblages. Through this focused investigation foundational insights for developing more comprehensive systems capable of handling multiple rock types in automated settings are aimed to be established. The primary contributions of this research are summarized as follows 1. An integrated framework that combines a One-dimensional Convolutional Neural Network 1D-CNN with a knowledge-enhanced expert system is proposed. This hybrid approach enables automated mineral identification while incorporating domain expertise through systematic knowledge integration. Using both data-driven learning and established geological knowledge the framework addresses the limitations of traditional expert systems which often rely solely on predefined rules or heuristics. weighting system is developed. This system accounts for the variations of mineral assemblages and their relative abundances in different geological settings providing a more robust classification framework compared to Iye Szin Ang et al. Preprint submitted to Elsevier Page 2 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach traditional binary classification approaches. The methodology enhances classification accuracy by considering the nuanced compositional differences that are often overlooked in simpler models. database structured according to expert-designed rock composition templates. This validation methodology ensures geological validity through systematic sampling of diagnostic mineral assemblages expert-verified compositional relationships and high-quality spectral data from standardized sources. The dataset s design and the validation process are described in detail to underscore the rigor and reliability of the findings. since its early laser-based implementations. The integration of artificial intelligence AI has transformed spectral data processing and analysis by enabling the use of advanced machine learning algorithms that can handle large volumes of spectral data leading to improved pattern recognition and classification capabilities. This transformation has resulted in more data being processed efficiently better image processing techniques and the development of comprehensive spectral databases. For example AI-driven Raman spectroscopy has been successfully applied in the automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin Ang et al. Preprint submitted to Elsevier Page 3 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach materials. Their work emphasizes not only the need for rapid material characterization and reliable identification methods but also the potential for automated systems to support sustainable construction practices through improved material recycling. Despite these advances existing research has focused primarily on mineral-level identification through Raman spectroscopy and machine learning. Although these methods excel at identifying individual minerals they seem to face fundamental limitations when applied to rock type classification. This is due to the fact that rocks are composite materials formed by varying combinations and proportions of minerals the same minerals can occur in different proportions to form entirely distinct rock types. For example both granite and sandstone contain mainly quartz and feldspar but a granite is an igneous rock formed by magma crystallization while a sandstone is a sedimentary rock formed by the compaction of mineral grains. Their different formation processes result in distinct textures and structures that define them as separate rock types even though they share common minerals. Current research trends focus on enhancing mineral identification through improved data preprocessing and validation methodologies yet there remains a critical gap in the literature the lack of automated systems that can deduce rock types from identified mineral assemblages. The remainder of this paper is structured as follows Section 3 describes the methodology including the development of the integrated framework and the quantitative methodology for rock type classification Section 4 presents the results of the validation experiments and discusses the implications of the findings. Finally Section 5 concludes the paper and suggests future research directions. A rock is a naturally occurring solid aggregate composed of minerals fragments of minerals or rocks remnants of organisms or in some cases non-mineral substances. They typically comprise one or more rock-forming minerals and develop as a result of their specific geological setting. This study presents a systematic methodology for mineral assemblage-based rock classification using Raman spectroscopy focusing specifically on the identification of three different rock types granite sandstone and limestone. Our approach integrates 1D-CNN with knowledge-guided systems to create a robust classification framework. The methodology encompasses four key components 1 a classification framework that establishes the theoretical foundation for mineral-based rock type identification 2 systematic dataset collection and generation procedures 3 development and implementation of two distinct mineral classification models a baseline 1D-CNN approach and an uncertainty-aware model and 4 a hierarchical confidence-based knowledge-guided system that take advantage of established geological knowledge for final classification decisions. Iye Szin Ang et al. Preprint submitted to Elsevier Page 4 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach content for the e.g. plutonic rocks i.e. granite is determined in relation to their quartz alkali feldspar and plagioclase contents and normalized to 100 % which places the rock in the corresponding field in the QAPF diagram i.e. the granite field. Other rock forming mineral e.g. micas are not considered in this classification and are consequently neglected. This standardized classification scheme provides the theoretical foundation for the decision trees implemented in our expert system particularly for granite and other plutonic igneous rock classifications. For sedimentary rocks such as sandstone and limestone the classification rules were developed through knowledge elicitation from expert geologists. Sandstone classification is based on the content of the different grains which are normalized to 100% and plotted in triangular diagrams with the corner points of the quartz-feldspar -lithic rock fragments. Two diagrams are used based on the fine-grained matrix grain sizes 0.03 mm content. Over the years numerous classification schemes for clastic sediments have been proposed. For sandstones with a grain size of 2 mm the scheme originally developed by Krynine 1948 and later refined by Dott 1964 and Pettijohn et al. 1987 has become widely accepted Okrusch Frimmel 2022. This ternary classification diagram is based on the relative proportions of Q quartz F feldspar and L lithic fragments which are normalized to a total of 100%. Furthermore variations in matrix content typically characterized by mean grain sizes of 30μm are represented along an axis perpendicular to the ternary diagram. Rocks which contain 15% matrix are classified as arenites 15% matrix as wacke 75% matrix are classified as claystone. We concentrate on our expert developed sandstone arenite decision tree on quartzitic/quartz sandstones with a matrix content of 0 15%. Hence the sandstone decision tree represents quartzarenit sublitharenit and subarkose. The typical classification of limestones is based on the calcite-dolomite ratio. With respect to limestone we concentrate on a typical limestone 10% dolomite and a dolomitic limestone 10 50% dolomite. This process incorporated standard sedimentary rock classification schemes expert field identification practices practical experience in distinguishing key mineral assemblages and common textural and compositional indicators. For our mineral-based classification approach the sandstone classification rules mentioned above were augmented by the presence of characteristic accessory minerals and the relative abundance of matrix materials. Similarly the limestone classification incorporates carbonate mineral assemblages and common impurities. For granite no minor compounds are considered in addition to the main minerals. 3.2. Rock Classification Framework The automated classification of rocks from spectral data presents unique computational challenges due to the complex relationship between mineral assemblages and lithological classification. The proposed framework addresses these challenges through an integrated approach that combines spectral analysis with established petrological principles implemented via a dual-layer classification architecture. Iye Szin Ang et al. Preprint submitted to Elsevier Page 6 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach where wmaxis the highest weight among all rock types w2ndis the second highest weight θcis the confidence threshold 0.7 and θdis the dominance threshold 0.3. The weight calculation for each rock type incorporates the relative proportions of key minerals as summarized in Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.3. Dataset Collection and Generation The RRUFF database contains approximately 7 000 mineral samples which represent 3 500 distinct mineral species. This discrepancy arises because multiple samples can belong to the same mineral species leading to a higher number of samples than species. Our analysis revealed a significant class imbalance with 1 522 mineral classes containing fewer than five samples and the corresponding spectral data. In this context a class refers to a specific mineral species that our model aims to identify. Given the complexity and breadth of the RRUFF dataset and the fact that no prior study has addressed this specific problem we chose to start our investigation by focusing on specific rock types and their related minerals. This approach allows us to simplify the initial scope of the study while still addressing a meaningful and practical subset of the data. Rather than employing standard data augmentation techniques that might introduce artifacts we adopted a geologically-informed sampling strategy. Sampling in this study refers to the process of selecting specific mineral samples from the RRUFF database. Our sampling strategy was geologically-informed focusing on minerals commonly found in granite sandstone and limestone formations see mineral selections in Figures 2 to 4. This selective approach ensures that our dataset is relevant to real-world field conditions and aligns with our hybrid architecture which integrates expert knowledge to compensate for limited training samples. This geological context-driven selection reflects both analytical and practical considerations. The use of unoriented spectra aligns with real-world field conditions where rock samples are not systematically oriented prior to analysis resulting in typically random crystallographic orientations of the present mineral phases. Rather than employing a purely data-driven approach that might retain overrepresented but geologically irrelevant mineral species our selection methodology prioritizes the minerals characteristic of these three rock types regardless of their representation in the database. This selective sampling strategy aligns with our hybrid architecture where expert knowledge compensates for limited training samples through geological constraints effectively addressing both the class imbalance challenge and the need for geologically meaningful classifications. Due to this limited sample size we expanded our dataset using two synthetic data generation methods. First we applied a PCA-based approach for minerals with larger datasets and second we used a direct variation method for minerals with limited samples. Our target dataset sizes were determined by applying a 4× multiplication factor to the initial sample counts resulting in projected totals of 439 samples for minerals associated with granite 449 for minerals associated with limestone 515 for minerals associated with sandstone and 123 for other minor minerals. It is important to note that these samples are mineral spectra not rock samples. The classification of rocks is inferred from the spectral data of their constituent minerals. All spectra were obtained from processed RRUFF data which typically includes standard preprocessing steps such as background subtraction and peak fitting though specific processing methods may vary as the database aggregates contributions from multiple research institutions worldwide. Iye Szin Ang et al. Preprint submitted to Elsevier Page 10 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach The effectiveness of this geological context-driven selection approach was later validated through expert-designed test cases as detailed in Section 3.5.5. 3.4. Mineral Classification For mineral classification four machine learning models were implemented and tested Support Vector Machine SVM Random Forest RF Multilayer Perceptron MLP and two variants of One-dimensional Convolutional Neural Networks 1D-CNN. Each model was trained using an expanded dataset of 1366 mineral samples. Two versions of the 1D-CNN architecture were developed. The base model consists of two convolutional layers 16 and 32 channels with Re LU activation and max pooling operations. The network processes the input spectra through these layers before passing through two fully connected layers to output predictions for the fourteen mineral classes. To handle unknown minerals the base architecture was enhanced with an uncertainty-aware version. This model maintains the same structure but incorporates Monte Carlo dropout layers with a rate of 0.3 after each convolutional layer and the first fully connected layer. During inference 30 stochastic forward passes are performed with enabled dropout layers allowing the estimation of both the predictive mean and variance for uncertainty quantification. This uncertainty estimation helps identify when the model encounters mineral spectra that do not match the fourteen defined classes. Both models were trained using cross-entropy loss and the Adam optimizer with a learning rate of 0.001. To avoid overfitting early stopping was implemented with a patience of 20 epochs which means training was stopped if the validation loss did not improve for 20 consecutive epochs. 3.5. Rule-Based Expert System The expert system was developed to automate rock classification based on Raman spectroscopy point measurements incorporating domain knowledge from mineralogy. The system employs a set of hierarchical rules that evaluate both prediction accuracy and the presence of mineral assemblages characteristic of different rock types. 3.5.1. Knowledge Base and Definitions The knowledge base KBis defined as a quintuple KB G R H P C 3 where G G1 G2... GK represents K distinct mineral assemblages R R1 R2... RL denotes L rock type classification rules H V E defines the hierarchical decision tree structure Iye Szin Ang et al. Preprint submitted to Elsevier Page 11 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach P pv v V comprises classification and composition parameters C C1 C2... CJ represents both compositional and confidence constraints For any mineral mand group Giwith weight wi the weighted membership function is defined as wi if m Giand pmin i fi m pmax i 0 otherwise 4 μGi m wi 3.5.2. Compositional Rules and Constraints The confidence-based compositional requirements for each rock type are formalized through constraint functions Cj M Gi w Rl fj ni αj w Rl βj 5 where fjrepresents a constraint function niis the count of minerals from group Giin measurements M αjis a parameter vector w Rlis the importance weight for rule Rl βjdefines the threshold value The classification rule incorporating confidence thresholds is expressed as Cconf wmax w2nd 6 Rl M j l Cj M Gij wij where Cconf wmax w2nd wmax θc wmax w2nd θd 3.5.3. Mineral Assemblages Mineral assemblages for each rock type are represented as weighted sets with composition ranges ARl Gi wi pmin i pmax i Gi G wi pmin i pmax i 7 where wirepresents the diagnostic weight and pmin i pmax i defines the valid composition range of mineral group Gi. Iye Szin Ang et al. Preprint submitted to Elsevier Page 12 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 3.5.4. Weighted Importance Model Given the weighted mineral assemblages ARl we define the confidence-adjusted probability of a rock type rule Rl given measurements Mas P Rl M wni i δi 8 Gi wi pmin i pmax i ARl where wiis derived from geological composition ranges and importance weights ni count M Gi is the number of minerals from group Giin measurements M δiis an indicator function that equals 1 if pmin i fi M pmax i and 0 otherwise 3.5.5. Evaluation Framework To evaluate the expert system s performance under the constraints of open-source geological datasets 10 test cases for each rock type were utilized specifically designed by an expert geologist. These cases represent both confident and non-confident classification scenarios that occur in real geological settings. For confident cases mineral assemblages that unambiguously indicate specific rock types were selected representing typical compositions found in well-documented geological formations. In contrast non-confident cases were designed with mineral assemblages that clearly indicate different rock types challenging the system s classification capabilities. Confusion matrix analysis was used to quantitatively assess classification performance. This analysis measures true positives correct rock type identification false positives incorrect identification of rock type true negatives correct rejection of wrong rock type and false negatives incorrect rejection of correct rock type. From these measurements standard performance metrics including accuracy precision recall and F1-score were calculated to provide a comprehensive assessment of the system s classification capabilities. 4. Results Discussion The experimental evaluation of our mineral assemblage-based classification framework encompasses three key aspects the performance of individual mineral identification the efficacy of the integrated rock classification system and the analysis of current limitations. Quantitative results from both the baseline and uncertainty-aware models are presented followed by a comparative analysis of their performance across a validation set. As highlighted by Resch et al. excavated materials from tunnel projects have diverse reuse pathways limestone can serve as raw material for the steel industry and feedstuffs production soil can be utilized for brick manufacturing Iye Szin Ang et al. Preprint submitted to Elsevier Page 13 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach rock dust has applications in agricultural land improvement and certain rock types yield valuable industrial minerals such as mica for the paint industry. Therefore accurate classification of rock types is crucial for optimizing the reuse of excavated materials and minimizing environmental impact. 4.1. Mineral classification The performance of different machine learning models for mineral classification was first evaluated. Figure 5 shows the mean accuracy across five-fold cross-validation for SVM Random Forest MLP 1D-CNN and 1D-CNN-UNK models. The 1D-CNN model achieved the highest accuracy at 98.37% while the 1D-CNN-UNK model showed slightly lower performance at 97.75%. Both models outperformed the baseline approaches SVM at 88.43% Random Forest at 95.85% and MLP at 96.25%. Although a confusion matrix would provide detailed per-mineral performance overall accuracy was focused on the main metric since the performance of the integrated system is the primary concern. Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach 4.2. Integrated System Performance a knowledge-guided 1D-CNN b uncertainty-aware knowledge-guided 1D-CNN Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach the classification accuracy decreases when processing complex mineral assemblages within whole rock samples ΔF1sandstone 0.19. Third the expert system rules despite incorporating established geological constraints demonstrate limited effectiveness in differentiating between compositionally similar rock types P granite 35% for both models. The observed performance patterns manifested most significantly in cases where rocks share similar mineral constituents in varying proportions such as granite and sandstone. The uncertainty-aware variant s performance metrics indicate that the primary challenge extends beyond the disparity between single-mineral training data and whole-rock classification objectives. Future research priorities should focus on the development of comprehensive mineral assemblage training datasets that capture spectral interactions within whole rock samples. Additionally measuring the relative amounts of different minerals could enhance the analysis providing more nuanced insights into rock composition and potentially improving classification accuracy. These findings indicate that improving classification accuracy requires addressing both fundamental data represen-tation challenges and the methodology for handling compositional uncertainty in rock sample analysis. 4.3. Limitation Although the method effectively detects the presence of specific minerals through their characteristic Raman spectral signatures it faces several key challenges i compositional ambiguity and ii classification constraints. Because different rock types can share similar mineral assemblages while having distinct geological origins and classifications a classification overlap was observed in the results. As demonstrated by our confusion matrices Fig-ure 6 both granites and sandstones exhibit significant classification overlap due to their shared mineral constituents despite different proportions. The binary presence/absence nature of the current approach does not capture the subtle variations in mineral proportions that often distinguish different rock types. This limitation is particularly evident in the low precision rates for granite classification 35% and the systematic patterns of misclassifications observed between compositionally similar rock types. and knowledge-enhanced deep learning methodologies. Our quantitative analysis based on a limited dataset of 30 samples demonstrates the effectiveness of the hybrid mineral-to-rock classification framework. The 1D-CNN architecture achieved 98.37 0.006% accuracy in mineral identification while the uncertainty-aware variant achieved 97.75 0.010% accuracy. The implementation of confidence thresholds in the knowledge system governed by the weighting function introduced in this paper provides systematic discrimination between compositionally similar rock Iye Szin Ang et al. Preprint submitted to Elsevier Page 16 of 23 Rock Classification through Knowledge-Enhanced Deep Learning A Hybrid Mineral-Based Approach types. This is particularly evident in the classification of limestone samples with a precision of 66.7% recall of 57.1% and an F1-score of 0.62. The methodological framework addresses some of the fundamental challenges in automated rock classification through the systematic integration of spectroscopic data analysis and expert geological knowledge. The achieved accuracy demonstrates the effectiveness of our knowledge-enhanced approach in compensating for data sparsity through expert rule integration. However it is important to note that the small sample size n 30 limits the generalizability of these findings and further validation with a larger dataset is necessary. While this preliminary investigation establishes methodological feasibility it also highlights clear pathways for future development. The transition from controlled laboratory conditions to conveyor belt operations presents opportunities for technological advancement. Integrating multiple sensing modalities and optimized data acquisition protocols could reduce the amount of laboratory experiments required although laboratory validation will remain essential. These developments coupled with our demonstrated success in mineral identification and classification provide a robust framework for advancing automated geological characterization systems. This study serves as a foundational effort in the subfield of petrology where open datasets are currently limited. By demonstrating the potential of our approach we aim to inspire the creation of comprehensive open-source mineral assemblage datasets. Such datasets would enable more robust validation of automated classification systems and further enhance the advantages of our knowledge-enhanced approach. Additionally incorporating the relative ratios of minerals in the analysis could improve classification accuracy and address the compositional ambiguity observed in this study. The established methodology not only addresses current industrial needs but also lays the groundwork for more comprehensive rock type classification systems positioning this research at the forefront of automated geological analysis.'}, {'rank': 5, 'score': 5.0, 'id': '2510.05163v1', 'title': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'text': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems enabling higher accuracy resilience to spoofing and seamless integration with hardware-based solutions. At the same time smart card technologies have evolved to include on-chip biometric verification cryptographic processing and secure storage thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work 2019 2025 at the intersection of deep learning biometrics and smart card technologies for MFA. We analyze biometric modalities face fingerprint iris voice review hardware-based approaches smart cards NFC TPMs secure enclaves and highlight integration strategies for real-world applications such as digital banking healthcare IoT and critical infrastructure. Furthermore we discuss the major challenges that remain open including usability security tradeoffs adversarial attacks on deep learning models privacy concerns surrounding biometric data and the need for standardization in MFA deployment. By consolidating current advancements limitations and research opportunities this survey provides a roadmap for designing secure scalable and user-friendly authentication frameworks. In today s hyperconnected digital environment safeguarding user identity has become more critical than ever. With increasing reports of data breaches phishing scams and account hijacking traditional password-based authentication has proven insufficient. Passwords are not only prone to being forgotten or reused but are also vulnerable to brute-force attacks phishing and database leaks. These vulnerabilities have driven the widespread adoption of Multi-Factor Authentication MFA as a more robust alternative. MFA relies on the combination of independent identity factors something the user knows e.g. a password something they have e.g. a smart card and something they are e.g. a biometric trait. This layered approach significantly improves security by ensuring that compromising one factor does not grant access to protected systems. According to recent surveys modern digital services including financial platforms compliant with regulations such as PSD2 increasingly require MFA to mitigate risks associated with single-factor systems. Privacy-preserving MFA approaches leveraging deep learning and biometrics have also been proposed. 1 Biometric authentication including modalities such as face fingerprint iris and voice recognition has emerged as a popular inherence factor in MFA systems. Biometrics offer nontransferable user-specific traits improving usability and reducing reliance on memory. Yet they raise unique challenges in terms of spoofing privacy and demographic bias. Recent advances in deep learning DL have significantly improved the accuracy and reliability of biometric systems. Convolutional Neural Networks CNNs and other deep architectures have shown strong performance in extracting robust features from noisy or occluded biometric data enabling real-time and multimodal biometric authentication. DL techniques also power liveness detection domain adaptation and template security enhancing resilience against spoofing and adversarial attacks. In parallel possession-based factors have evolved. Smart cards Trusted Platform Modules TPMs and secure enclaves now provide cryptographic computation biometric match-on-card and tamper-resistant credential storage. Recent advances in biometric payment cards and NFCenabled devices illustrate how hardware tokens can securely integrate with DL-based biometric authentication to form compact and user-friendly MFA schemes. These solutions reduce fraud while ensuring compliance with data protection requirements. Despite this progress several open challenges remain ensuring usability without degrading security defending against presentation attacks preserving biometric privacy and ensuring interoperability across vendors and regulatory frameworks. Objectives. This survey provides a comprehensive overview of recent research 2019 2025 on DL-based MFA systems integrating biometrics and smart cards. Specifically it Reviews deep learning methods applied to biometric authentication within MFA frameworks. Examines smart card and hardware-based approaches for secure factor integration. Compares architectures fusion strategies datasets and benchmarks employed in state-ofthe-art systems. Analyzes threat models and countermeasures against adversarial and spoofing attacks. Identifies open research questions and outlines future directions for scalable privacypreserving and user-friendly authentication. 2 Background MFA Biometrics and Smart Cards Multi-Factor Authentication MFA is a security mechanism that requires users to verify their identity using a combination of independent factors typically categorized as knowledge something the user knows possession something the user has and inherence something the user is. The rationale behind MFA is that compromising multiple independent factors is significantly harder for attackers thus enhancing overall system security. Traditionally authentication relied heavily on passwords. However due to their susceptibility to guessing phishing and large-scale leaks passwords alone have become increasingly risky. In response MFA has been adopted across domains handling sensitive data particularly digital banking e-government and healthcare Io T. Biometric authentication represents the inherence factor leveraging physiological traits e.g. face fingerprint iris or behavioral patterns e.g. gait keystroke dynamics. Unlike passwords or tokens biometric traits are intrinsic to individuals and difficult to replicate. However they raise challenges such as privacy concerns irrevocability and vulnerability to spoofing attacks. To mitigate these risks standardization bodies e.g. ISO/IEC 30107 have proposed presentation attack detection PAD guidelines and researchers are increasingly focused on fairness and robustness across demographics. 2 Smart cards typically associated with the possession factor are tamper-resistant hardware devices capable of securely storing credentials cryptographic keys and even performing biometric matching. In the banking sector EMV-compliant cards enable secure offline authentication through digital signatures. When combined with biometrics smart cards can implement matchon-card verification ensuring that sensitive templates never leave the card s secure chip. Beyond traditional smart cards Trusted Platform Modules TPMs and Secure Enclaves extend these guarantees to general-purpose devices such as smartphones and laptops. These components isolate sensitive data and computations enabling secure biometric enrollment inference and key storage and underpin modern standards such as FIDO2 and Web Authn. In practice effective MFA requires balancing usability cost and risk. A typical modern system may combine a fingerprint scan inherence a smartphone secure enclave or biometric smart card possession and a PIN or behavioral pattern knowledge/behavior. The integration of deep learning into biometric systems coupled with trusted hardware marks the next evolution in MFA explored in detail in the following sections. Deep learning DL has fundamentally transformed biometric authentication by enabling endto-end learning robust feature extraction and scalability across diverse modalities. Traditional biometric systems relied on handcrafted features which often lacked generalizability across populations or environmental conditions. DL models particularly Convolutional Neural Networks CNNs Recurrent Neural Networks RNNs and Transformers now power state-of-the-art systems for face fingerprint iris voice and behavioral biometrics. 3.1 Facial Recognition and Anti-Spoofing Facial recognition has rapidly advanced with architectures such as Face Net Arc Face and Cos Face which learn highly discriminative embeddings from images. Wang and Deng survey modern DL-based face recognition. However face systems remain vulnerable to spoofing 2D photos replay videos 3D masks. Liveness detection networks address this by analyzing texture motion cues or physiological signals e.g. eye blinking r PPG. Guo et al. proposed CNN-based liveness detection while recent works integrate anti-deepfake detection. 3.2 Fingerprint and Iris Recognition DL enhances fingerprint authentication by improving minutiae detection ridge classification and partial print matching. Zahid et al. show CNNs outperform traditional Gabor-based methods under noisy conditions. Similarly iris recognition benefits from CNNs and attentionbased models trained on datasets such as CASIA-Iris and ND-Iris robust to illumination and pupil dilation. 3.3 Voice and Behavioral Biometrics DL also advances speaker verification via spectrogram-based CNNs and LSTM embeddings. Combining voice with face audiovisual biometrics strengthens robustness for remote banking authentication. Behavioral biometrics keystroke gait touchscreen mouse movement enable continuous MFA. Verma et al. demonstrate smartphone motion-based DL models for adaptive MFA. 3 algorithms. The synergy between biometric recognition and possession-based hardware is foundational to secure MFA. 4.1 Smart Cards in MFA Systems Smart cards have long been used in authentication due to their ability to securely store user credentials and perform local computations. In modern MFA biometric smart cards BSCs integrate fingerprint or facial recognition sensors directly into the card or terminal. There are two main architectures Match-on-Card Mo C Biometric matching is performed entirely on the card s chip and the template never leaves the card. This ensures maximum privacy. Match-off-Card The biometric is matched externally with the template read from the card. This mode is more flexible but less private. Mo C provides superior privacy and is increasingly supported by commercial products such as Idemia s biometric payment cards and Gemalto s biometric EMV solutions with technical standards maintained by EMVCo. 4.4 Smart Card + DL System Architectures Recent architectures combine deep learning with secure hardware to enhance biometric verification DL on-chip Miniaturized CNNs embedded in smart cards or tokens. Secure template fusion Combining multiple traits e.g. fingerprint + iris with fused templates stored in secure elements. On-device adaptation DL models fine-tuned per user during enrollment stored within TEEs. Tani et al. 2025 validated the feasibility of such architectures for real-world banking authentication. 4.5 Challenges in Hardware-Based MFA Despite their advantages hardware-integrated MFA systems face challenges Cost and Scalability Biometric smart cards are more expensive than traditional tokens. Hardware Standardization Fragmentation across vendors complicates integration. Energy Constraints DL inference on low-power chips requires lightweight models and quantization. Ongoing research explores energy-efficient DL models and secure co-processors to address these limitations. The integration of deep learning DL based biometric authentication with traditional MFA systems introduces several design options depending on modality fusion user experience UX requirements and environmental constraints. This section discusses core system-level integration strategies such as fusion techniques adaptive MFA policies and UX considerations like latency. 5.1 Fusion Techniques in DL-Based MFA To leverage the strengths of multiple authentication factors especially in multimodal biometrics systems often use fusion strategies to combine different sources of evidence. Fusion can occur at various levels Sensor-Level Fusion Raw biometric signals e.g. fingerprint + face are captured and preprocessed jointly. Feature-Level Fusion Deep embeddings from CNN/RNN models are concatenated before classification. Score-Level Fusion Independent DL models output match scores that are weighted and combined. Decision-Level Fusion Each modality votes independently a decision is made based on predefined logic e.g. majority voting. Score-level fusion offers a balance between flexibility and performance and is widely used in commercial systems. Decision-level fusion is preferred in scenarios with hardware heterogeneity or legacy compatibility. 7 6.5 Summary DL-based MFA improves identity assurance but still faces challenges in adversarial robustness deepfake resistance privacy and bias mitigation. The future of trustworthy MFA depends on standardized testing open datasets and secure hardware-software co-design. 6.6 Standardization and Interoperability DL-MFA systems often combine heterogeneous sensors inference engines and secure hardware. Without standards integration is fragile and error-prone. Key standards include FIDO2/Web Authn ISO/IEC 30107 PAD and EMVCo for biometric payment cards. Future work must emphasize pluggable frameworks formal verification of workflows and government-led certification e.g. e IDAS NIST 800-63. 6.7 Summary While DL-based MFA significantly strengthens digital identity protection its adoption introduces challenges in robustness privacy fairness and interoperability. Addressing these threats requires cross-disciplinary efforts spanning ML research hardware security regulatory compliance and usability studies. 7 Datasets Benchmarks and Metrics Robust evaluation of DL-based MFA systems requires standardized biometric datasets and consistent performance metrics. This section presents widely used benchmarks for facial fingerprint iris and multimodal biometrics and outlines key evaluation criteria such as FAR FRR and EER. 7.1 Benchmark Datasets for Biometric MFA Research in DL-powered biometric authentication relies on curated datasets that represent different modalities under diverse conditions. The quality and bias of these datasets significantly influence model generalizability. Well-established datasets underpin biometric research. Examples include LFW VGGFace2 Age DB CASIA-Iris V4 and FVC2004 which have become de facto benchmarks for DL-based evaluation. Many of these datasets include variations in lighting pose aging and acquisition devices to simulate real-world conditions. Public availability supports benchmarking and fair comparison across DL architectures. 7.2 Performance Metrics in MFA Systems Accurate evaluation of DL-based MFA systems requires standardized biometric metrics and protocols as defined in ISO/IEC 19795 and NIST SP 800-63B. These standards ensure comparability across algorithms datasets and deployment environments. False Acceptance Rate FAR Probability that an impostor is incorrectly accepted as a genuine user. FAR is critical for measuring system security and is often reported at operating points such as FAR 10 3 or 10 4. False Rejection Rate FRR Probability that a legitimate user is incorrectly rejected. FRR reflects system usability and user experience. Equal Error Rate (EER): The rate at which FAR and FRR are equal, often used as a single scalar indicator of performance. Lower EER implies a more accurate system. Receiver Operating Characteristic (ROC): A curve plotting the trade-off between FAR and True Positive Rate (1–FRR). Widely used to visualize model discriminability. Detection Error Tradeoff (DET): A log-scaled version of the ROC curve emphasizing low-error regions, recommended by ISO/IEC 19795 for biometric evaluations. Failure to Enroll (FTE) / Failure to Acquire (FTA): Rates at which biometric data cannot be captured or enrolled successfully, critical for deployment evaluation. Authentication Latency: The time required to complete an MFA process (biometric + token verification). Recommended latency for practical systems is below 1.5 seconds. In multimodal DL-MFA, researchers also evaluate: Fusion Gain: Improvement in EER or accuracy when combining multiple modalities compared to single-modality baselines. Robustness to Noise and Spoofing: Performance degradation under environmental noise, aging, or adversarial conditions. Template Security Impact: Trade-offs between encryption, privacy-preserving operations, and system accuracy. Public benchmarks such as FVC2004, CASIA-IrisV4, and AgeDB are typically evaluated using these metrics under standard protocols. As digital ecosystems expand and cyber threats evolve, MFA has become a central pillar of secure access control. This survey reviewed how deep learning, biometric modalities, and hardware tokens such as smart cards and secure enclaves can converge to build the next generation of MFA systems. Improvements include accuracy, liveness detection, and multimodal fusion, while challenges include robustness, privacy, fairness, and interoperability.'}]",The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy.
