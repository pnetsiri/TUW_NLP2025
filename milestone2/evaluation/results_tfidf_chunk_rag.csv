question,predicted_paper,correct_paper,is_correct,retrieved_ids,retrieved_scores,retrieved_titles,retrieved_contexts,generated_answer
How are Transformers different from RNNs?,2510.05736v1,,,"['2510.05736v1', '2510.05163v1', '2510.10822v1', '2510.05736v1', '2509.23158v1']","[0.08708555213522122, 0.06665691419637076, 0.04911865463273481, 0.042831170493336124, 0.03933275791225555]","['Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization']","[{'rank': 1, 'score': 0.08708555213522122, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': 'Trees BDTs trained on parameterized image features or goodness-of-fit parameters for this task 3 5. Consequently a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Multiple studies have explored deep learning methods for identifying γ-rays and demonstrated exceptional performance on simulated data 6 10. Most model architectures use convolutional neural networks CNNs for extracting information from camera images and recurrent neural networks RNNs for its aggregation across an event. Graph neural networks GNNs applied on images represented as point clouds have also been established as a viable approach for the same task. Despite their potential a complete deployment of deep learning-based models on IACT data remains non-trivial due to a variety of issues ranging from observational systematics to discrepancies between simulations and real-world data. The construction of models that can generalize to unseen situations is a long-standing problem in deep learning. The use of network architectures with inductive biases suitable for the given task can lead to improved generalizations by guiding the learning process towards more physically meaningful representations. For example translational invariance in CNNs temporal dependence in RNNs and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to thus contributing to their success in'}, {'rank': 2, 'score': 0.06665691419637076, 'id': '2510.05163v1', 'title': 'Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches', 'text': 'traditional smart cards Trusted Platform Modules TPMs and Secure Enclaves extend these guarantees to general-purpose devices such as smartphones and laptops. These components isolate sensitive data and computations enabling secure biometric enrollment inference and key storage and underpin modern standards such as FIDO2 and Web Authn. In practice effective MFA requires balancing usability cost and risk. A typical modern system may combine a fingerprint scan inherence a smartphone secure enclave or biometric smart card possession and a PIN or behavioral pattern knowledge/behavior. The integration of deep learning into biometric systems coupled with trusted hardware marks the next evolution in MFA explored in detail in the following sections. Deep learning DL has fundamentally transformed biometric authentication by enabling endto-end learning robust feature extraction and scalability across diverse modalities. Traditional biometric systems relied on handcrafted features which often lacked generalizability across populations or environmental conditions. DL models particularly Convolutional Neural Networks CNNs Recurrent Neural Networks RNNs and Transformers now power state-of-the-art systems for face fingerprint iris voice and behavioral biometrics. 3.1 Facial Recognition and Anti-Spoofing Facial recognition has rapidly advanced with architectures such as Face Net Arc Face and Cos Face which learn highly discriminative embeddings from images. Wang and Deng survey modern DL-based face recognition. However face systems remain vulnerable to spoofing 2D photos replay videos 3D masks. Liveness detection networks'}, {'rank': 3, 'score': 0.04911865463273481, 'id': '2510.10822v1', 'title': 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'text': 'subgroup analysis is affected by class imbalance particularly for Black patients. This underrepresentation limits the statistical robustness of bias evaluations and may obscure subtle disparities. Moreover our work focuses only on CNN-based models applied to CXRs. Therefore the generalizability of our findings to other imaging modalities e.g. CT MRI and tasks e.g. segmentation remains to be established. Finally our approach relies on last-layer retraining. While efficient it may be insufficient to fully mitigate spurious correlations compared with approaches leveraging representations from all network layers. Future work include extending the framework to other architectures such as Vision Transformers Vi Ts and applying and validating this approach on other imaging modalities and in tasks beyond classification.'}, {'rank': 4, 'score': 0.042831170493336124, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': 'representations. For example translational invariance in CNNs temporal dependence in RNNs and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to thus contributing to their success in their domains. In the context of IACTs as each image is a projection of the same event there is no inherent ordering between them making them permutation equivariant. This motivates a shift towards exploring GNNs for aggregating information across multiple telescopes. To that end this work introduces a combined convolutional and graph neural network CNN-GNN based approach for γ/hadron separation in IACTs. Three models with two distinct training strategies are proposed and evaluated on simulated data. These include a CNN-GNN model trained on image and graph data an enhanced CNN-GNN variant with additional reconstructed event information incorporated into the graph structure and a baseline GNN model utilizing image moments serving as a reference for existing methods. The models were trained on simulations of the High Energy Stereoscopic System H.E.S.S. located in Go llschau Namibia. The H.E.S.S. array consists of four 12-meter telescopes CT1-4 arranged in a square of side 120 meters with an additional 28-meter telescope CT5 at its center. Diffuse proton and γ-ray events simulated at a zenith of 20 and with a maximum view cone of 5 were'}, {'rank': 5, 'score': 0.03933275791225555, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al.'}]",Consequently a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Most model architectures use convolutional neural networks CNNs for extracting information from camera images and recurrent neural networks RNNs for its aggregation across an event.
What are the advantages and drawbacks of batch normalization compared to layer normalization?,2510.12758v1,,,"['2510.12758v1', '2510.08662v1', '2510.08662v1', '2510.12758v1', '2510.11073v1']","[0.0970646242055396, 0.08874805505678277, 0.07851098479279076, 0.07265644329343962, 0.0698451110790722]","['PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer']","[{'rank': 1, 'score': 0.0970646242055396, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': 'section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. Finally a fully connected multi-layer perceptron MLP block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs we incorporate a self-gating mechanism. This approach assigns variable weights to the input data enabling the model to discern and selectively integrate relevant information from both the moving and reference PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input ensuring that the most informative parts have a greater influence on the outcome of the motion estimation which is formulated as follows Gref Gmov σ G Aref σ G Amov HW D where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features'}, {'rank': 2, 'score': 0.08874805505678277, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': 'sequence dimension of the model s input tensor faithfully represents the physical arrangement of SNPs along the chromosome thereby allowing the convolutional and selfattention layers to effectively capture local and long-range spatial dependencies. 5 Uniform Length Padding To facilitate batch processing and conform to the network s fixed input dimensionality the φ S f a1 f a2 0 1 8 1 where denotes concatenation. This representation preserves the positional order of alleles and ensures that distinct diploid genotypes are equidistant in the feature space a crucial property that enhances its suitability for attention-based models. 2 Chromosome Segmentation via MAP File The encoded SNP sequence is partitioned into chromosome-specific subsequences based on the identifiers and physical coordinates where Q Query K Key and V Value are vectors derived from the input sequence via linear transformations and dk is the dimension of the key vectors. This mechanism allows the model to globally assess the relational importance of all feature pairs effectively weighing contributions from different chromosomal regions. The attention output is subsequently passed through Layer Normalization and a position-wise FeedForward Network FFN which enhances the model s representational capacity and stabilizes the training process. 3 Phenotype Prediction and Loss Function The features refined by the Transformer are flattened and passed through a Multi-Layer Perceptron MLP for regression yielding the final predicted phenotype'}, {'rank': 3, 'score': 0.07851098479279076, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': '1D convolutional layers Conv1D and Re LU activations. The Conv1×1 term denotes a shortcut connection for dimensionality matching and BN is Batch Normalization. This module transforms the raw sequence Xj of each chromosome into a high-level feature map Ej. 2 Cross-Chromosome Information Fusion To model long-range dependencies and potential epistatic effects between different chromosomes the feature maps from all chromosomes Ej Nchr j 1 are concatenated along the sequence dimension to form a unified feature sequence Ecombined. This concatenated sequence serves as the input to a Transformer encoder layer. The core of this layer is the Multi-Head SelfAttention mechanism which is computed as III. RESULTS AND DISCUSSION A. Performance comparison and analysis To evaluate the performance of our proposed model we investigated the application of DPCformer on five different datasets using the PCC of the test set as the prediction performance evaluation metric. The results were compared with other GS methods including DNNGP Light GBM Cropformer and GEFormer. As shown in Figure 3 DPCformer achieved excellent performance across all datasets. QK 1 Maize Dataset Performance DPCformer demonstrates superior performance across dk Attention Q K V softmax V 5 of 73.49 % for the Sopim BGV006775 12T001232 trait. This represents significant improvements of 11.45 % over DNNGP 9.13 % above Light GBM 57.35 % beyond Cropformer and 41.02 % superior to'}, {'rank': 4, 'score': 0.07265644329343962, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': 'three rotation rd parameters for each axis d x y z. attention map Amr the attention features are updated for both the reference and moving features as follows Aref Amr Vref Amov AT mr Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components Fig. 1 i the feature extractor ii the cross-attention module and iii the regression layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder here we adopt a U-Net encoder with fewer parameters to extract features. Specifically this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel with the number of feature channels set to 32 64 and 128 respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs which will be elaborated in the following section. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process. The DNF module includes a series of convolutional layers batch normalization'}, {'rank': 5, 'score': 0.0698451110790722, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'to infer patterns of the original image. The DA-Former module Figure 6c aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a onedimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations the output is unflattened back into the feature map. The refinement network is guided by the GAN loss which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module Figure 6d aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely the Dec-Net architecture sequentially comprises four residual blocks an up-sampling layer two residual blocks another up-sampling layer one residual block and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector'}]",The DNF module includes a series of convolutional layers batch normalization and Re LU activation to refine the feature integration process. To enhance the representation of aggregated information following the cross-attention phase we integrate a Deep Normalization and Fusion DNF module both prior to and after the concatenation process.
What regularization techniques help reduce overfitting in large language models?,2510.12850v1,,,"['2510.12850v1', '2510.12850v1', '2510.13137v1', '2509.23158v1', '2510.12850v1']","[0.1269238695018409, 0.08523084735506876, 0.074447234983625, 0.06458447026656505, 0.06280306762090669]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification']","[{'rank': 1, 'score': 0.1269238695018409, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model'}, {'rank': 2, 'score': 0.08523084735506876, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been'}, {'rank': 3, 'score': 0.074447234983625, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a'}, {'rank': 4, 'score': 0.06458447026656505, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research'}, {'rank': 5, 'score': 0.06280306762090669, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of'}]",Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting.
Which learning rate schedules are most effective when training deep learning models?,2510.12850v1,,,"['2510.12850v1', '2510.12850v1', '2510.08662v1', '2510.05736v1', '2510.12850v1']","[0.18765682433735847, 0.14660531256422205, 0.11653880419909748, 0.10239218054931531, 0.10126564544616432]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification']","[{'rank': 1, 'score': 0.18765682433735847, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas leading to more efficient optimization. In ethical classification tasks adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt η0 1 t 11 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training activations Htask in the classification head were stochastically zeroed out. In Equation 12 D Bernoulli 1 p is a dropout mask represents element-wise multiplication and p is the dropout rate. At inference time activations were scaled by 1 p to maintain consistent output expectations shown in Equation 13. Regularization techniques dropout and batch normalization help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios reducing biases and improving fairness. In ethical classification tasks regularization enhances robustness by making the model'}, {'rank': 2, 'score': 0.14660531256422205, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning it with ethical guidelines. This approach minimizes bias enhances reliability and ensures responsible decision-making. L Y L θi L Hj Hj 1 Hi HL θi i 1 2... L 8 j i+1 Gradient Accumulation To address memory constraints gradient accumulation was employed. Gradients g b for each mini-batch b were accumulated over Nacc steps. Equation 9 gradients L b from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulating gradients over multiple steps. The learning rate η scales the average accumulated gradient ensuring stable optimization. It ensures better representation of ethical considerations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning'}, {'rank': 3, 'score': 0.11653880419909748, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': 'unprecedented challenges to food conventional GS models often inadequately capture complex non-additive genetic effects which limits their prediction accuracy and robustness. Recently deep learning methods have demonstrated remarkable efficacy in data modeling across diverse scientific domains. Their capacity to automatically learn complex features enables the effective modeling of non-linear relationships between genotype and phenotype rendering them highly suitable for genomic prediction. Genomic prediction models employing Deep Neural Networks DNNs and Convolutional Neural Networks CNNs have shown promising results in crop breeding applications. For instance DNNGP leverages automatic feature extraction to enhance the analysis of high-dimensional genomic data. Similarly machine learning models such as the gradient boosting framework Crop GBM have proven effective for handling large-scale datasets. Additionally the GEFormer model incorporates genotypeenvironment interactions by utilizing both environmental and genetic data for phenotypic prediction. Furthermore Cropformer has demonstrated notable success in predicting maize heterosis by integrating CNNs with self-attention mechanisms. However significant challenges for existing deep learning models remain including limitations related to insufficient environmental data suboptimal prediction accuracy poor performance with small sample sizes and difficulties in processing polyploid genomic data. To address the limitations of traditional genomic selection this study introduces Deep Pheno Correlation Former DPCformer a novel deep learning framework Figure 1 that integrates a convolutional neural network CNN with a multihead self-attention mechanism to predict crop phenotypes'}, {'rank': 4, 'score': 0.10239218054931531, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': '50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies.'}, {'rank': 5, 'score': 0.10126564544616432, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been'}]",b 1 θL b 9 gacc θ t+1 θ t η gacc Nacc 10 9 Adaptive Learning Rate An adaptive learning rate schedule was used reducing the learning rate as training progressed. In Equation 11 η0 is the initial learning rate and t is the training step.
What problems might I encounter when fine-tuning models on domain-specific data?,2510.12850v1,,,"['2510.12850v1', '2510.12850v1', '2510.12850v1', '2510.09187v1', '2510.12850v1']","[0.24163809302242523, 0.16473975468394392, 0.13756907023269657, 0.11269348070647225, 0.10452769135687995]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification']","[{'rank': 1, 'score': 0.24163809302242523, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'between terms like Title IX and title ix or US and us preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity. By leveraging this approach we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model chosen for its strong contextual understanding and adaptability to text classification tasks. In the following we detail the architecture training process and fine-tuning innovations along with the mathematical formulations underpinning these methods illustrated in Fig. 2 Fine tuning of BERT for ethical reasoning θ t+1 θ t η ˆvt + ε ˆmt 7 Equation 7 ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model s adaptability to the ethical reasoning task we implemented the following innovations Full Fine-Tuning All layers of the BERT model were unfrozen allowing parameter adjustments across the entire network. From Equation 8 the loss gradient θL was backpropagated through all layers where L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. It refines the model s understanding beyond general pre-trained knowledge aligning'}, {'rank': 2, 'score': 0.16473975468394392, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection hate speech recognition offensive language identification sentiment analysis and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success this study applies pre-trained LLMs to ethical content classification aiming for a more reliable context-aware and fair moderation system. soning using machine learning techniques. It includes details about the dataset data preprocessing and implementation of our machine learning pipeline. Additionally we elaborate on the fine-tuning process showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words reducing vocabulary size and enhancing embedding stability. It splits words into subwords based on frequency preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words making training more efficient and robust. In this process the'}, {'rank': 3, 'score': 0.13756907023269657, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of ethical content detection Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets highlighting the model s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based architectures in capturing complex linguistic patterns and moral nuances making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models ability to reason about morality across different ethical frameworks including justice virtue ethics deontology utilitarianism and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy particularly on adversarial test cases highlighting their limitations in generalizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels requiring models to infer relative rankings rather than performing direct classification. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These'}, {'rank': 4, 'score': 0.11269348070647225, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'to provide richer temporal context. Crucially we employ aspect-ratio-preserving resizing followed by padding to prevent image distortion ensuring that the geometric integrity of the batting form is maintained. Custom CNN-GRU The baseline is a custom-designed 5-layer CNN paired with a GRU representing a standard deep learning approach for video classification. Dilated CNN-GRU To enhance the CNN s ability to capture spatial context without increasing the number of parameters this model replaces standard convolutions with dilated convolutions. This modification expands the receptive field of the convolutional filters allowing them to capture information from a larger area of the input frame. B. Model Architectures 1 Paradigm 1 Adaptation Study Kumar et al. This approach adapts the Long-term Recurrent Convolutional Network LRCN architecture originally proposed by Donahue et al. for the task of cricket shot classification. The model is designed for high computational efficiency. VGG16-GRU The most successful model from this paradigm utilizes transfer learning. A VGG16 network pretrained on the Image Net dataset serves as a powerful feature extractor. The authors experimented with different fine-tuning strategies 1 freezing the entire VGG16 backbone 2 fine-tuning only the final 4 layers and 3 fine-tuning the final 8 layers. This approach leverages the rich visual features learned from a large-scale dataset. Spatial Feature Extractor A lightweight 4-layer Convolutional Neural Network CNN processes each frame independently.'}, {'rank': 5, 'score': 0.10452769135687995, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}]",Fully fine-tuning in ethical classification tasks helps the model grasp domain-specific ethical nuances leading to more precise and fair decisions. Additionally the study relied on standard fine-tuning techniques but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training.
How can I assess image-text alignment in multimodal models?,2510.10729v1,,,"['2510.10729v1', '2510.12850v1', '2510.10729v1', '2510.12758v1', '2510.10729v1']","[0.07664635471461104, 0.06537740960067305, 0.05838252043381437, 0.0507046019666835, 0.0460141742771288]","['Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning']","[{'rank': 1, 'score': 0.07664635471461104, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'detection modules. Each module processes the text independently producing an output feature vector that encapsulates its specialized analysis. 3. Aggregation and Fusion Feature vectors from all modules are aggregated using concatenation and passed through dimensionality reduction e.g. PCA or attention-based fusion to form a composite representation. This vector is processed by the classification engine. 4. Classification Layer A meta-classifier trained on labeled sarcasm datasets takes the fused representation and outputs the probability of the text being sarcastic. Logistic regression or shallow feedforward neural networks are typically used at this stage. 5. Feedback Loop User feedback is captured through upvotes/downvotes or flags enabling continuous retraining and improving the model s precision over time. This adaptive learning loop enhances robustness to evolving sarcastic trends. Such a modular design supports horizontal scalability modules can be parallelized or independently optimized while ensuring maintainability and extensibility of the system. A conceptual case study evaluates our framework on a multimodal sarcasm dataset comprising text-image tweet pairs. Text is encoded via BERT while images are processed using Dense Net for visual sarcasm features such as facial expressions contextual image cues or meme-style exaggeration. This method ensures that both language and visual content contribute to the sarcasm prediction process. The dataset used in the study includes publicly available labeled tweets containing sarcastic hashtags such as sarcasm irony and'}, {'rank': 2, 'score': 0.06537740960067305, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech'}, {'rank': 3, 'score': 0.05838252043381437, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'across multidomain datasets. Their work emphasized the combination of spatial and sequential learning for improved accuracy. Razali et al. explored deep contextual embedding techniques highlighting the importance of semantic features and domain knowledge in sarcasm detection. Their model significantly enhanced performance by considering word context at the sentence level. Poria et al. presented a Deep CNN-based architecture for sarcastic tweet classification which captured local text patterns and contributed to detecting sarcastic undertones. Their research focused on leveraging spatial hierarchies within tweets. Liu et al. designed a multitask deep neural network for general language understanding tasks providing foundational models that indirectly benefit sarcasm detection through shared contextual learning across related tasks. Zhang et al. proposed a deep neural network approach specifically tailored to sarcasm detection in tweets incorporating embedding layers and convolutional filters to capture sarcastic phrases and sentence structures. Du et al. proposed an approach that combined sentimental context and individual expression patterns to detect sarcasm more reliably. Their study emphasized personalized modeling for better generalization. Bharti et al. extended the problem into the multimodal domain integrating image data alongside text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must'}, {'rank': 4, 'score': 0.0507046019666835, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': 'the Vicra gold-standard. Qualitatively reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation normal-ized error map from the Vicra gold-standard. Additionally we compute the Structural Similarity Index SSIM and Nor-malized Mean Squared Error NMSE for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study DL-HMC++-based recon-struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view as well as the parietal and frontal lobes in the coronal and sagittal views respectively. In addition DL-HMC++ exhibits the highest SSIM the lowest NMSE and 323 1.27 0.46 1.16 1.20 643 1.45 0.78 1.37 1.75 963 1.59 0.60 1.49 1.85 PET Cloud Size computing the similarity between both the moving and ref-erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self-attention in both translation and rotation. These results demon-strate that our approach boosts the model s MC performance by creating spatial correspondences between the moving and reference clouds. 3 Training Set Size We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20 40 60 80 and 100 subjects. As the number of subjects increases we observe a corresponding enhancement in the'}, {'rank': 5, 'score': 0.0460141742771288, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'contextual image cues or meme-style exaggeration. This method ensures that both language and visual content contribute to the sarcasm prediction process. The dataset used in the study includes publicly available labeled tweets containing sarcastic hashtags such as sarcasm irony and not. Each tweet was paired with its respective image and then preprocessed text was tokenized and embedded using BERT while images were resized and fed into a pre-trained Dense Net. Feature vectors from both models were concatenated and passed to a fusion classifier for final sarcasm prediction. Fig. 5. Model Accuracy Comparison on Twitter Dataset The results demonstrate clear advantages of multimodal learning. BERT alone achieved an accuracy of 88.6% Dense Net alone achieved 74.3% and the combined model reached 93.2%. These findings confirm that visual signals add significant value in identifying sarcasm especially when textual cues are ambiguous. In a practical application scenario this model could assist content moderators by automatically flagging sarcastic content thus enhancing sentiment analysis systems. Future studies could expand this work by exploring multilingual sarcastic tweets using emotion-labeled image datasets and integrating audio cues such as tone and inflection. User-centered evaluation through crowdsourcing platforms e.g. Amazon Mechanical Turk can further validate the model s predictions by comparing them with human judgments. This process enhances the credibility of sarcasm classification in real-world deployments. We propose a'}]",A conceptual case study evaluates our framework on a multimodal sarcasm dataset comprising text-image tweet pairs. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation.
What metrics should I use to evaluate text generation models?,2510.12850v1,,,"['2510.12850v1', '2510.10729v1', '2510.10729v1', '2510.13050v1', '2510.05736v1']","[0.13046911153387353, 0.05858472405168518, 0.052006503802506196, 0.04835348334144063, 0.04814027447439101]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes']","[{'rank': 1, 'score': 0.13046911153387353, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'space and skin tone detection achieved 95% accuracy in classifying explicit images and videos. Similarly Khandekar et al. focused on NLP techniques for detecting unethical and offensive text leveraging LSTM and Bi LSTM networks which outperformed traditional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection emphasizing algorithmic bias and lack of generalizability. Their analysis of 381 000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an Ethics by Design framework for abusive content detection highlighting fairness explainability and bias mitigation. Their two-step process categorized identity-related content before assessing severity reinforcing the importance of ethical considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech'}, {'rank': 2, 'score': 0.05858472405168518, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'detection modules. Each module processes the text independently producing an output feature vector that encapsulates its specialized analysis. 3. Aggregation and Fusion Feature vectors from all modules are aggregated using concatenation and passed through dimensionality reduction e.g. PCA or attention-based fusion to form a composite representation. This vector is processed by the classification engine. 4. Classification Layer A meta-classifier trained on labeled sarcasm datasets takes the fused representation and outputs the probability of the text being sarcastic. Logistic regression or shallow feedforward neural networks are typically used at this stage. 5. Feedback Loop User feedback is captured through upvotes/downvotes or flags enabling continuous retraining and improving the model s precision over time. This adaptive learning loop enhances robustness to evolving sarcastic trends. Such a modular design supports horizontal scalability modules can be parallelized or independently optimized while ensuring maintainability and extensibility of the system. A conceptual case study evaluates our framework on a multimodal sarcasm dataset comprising text-image tweet pairs. Text is encoded via BERT while images are processed using Dense Net for visual sarcasm features such as facial expressions contextual image cues or meme-style exaggeration. This method ensures that both language and visual content contribute to the sarcasm prediction process. The dataset used in the study includes publicly available labeled tweets containing sarcastic hashtags such as sarcasm irony and'}, {'rank': 3, 'score': 0.052006503802506196, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'should be able to understand why a specific piece of content was labeled as sarcastic. Techniques like LIME and SHAP help visualize feature importance and decision rationale. Moreover models should include feedback loops that allow users to flag misclassifications facilitating continual learning and correction. Privacy preservation is essential when scraping social media data for training. Proper anonymization and ethical approval must be obtained before model development. Finally regular auditing of models for bias drift and ethical compliance ensures that systems remain accountable and socially responsible throughout their lifecycle. VI. SYSTEM ARCHITECTURE Fig. 4. Proposed Modular System Architecture for Sarcasm Detection VIII. CASE STUDY The system architecture is structured into a layered pipeline starting with input ingestion and preprocessing followed by parallel feature extraction streams and a final aggregation layer. 1. Input Preprocessing Text data is collected from social media platforms and undergoes extensive cleaning removal of special characters hashtags emojis links and user handles. The cleaned text is tokenized and passed through lemmatization for standardization. 2. Feature Extraction Modules The input is fed simultaneously into sentiment contextual linguistic and emotion detection modules. Each module processes the text independently producing an output feature vector that encapsulates its specialized analysis. 3. Aggregation and Fusion Feature vectors from all modules are aggregated using concatenation and passed through dimensionality reduction e.g. PCA or attention-based'}, {'rank': 4, 'score': 0.04835348334144063, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'agencies that maintain them. We also use equivalent older satellites for model training where available. We use the satpy library to do the parsing and reprojecting of the raw data into 18 mosaics at varying wavelengths from 0.47 μm to 13.3 μm. Supplement A.1.3 provides more details on each band in the mosaic. Mosaic Time Resolution and Latency We make mosaics at 30 minute intervals. This is the lowest common multiple for any dataset that contains Meteosat Second Generation satellites. Our data delivery delays are about half an hour and our processing time is under half an hour. This means our realtime mosaics lag actual real time by about an hour in total. We use level 1b calibrated data for our mosaics. This gets all the data in reflectance units for the visual bands and brightness temperature Kelvin for the IR bands. We also use a Gaussian center weighted blended average to blend the data together. This avoids any artifacts at the boundaries of the different satellite coverage areas. We try to blend each mosaic at the highest resolution available for any input to the given mosaic but we cap resolutions to 1km nominal pixel size for runtime reasons. Table 3 Geostationary satellites used for creating mosaics. Satellite Name Region Covered Agency Meteosat-11 Europe/North Africa EUMETSAT Meteosat-9 Indian Ocean EUMETSAT'}, {'rank': 5, 'score': 0.04814027447439101, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': '50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies.'}]",examined the moral biases embedded in large pre-trained models like BERT demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a moral direction within the embedding space which could be used to rate the normativity of text.
How can we detect sarcasm using deep learning?,2510.10729v1,,,"['2510.10729v1', '2510.10729v1', '2510.10729v1', '2510.10729v1', '2510.10729v1']","[0.22758892359052502, 0.2136131312251521, 0.16201755086460928, 0.13170408200865957, 0.1298920301159982]","['Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning']","[{'rank': 1, 'score': 0.22758892359052502, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must infer sarcasm from textual patterns alone. This paper explores a conceptual solution using DCNNs combined with contextual embedding models to understand sarcasm s complex indicators such as irony sentiment contradiction and hyperbole. Applications range from content moderation on social platforms to enhancing virtual assistant interactions. Sarcasm is a complex form of communication that relies heavily on tone context and cultural cues. Humans often detect sarcasm by recognizing exaggerated language contradictions or situational irony which are difficult for machines to grasp. Traditional text processing tools typically fail to interpret such nuanced expressions. With the emergence of deep learning models like CNNs and transformers it has become feasible to explore sarcasm detection using machine learning. This paper proposes a deep neural architecture that mimics this human-like understanding by analyzing multiple model achieved superior performance on social media sarcasm datasets. Sharma et al. developed a hybrid autoencoder model capable of capturing both shallow and deep semantic patterns contributing to accurate sarcasm classification in social media posts. Eke et al. focused on context-based feature engineering using BERT proposing a system that leverages sentence-level embeddings'}, {'rank': 2, 'score': 0.2136131312251521, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through a multi-layer architecture. Although the model is not yet implemented the design demonstrates feasibility for real-world applications like chatbots and social media monitoring. Additional discussions on data preprocessing techniques model evaluation strategies and ethical implications further contextualize the approach. Index Terms Sarcasm Detection Deep Learning Convolutional Neural Networks Natural Language Processing BERT Multimodal Learning Text Analysis Emotion Detection. Sarcasm detection has been an active area of research due to its implications in sentiment analysis and opinion mining. Several studies have proposed diverse approaches ranging from rule-based systems to deep learning architectures. Jamil et al. introduced a hybrid model using Convolutional Neural Networks CNNs and Long Short-Term Memory LSTM networks to detect sarcasm across multidomain datasets. Their work emphasized the combination of spatial and sequential learning for improved accuracy. Razali et al. explored deep contextual embedding techniques highlighting the importance of semantic features and domain knowledge in sarcasm detection. Their model significantly enhanced'}, {'rank': 3, 'score': 0.16201755086460928, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'across multidomain datasets. Their work emphasized the combination of spatial and sequential learning for improved accuracy. Razali et al. explored deep contextual embedding techniques highlighting the importance of semantic features and domain knowledge in sarcasm detection. Their model significantly enhanced performance by considering word context at the sentence level. Poria et al. presented a Deep CNN-based architecture for sarcastic tweet classification which captured local text patterns and contributed to detecting sarcastic undertones. Their research focused on leveraging spatial hierarchies within tweets. Liu et al. designed a multitask deep neural network for general language understanding tasks providing foundational models that indirectly benefit sarcasm detection through shared contextual learning across related tasks. Zhang et al. proposed a deep neural network approach specifically tailored to sarcasm detection in tweets incorporating embedding layers and convolutional filters to capture sarcastic phrases and sentence structures. Du et al. proposed an approach that combined sentimental context and individual expression patterns to detect sarcasm more reliably. Their study emphasized personalized modeling for better generalization. Bharti et al. extended the problem into the multimodal domain integrating image data alongside text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must'}, {'rank': 4, 'score': 0.13170408200865957, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'as tone and inflection. User-centered evaluation through crowdsourcing platforms e.g. Amazon Mechanical Turk can further validate the model s predictions by comparing them with human judgments. This process enhances the credibility of sarcasm classification in real-world deployments. We propose a conceptual sarcasm detection framework integrating sentiment, emotion, context, and linguistic cues through deep learning. The modular nature of the architecture enables flexibility in upgrading or replacing individual detection modules, making the system highly extensible and suitable for varied use cases. The integration of multiple feature domains ensures a holistic understanding of sarcasm, improving interpretability and robustness. This research highlights the potential of combining advanced NLP techniques with deep learning models to enhance automated language understanding, particularly in nuanced areas like sarcasm. Our approach leverages pretrained models like BERT for deep contextual embeddings, complemented by handcrafted linguistic rules and emotion analysis. The preliminary design showcases promising capabilities and adaptability across domains. Future work involves implementing the full pipeline and conducting large-scale experiments across multiple datasets, including multilingual corpora. Real-time testing scenarios, such as integration with chatbots, virtual assistants, or sentiment analysis systems, are intended to validate the model s practical effectiveness. We also aim to introduce adversarial training to make the model resilient against input manipulations and sarcasm obfuscation techniques. Further improvements include enhancing multimodal detection by incorporating audio and video'}, {'rank': 5, 'score': 0.1298920301159982, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'developed a hybrid autoencoder model capable of capturing both shallow and deep semantic patterns contributing to accurate sarcasm classification in social media posts. Eke et al. focused on context-based feature engineering using BERT proposing a system that leverages sentence-level embeddings and fine-tuned transformers to boost classification accuracy. These prior works demonstrate the effectiveness of deep learning methods in sarcasm detection. However many lack scalability explainability or modularity. Our work builds upon these foundations proposing a modular framework that unifies sentiment context linguistic cues and emotion analysis into an adaptable and extensible system for robust sarcasm detection. Fig. 2. Architecture of a Convolutional Neural Network IV. A. Neural Networks A Neural Network is a machine learning model inspired by the human brain consisting of interconnected neurons arranged in layers. Each layer processes input data through weighted connections and activation functions to learn patterns and make decisions. Fig. 3. Structure of a Deep Convolutional Neural Network V. PROPOSED METHODOLOGY The proposed sarcasm detection system is designed using a modular architecture composed of four specialized detection modules that collaboratively interpret different linguistic signals. These include Sentiment Analysis This module employs VADER or BERT-based sentiment analysis models to capture the emotional polarity of a sentence. Sarcasm often involves polarity flips where positive sentiment is expressed with a negative undertone or vice versa. VADER with'}]",With the emergence of deep learning models like CNNs and transformers it has become feasible to explore sarcasm detection using machine learning. proposed a deep neural network approach specifically tailored to sarcasm detection in tweets incorporating embedding layers and convolutional filters to capture sarcastic phrases and sentence structures.
How can I detect or measure bias in deep learning models?,2510.10822v1,,,"['2510.10822v1', '2510.05736v1', '2510.08662v1', '2510.10822v1', '2510.12850v1']","[0.1655316360762327, 0.10020763919996128, 0.08808662581245524, 0.08525982191227614, 0.08465606638404019]","['From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification']","[{'rank': 1, 'score': 0.1655316360762327, 'id': '2510.10822v1', 'title': 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'text': 'From Detection to Mitigation Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis. Deep learning models have shown promise in improving diagnostic accuracy from chest Xrays but they also risk perpetuating healthcare disparities when performance varies across demographic groups. In this work we present a comprehensive bias detection and mitigation framework targeting sex age and race-based disparities when performing diagnostic tasks with chest X-rays. We extend a recent CNN XGBoost pipeline to support multi-label classification and evaluate its performance across four medical conditions. We show that replacing the final layer of CNN with an e Xtreme Gradient Boosting classifier improves the fairness of the subgroup while maintaining or improving the overall predictive performance. To validate its generalizability we apply the method to different backbones namely Dense Net-121 and Res Net-50 and achieve similarly strong performance and fairness outcomes confirming its model-agnostic design. We further compare this lightweight adapter training method with traditional full-model training bias mitigation techniques including adversarial training reweighting data augmentation and active learning and find that our approach offers competitive or superior bias reduction at a fraction of the computational cost. Finally we show that combining e Xtreme Gradient Boosting retraining with active learning yields the largest reduction in bias across all demographic subgroups both in and out of distribution on the Che Xpert and MIMIC'}, {'rank': 2, 'score': 0.10020763919996128, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': '50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies.'}, {'rank': 3, 'score': 0.08808662581245524, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': 'unprecedented challenges to food conventional GS models often inadequately capture complex non-additive genetic effects which limits their prediction accuracy and robustness. Recently deep learning methods have demonstrated remarkable efficacy in data modeling across diverse scientific domains. Their capacity to automatically learn complex features enables the effective modeling of non-linear relationships between genotype and phenotype rendering them highly suitable for genomic prediction. Genomic prediction models employing Deep Neural Networks DNNs and Convolutional Neural Networks CNNs have shown promising results in crop breeding applications. For instance DNNGP leverages automatic feature extraction to enhance the analysis of high-dimensional genomic data. Similarly machine learning models such as the gradient boosting framework Crop GBM have proven effective for handling large-scale datasets. Additionally the GEFormer model incorporates genotypeenvironment interactions by utilizing both environmental and genetic data for phenotypic prediction. Furthermore Cropformer has demonstrated notable success in predicting maize heterosis by integrating CNNs with self-attention mechanisms. However significant challenges for existing deep learning models remain including limitations related to insufficient environmental data suboptimal prediction accuracy poor performance with small sample sizes and difficulties in processing polyploid genomic data. To address the limitations of traditional genomic selection this study introduces Deep Pheno Correlation Former DPCformer a novel deep learning framework Figure 1 that integrates a convolutional neural network CNN with a multihead self-attention mechanism to predict crop phenotypes'}, {'rank': 4, 'score': 0.08525982191227614, 'id': '2510.10822v1', 'title': 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'text': 'cases mitigating bias may unintentionally disadvantage specific subgroups. To measure overall performance we use the Area Under the Precision-Recall Curve AUPRC which provides a balanced assessment of precision and recall and is particularly suited for imbalanced datasets. To evaluate fairness we compute the performance disparity across subgroups using AUPRC the absolute difference in AUPRC between subgroups. In cases with more than two subgroups such as race we report the maximum observed AUPRC as the fairness metric. In our framework the goal is to achieve high AUPRC strong overall performance and low AUPRC minimal disparity across subgroups. 3.3. Bias Detection Framework Before mitigating the bias it is essential to detect and understand its sources. Model bias can come from various factors including data composition clinical context and the algorithm itself. In this analysis we focus on identifying potential sources of bias on Che Xpert. 1 Data and Clinical Context We study disparities introduced during data collection. For each disease and demographic subgroup we analyze the distribution of positive and negative labels to assess imbalances. Additionally we study differences in disease prevalence across demographic groups to understand potential confounding clinical patterns. 2 Model Behavior To investigate the model s internal representations we visualize learned embeddings using Principal Component Analysis PCA and t-distributed Stochastic Neighbor Embedding t-SNE 20 plots stratified by demographic group.'}, {'rank': 5, 'score': 0.08465606638404019, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been'}]",From Detection to Mitigation Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis. Deep learning models have shown promise in improving diagnostic accuracy from chest Xrays but they also risk perpetuating healthcare disparities when performance varies across demographic groups.
How can I reduce hallucinations in large language models?,2510.12850v1,,,"['2510.12850v1', '2509.23158v1', '2510.12850v1', '2510.13137v1', '2510.13137v1']","[0.09955296861691841, 0.0910344978663533, 0.08852353672235123, 0.06275836193629802, 0.06253846152848407]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN']","[{'rank': 1, 'score': 0.09955296861691841, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been'}, {'rank': 2, 'score': 0.0910344978663533, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research'}, {'rank': 3, 'score': 0.08852353672235123, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of'}, {'rank': 4, 'score': 0.06275836193629802, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': '2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is'}, {'rank': 5, 'score': 0.06253846152848407, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big'}]",review advancements in sentiment analysis highlighting machine learning deep learning and large language models. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series.
What techniques improve Transformer training speed without losing performance?,2510.11073v1,,,"['2510.11073v1', '2510.09187v1', '2510.12850v1', '2509.23158v1', '2510.12850v1']","[0.07354348231704087, 0.07002978600442081, 0.06958342287839536, 0.05116340248945674, 0.05062033407590502]","['ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification']","[{'rank': 1, 'score': 0.07354348231704087, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector drawn from the Gaussian distribution with unit variance and is pre-pended before the flattened feature. Then the combined features are passed through six Transformer blocks which leverage self-attention mechanism to transform the bio-identifying information within feature effectively protecting the privacy. Finally the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12 the ID protection rate increases with the number of Transformer blocks from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement 96.61% while significantly increasing computational cost and model size. 10 Therefore we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7 the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection the facial features are largely'}, {'rank': 2, 'score': 0.07002978600442081, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'Sen et al. This work uses a shorter sequence length of 15 frames and introduces random frame sampling as a data augmentation technique. This can improve model generalization by introducing variability during training but it may also fail to capture the full temporal evolution of a shot. The resolution is higher at 180×224 or 224× 224 depending on the specific model. Vision Transformer Hybrid This hybrid model combines a Vision Transformer Vi T with an RNN. A Vi T with a 6-layer Transformer encoder processes each frame s 20x20 patches for spatial feature extraction. The resulting sequence of frame embeddings is then passed to a GRU to model their temporal relationships. 3 Paradigm 3 Progressive Enhancement Sen et al. This suite of models demonstrates a systematic progressive enhancement strategy starting with a custom architecture and moving towards transfer learning. Proposed Method Our approach is aligned with modern best practices for video analysis. We use a 224 × 224 resolution which is standard for many pretrained Image Net models like Efficient Net. A longer sequence of 30 frames is sampled uniformly to provide richer temporal context. Crucially we employ aspect-ratio-preserving resizing followed by padding to prevent image distortion ensuring that the geometric integrity of the batting form is maintained. Custom CNN-GRU The baseline is a custom-designed 5-layer CNN paired with a'}, {'rank': 3, 'score': 0.06958342287839536, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense and Deontology domains especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples incorporating external knowledge sources or adopting domain-specific pretraining strategies. In general this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model.'}, {'rank': 4, 'score': 0.05116340248945674, 'id': '2509.23158v1', 'title': 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization', 'text': 'scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small'}, {'rank': 5, 'score': 0.05062033407590502, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'in these domains. The AUC values for domains 90.78 87.36 88.78 89.93 further affirm the model s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a 4b 4c and 4d correspond to the Common Sense Justice Virtue and Deontology frameworks respectively highlighting differences in model performance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs highlighting key trends. Training loss consistently decreases while accuracy improves indicating effective learning. Some models maintain stable validation accuracy suggesting good generalization. In some cases training and validation loss patterns differ which may indicate areas for refinement. Adjustments like regularization could improve performance. Overall the models demonstrate effective learning with some showing stronger generalization. a Common Sense b Justice c Virtue 14 d Deontology Fig. 5 Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split the model showcased its robustness achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations including comprehensive fine-tuning by unfreezing all layers of the BERT model implementing gradient accumulation and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations resulting in superior performance. Despite these successes challenges persist in the Commonsense'}]",This can improve model generalization by introducing variability during training but it may also fail to capture the full temporal evolution of a shot. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model..
Are deep learning methods effective for crime forecasting compared to traditional models?,2509.20913v1,2509.20913v1,True,"['2509.20913v1', '2509.20913v1', '2509.20913v1', '2509.20913v1', '2509.20913v1']","[0.21770376063076619, 0.16790683236562415, 0.1487895493781732, 0.1379040802228534, 0.11208998747634998]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales']","[{'rank': 1, 'score': 0.21770376063076619, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic'}, {'rank': 2, 'score': 0.16790683236562415, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions'}, {'rank': 3, 'score': 0.1487895493781732, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing'}, {'rank': 4, 'score': 0.1379040802228534, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and'}, {'rank': 5, 'score': 0.11208998747634998, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to'}]",Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data.
"Should I train separate models for different crime types, or combine them?",2509.20913v1,2509.20913v1,True,"['2509.20913v1', '2509.20913v1', '2509.20913v1', '2509.20913v1', '2509.20913v1']","[0.1546117858894527, 0.09897484496603187, 0.09419077885808616, 0.08541822749748268, 0.08518153642495711]","['Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales']","[{'rank': 1, 'score': 0.1546117858894527, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using'}, {'rank': 2, 'score': 0.09897484496603187, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences.'}, {'rank': 3, 'score': 0.09419077885808616, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average'}, {'rank': 4, 'score': 0.08541822749748268, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing'}, {'rank': 5, 'score': 0.08518153642495711, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design.'}]",Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones.
Which deep learning approaches work well for gamma/hadron separation?,2510.05736v1,2510.05736v1,True,"['2510.05736v1', '2510.05736v1', '2510.09187v1', '2510.05736v1', '2510.13937v1']","[0.17902569547104333, 0.14310429185569953, 0.09300877326900739, 0.07544475603486292, 0.07065441772765707]","['Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach']","[{'rank': 1, 'score': 0.17902569547104333, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': '50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies.'}, {'rank': 2, 'score': 0.14310429185569953, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes. The identification of γ-rays from the predominant hadronic-background is a key aspect in their ground-based detection using Imaging Atmospheric Cherenkov Telescopes IACTs. While current methods are limited in their ability to exploit correlations in complex data deep learning-based models offer a promising alternative by directly leveraging image-level information. However several challenges involving the robustness and applicability of such models remain. Designing model architectures with inductive biases relevant for the task can help mitigate the problem. Three such deep learning-based models are proposed trained and evaluated on simulated data 1 a hybrid convolutional and graph neural network model CNN-GNN using both image and graph data 2 an enhanced CNN-GNN variant that incorporates additional reconstructed information within the graph construction and 3 a graph neural network GNN model using image moments serving as a baseline. The new combined convolution and graph-based approach demonstrates improved performance over traditional methods and the inclusion of reconstructed information offers further potential in generalization capabilities on real observational data. The field of very-high-energy VHE γ-ray astronomy has evolved significantly over the past three decades driven largely due to observations from ground-based imaging telescopes. These Imaging Atmospheric Cherenkov Telescopes IACTs capture Cherenkov light produced by highly energetic particles interacting with the Earth s atmosphere.'}, {'rank': 3, 'score': 0.09300877326900739, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study. Cricket shot classification from video sequences remains a challenging problem in sports video analysis requiring effective modeling of both spatial and temporal features. This paper presents the first comprehensive baseline study comparing seven different deep learning approaches across four distinct research paradigms for cricket shot classification. We implement and systematically evaluate traditional CNN-LSTM architectures attention-based models vision transformers transfer learning approaches and modern Efficient Net-GRU combinations on a unified benchmark. A critical finding of our study is the significant performance gap between claims in academic literature and practical implementation results. While previous papers reported accuracies of 96% Balaji LRCN 99.2% IJERCSE and 93% Sensors our standardized reimplementations achieve 46.0% 55.6% and 57.7% respectively. Our modern SOTA approach combining Efficient Net-B0 with a GRU-based temporal model achieves 92.25% accuracy demonstrating that substantial improvements are possible with modern architectures and systematic optimization. All implementations follow modern MLOps practices with Py Torch Lightning providing a reproducible research platform that exposes the critical importance of standardized evaluation protocols in sports video analysis research. While sports video analysis has seen substantial progress with deep learning cricket shot classification remains underexplored compared to other sports like soccer or basketball. Existing approaches often focus on single architectural paradigms without systematic comparison use limited datasets or'}, {'rank': 4, 'score': 0.07544475603486292, 'id': '2510.05736v1', 'title': 'Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes', 'text': 'representations. For example translational invariance in CNNs temporal dependence in RNNs and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to thus contributing to their success in their domains. In the context of IACTs as each image is a projection of the same event there is no inherent ordering between them making them permutation equivariant. This motivates a shift towards exploring GNNs for aggregating information across multiple telescopes. To that end this work introduces a combined convolutional and graph neural network CNN-GNN based approach for γ/hadron separation in IACTs. Three models with two distinct training strategies are proposed and evaluated on simulated data. These include a CNN-GNN model trained on image and graph data an enhanced CNN-GNN variant with additional reconstructed event information incorporated into the graph structure and a baseline GNN model utilizing image moments serving as a reference for existing methods. The models were trained on simulations of the High Energy Stereoscopic System H.E.S.S. located in Go llschau Namibia. The H.E.S.S. array consists of four 12-meter telescopes CT1-4 arranged in a square of side 120 meters with an additional 28-meter telescope CT5 at its center. Diffuse proton and γ-ray events simulated at a zenith of 20 and with a maximum view cone of 5 were'}, {'rank': 5, 'score': 0.07065441772765707, 'id': '2510.13937v1', 'title': 'Rock Classification through Knowledge-Enhanced Deep Learning: A Hybrid Mineral-Based Approach', 'text': 'automated identification of minerals in geological samples significantly enhancing the efficiency and accuracy of mineralogical studies. The development of spectral databases has been crucial in advancing mineral identification through Raman spectroscopy as it has enabled the creation of standardized reference spectra for thousands of minerals. The RRUFF project can be regarded as one of the cornerstones of this domain providing quality-controlled data detailed crystallographic information and documentation of sample origins and conditions. This standardized repository has been used for various applications from portable gemstone identification systems to machine learning and deep learning approaches. Recent developments have further expanded its utility through high-throughput computational methods and open-source analysis tools. Progress in machine learning has transformed the analysis of Raman spectrum data. Qi et al. provide a comprehensive review of these advances documenting the progression from traditional statistical methods to more sophisticated deep learning approaches. Among these methods 1D-CNN have emerged as particularly effective architectures for spectral data analysis demonstrating excellent performance across various spectroscopic applications including chemical substance identification molecular structure analysis and material characterization. The practical application of automated Raman analysis extends to various industrial settings particularly in sustainable resource management. Resch et al. demonstrated in their study of tunnel excavation materials that the proper characterization and classification of excavated materials could allow their recycling as high-value raw Iye Szin'}]","Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach."
What frameworks and optimization strategies were used to train DPCformer?,2510.08770v1,2510.08662v1,False,"['2510.08770v1', '2510.08770v1', '2510.08662v1', '2510.08116v1', '2510.08662v1']","[0.07870371141107305, 0.05906415092977438, 0.053014247116383514, 0.051818256106217986, 0.05076749326395671]","['Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops']","[{'rank': 1, 'score': 0.07870371141107305, 'id': '2510.08770v1', 'title': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'text': 'test set and real-time input. The same hardware was used to train the model as was used to collect data A Lenovo Legion Pro 7i with NVIDIA RTX 4080. Data used and real-time testing area were homogeneous across room and liquid types for this comparison For the training strategy the last 5 layers were fine-tuned an RMSprop optimizer was used lr 1e-5 a binary crossentropy loss function was used and early stopping was used with patience 5. A batch size of 8 was used for training and validation while a batch size of 2 was used for test. The model was trained with up to 50 epochs however this was never reached due to the application of the early stopping mechanism. For data augmentation images were randomly flipped horizontally slightly rotated and had small variations in contrast during training The random rotation and contrast had factor 0.01. The standard VGG preprocessing pipeline was then used. The results are found in Table I. Testing The results show that Combined although accurate on the test dataset is not effective in real-time testing. RGB although effective in real-time testing and near-perfect in the test set is slower for inferences and has a model size three times that of thermal. Thermal performs with 100% accuracy on the test set and real-time testing along with'}, {'rank': 2, 'score': 0.05906415092977438, 'id': '2510.08770v1', 'title': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'text': 'yellow juice were used and two rooms were used in data collection Atrium and J234. This resulted in 8 combinations Room x Liquid x Modality. A variety of spill sizes were collected forming a diverse spill dataset. Within these sizes a typical small spill would have a diameter of 2-4 inches with the regions being approximately circular. A typical large spill would have a diameter of up to 12 inches assuming an approximately circular region. Over time these large spill regions deformed as the liquid flowed across the floor which led to an increase in certain dimensions. The atrium had porcelain tiled floors while J234 had polished concrete flooring. Foot traffic was not included in pictures in the dataset. In the atrium lighting conditions were kept consistent while lighting conditions in J234 were dynamic due to the high presence of natural lighting and variability of sunlight. Training In order to determine whether RGB thermal or combined images would yield the best accuracy and efficiency a VGG19 model was trained on each type and the resulting model was tested on a test set and real-time input. The same hardware was used to train the model as was used to collect data A Lenovo Legion Pro 7i with NVIDIA RTX 4080. Data used and real-time testing area were homogeneous across room and'}, {'rank': 3, 'score': 0.053014247116383514, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': 'DTT 68.93% PH and 91.41% EW outperforming the second-best methods with improvements of 2.92% against Light GBM 0.74% over Cropformer and 1.10% beyond GEFormer Fig. 3a. Similarly in Beijing DPCformer reaches 93.50% DTT 76.24% PH and 93.01% EW exceeding GEFormer s DTT and PH by 1.48% and 2.40% respectively while surpassing Cropformer s EW by 19.25%. 2 Rice Small-Sample Dataset In resource-limited samples DPCformer demonstrated exceptional capability. For the rice dataset comprising only 530 samples it achieved a PCC of 84.45% for the plant height trait surpassing GEFormer by 38.98% DNNGP by 7.94% Cropformer by 3.56% and LightGBM by 5.53% Fig. 3a. 3 Cotton Dataset DPCformer significantly outperforms baseline models with accuracy rates of 74.19% FE 71.45% FL 74.85% FS and 72.30% FM surpassing Cropformer 65.97% Light GBM 71.03% Cropformer 71.43% and DNNGP 63.93% by 8.22% 0.42% 3.42% and 8.37% respectively Figure 3b. 4 Tomato Small-Sample Dataset With only 332 accessions DPCformer attained a PCC C. Interpretability Analysis of Machine Learning Models Using SHAP Values To interpret the model s predictions SHAP values were calculated for the SNPs in the best-performing model to identify the most influential loci. These top-ranking SNPs were then mapped to their corresponding genes.This analysis revealed several candidate genes associated with TABLE I RESULTS OF ABLATION EXPERIMENTS 8-Dim Encoding Position Sort PMF PCC 0.8376 0.8816 0.8668'}, {'rank': 4, 'score': 0.051818256106217986, 'id': '2510.08116v1', 'title': 'Random Window Augmentations for Deep Learning Robustness in CT and Liver Tumor Segmentation', 'text': 'difficult cases easier for the model resulting in increased robustness. Contrary to traditional intensity augmentations applied to preprocessed HU values our method does not introduce artifacts from shifting and scaling pre-clipped HU. We show that our proposed approach for CT augmentation improves robustness and generalization across multiple architectures and datasets and for liver tumor segmentation in both the 2D and 3D settings. Additionally we show that some of the traditional augmentation schemes not respecting the unit of intensity in the CT modality in fact can hurt performance in certain settings. D. CT-specific augmentations CT-specific intensity augmentations have in common that they leverage the HU distribution more cautiously. Clusterwise voxel intensity range shift applies additional predefined region-specific or manufacturer-specific viewing windows after initial windowing to further focus the model on specific parts of the CT distribution. Similar strategies that sample between predefined and task-specific viewing windows have been proposed independently as augmentation strategy or as a training technique multiple times since generated samples from three predefined viewing windows and used them to train a COVID classifier from lung CT images. Similarly used images preprocessed with four predefined viewing windows as augmentation for liver vessel segmentation and found it to be favorable over geometric transformation such as flipping and mirroring. investigated the effect of using various predefined viewing windows in training and'}, {'rank': 5, 'score': 0.05076749326395671, 'id': '2510.08662v1', 'title': 'DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops', 'text': '1D convolutional layers Conv1D and Re LU activations. The Conv1×1 term denotes a shortcut connection for dimensionality matching and BN is Batch Normalization. This module transforms the raw sequence Xj of each chromosome into a high-level feature map Ej. 2 Cross-Chromosome Information Fusion To model long-range dependencies and potential epistatic effects between different chromosomes the feature maps from all chromosomes Ej Nchr j 1 are concatenated along the sequence dimension to form a unified feature sequence Ecombined. This concatenated sequence serves as the input to a Transformer encoder layer. The core of this layer is the Multi-Head SelfAttention mechanism which is computed as III. RESULTS AND DISCUSSION A. Performance comparison and analysis To evaluate the performance of our proposed model we investigated the application of DPCformer on five different datasets using the PCC of the test set as the prediction performance evaluation metric. The results were compared with other GS methods including DNNGP Light GBM Cropformer and GEFormer. As shown in Figure 3 DPCformer achieved excellent performance across all datasets. QK 1 Maize Dataset Performance DPCformer demonstrates superior performance across dk Attention Q K V softmax V 5 of 73.49 % for the Sopim BGV006775 12T001232 trait. This represents significant improvements of 11.45 % over DNNGP 9.13 % above Light GBM 57.35 % beyond Cropformer and 41.02 % superior to'}]",Data used and real-time testing area were homogeneous across room and liquid types for this comparison For the training strategy the last 5 layers were fine-tuned an RMSprop optimizer was used lr 1e-5 a binary crossentropy loss function was used and early stopping was used with patience 5. For data augmentation images were randomly flipped horizontally slightly rotated and had small variations in contrast during training The random rotation and contrast had factor 0.01.
How do modern architectures perform on complex video tasks compared to older methods?,2510.09187v1,2510.09187v1,True,"['2510.09187v1', '2510.09187v1', '2510.09187v1', '2510.09187v1', '2509.20913v1']","[0.12703450611654166, 0.1256092188073936, 0.10925080275181825, 0.09664474637447318, 0.047144407691455587]","['Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales']","[{'rank': 1, 'score': 0.12703450611654166, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study. Cricket shot classification from video sequences remains a challenging problem in sports video analysis requiring effective modeling of both spatial and temporal features. This paper presents the first comprehensive baseline study comparing seven different deep learning approaches across four distinct research paradigms for cricket shot classification. We implement and systematically evaluate traditional CNN-LSTM architectures attention-based models vision transformers transfer learning approaches and modern Efficient Net-GRU combinations on a unified benchmark. A critical finding of our study is the significant performance gap between claims in academic literature and practical implementation results. While previous papers reported accuracies of 96% Balaji LRCN 99.2% IJERCSE and 93% Sensors our standardized reimplementations achieve 46.0% 55.6% and 57.7% respectively. Our modern SOTA approach combining Efficient Net-B0 with a GRU-based temporal model achieves 92.25% accuracy demonstrating that substantial improvements are possible with modern architectures and systematic optimization. All implementations follow modern MLOps practices with Py Torch Lightning providing a reproducible research platform that exposes the critical importance of standardized evaluation protocols in sports video analysis research. While sports video analysis has seen substantial progress with deep learning cricket shot classification remains underexplored compared to other sports like soccer or basketball. Existing approaches often focus on single architectural paradigms without systematic comparison use limited datasets or'}, {'rank': 2, 'score': 0.1256092188073936, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'and the Performance Gap Total 1 320 284 284 Our results strongly suggest that modern architectures when properly optimized are superior for complex video tasks. The SOTA model s success is attributable to the powerful spatial features from Efficient Net and the GRU s ability to effectively model temporal dependencies further enhanced by a temporal attention mechanism. The performance gap between our results and those reported in prior work is a critical finding. This discrepancy is likely due to differences in dataset splits evaluation code or minor implementation details not specified in the papers. It underscores the necessity of open-sourcing code and using standardized benchmarks to ensure the field progresses on solid footing. Our work provides this necessary baseline. B. Evaluation Metrics Accuracy and weighted Precision Recall and F1-Score are used to evaluate model performance. C. Implementation Details All models were implemented strictly following the details described in the respective papers. Training and evaluation were performed on a single NVIDIA A100 GPU 40GB memory. This paper presented the first comprehensive unified baseline study for cricket shot classification re-implementing and evaluating seven models across four distinct research paradigms. Our work makes three primary contributions. First we introduce a novel state-of-the-art model combining Efficient Net and a GRU-based temporal model which achieves a new benchmark accuracy of 92.25% on the Cric Shot10'}, {'rank': 3, 'score': 0.10925080275181825, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'over traditional approaches. 2 The Performance Gap There is a stark contrast between the accuracies reported in the original Bhat et al. 99.2% and Sen et al. 93% papers and the results from our re-implementation 55.6% and 57.7% respectively. This highlights the critical importance of standardized open-source benchmarking. 3 Architectural Insights The Sensors dilated CNN-GRU approach proves most effective among baseline methods 57.7% while attention-based and vision transformer approaches show surprisingly poor performance 40.5% and 10.6% respectively under standardized evaluation conditions. Data Splits We employ a stratified split using a fixed random seed 27 to ensure consistent class distribution across sets and reproducibility. The dataset is divided into 70% for training 1 320 samples 15% for validation 284 samples and 15% for testing 284 samples. This stratified approach maintains balanced representation of all 10 shot classes across each split. TABLE II DETAILED DATASET SPLIT DISTRIBUTION BY CLASS Shot Class Training Set Validation Set Test Set Cover 131 29 134 29 127 27 127 27 127 27 138 30 125 27 140 30 135 29 136 29 29 A. Architectural Insights and the Performance Gap Total 1 320 284 284 Our results strongly suggest that modern architectures when properly optimized are superior for complex video tasks. The SOTA model s success is attributable to the powerful spatial features from Efficient Net'}, {'rank': 4, 'score': 0.09664474637447318, 'id': '2510.09187v1', 'title': 'Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study', 'text': 'research. While sports video analysis has seen substantial progress with deep learning cricket shot classification remains underexplored compared to other sports like soccer or basketball. Existing approaches often focus on single architectural paradigms without systematic comparison use limited datasets or lack production-ready implementations. Furthermore the rapid evolution of deep learning architectures from CNN-LSTM combinations to modern transformer-based approaches has created a need for comprehensive baseline studies that fairly compare different methodological paradigms. This paper makes the following key contributions 1 Comprehensive Baseline Study with Open-Source Implementation We implement systematically evaluate and release seven different approaches across four research paradigms providing both rigorous experimental comparison and a complete framework for future cricket video analysis research. 2 Performance Gap Analysis We reveal and quantify significant discrepancies between academic claims and practical performance with re-implemented baselines achieving 10.6-57.7% compared to reported 93-99.2% highlighting critical reproducibility challenges in sports video analysis. 3 Modern Architecture Integration We introduce a systematically optimized Efficient Net-GRU architecture achieving 92.25% accuracy demonstrating substantial improvements possible with modern techniques and standardized evaluation protocols. Cricket being one of the world s most popular sports with over 2.5 billion fans globally generates enormous amounts of video content requiring automated analysis for broadcasting coaching and fan engagement applications. Cricket shot classification the task of automatically identifying different batting techniques from video sequences represents'}, {'rank': 5, 'score': 0.047144407691455587, 'id': '2509.20913v1', 'title': 'Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales', 'text': 'Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic'}]",and the Performance Gap Total 1 320 284 284 Our results strongly suggest that modern architectures when properly optimized are superior for complex video tasks. Architectural Insights and the Performance Gap Total 1 320 284 284 Our results strongly suggest that modern architectures when properly optimized are superior for complex video tasks.
How can I train models stably with limited computational resources?,2510.12758v1,2510.12850v1,False,"['2510.12758v1', '2510.10822v1', '2510.13137v1', '2510.12758v1', '2510.11073v1']","[0.10247576433385247, 0.0969395742433313, 0.07614167418023042, 0.0743411875313391, 0.07181611323798981]","['PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer']","[{'rank': 1, 'score': 0.10247576433385247, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': 'the model to learn. Additionally Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech-anism Du SFE and DL-HMC++. As shown in Table IX the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically DL-HMC++ shows a 37% reduc-tion in the number of parameters 2.2M vs. 3.5M an 81% de-crease in computational cost 4.0G FLOPs vs. 21.3G FLOPs and a 57% faster inference time 3.30ms vs. 7.67ms. These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and'}, {'rank': 2, 'score': 0.0969395742433313, 'id': '2510.10822v1', 'title': 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis', 'text': 'in DL models for CXR diagnosis. By replacing the final classification layer of a CNN with an XGBoost model we demonstrated that it is possible to significantly reduce disparities across sex age and race subgroups while preserving if not improving overall model performance. Our approach generalizes effectively across multiple medical conditions and remains robust in both ID Che Xpert and OOD MIMIC evaluations. Through our experiments we showed that XGBoost outperforms alternative classifier heads in balancing fairness and accuracy. The method is model-agnostic and can be applied to any architecture capable of extracting embeddings from images. Our method rivals or exceeds traditional full-model bias mitigation techniques including weighted sampling adversarial training data augmentation and active learning while requiring far fewer computational resources. Combining our XGBoost head retraining with active learning yields the most substantial bias reduction across all subgroups while maintaining a competitive performance. These findings offer a compelling pathway for deploying fair and efficient medical AI models in real-world clinical settings where computational constraints are often a major barrier. Despite promising results this study has several limitations. The racial subgroup analysis is affected by class imbalance particularly for Black patients. This underrepresentation limits the statistical robustness of bias evaluations and may obscure subtle disparities. Moreover our work focuses only on CNN-based models applied to CXRs. Therefore the generalizability of'}, {'rank': 3, 'score': 0.07614167418023042, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs while the LSTM captures long-range dependencies across frames. A key innovation is their multi-task framework which simultaneously optimizes for action classification and localization improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets e.g. UCF101 though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language ISL. The paper highlights the model s ability to extract spatiotemporal features directly from raw video inputs achieving robust performance on ISL datasets. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework particularly in evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system'}, {'rank': 4, 'score': 0.0743411875313391, 'id': '2510.12758v1', 'title': 'PET Head Motion Estimation Using Supervised Deep Learning with Attention', 'text': 'viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts Tab. II we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al. PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners including an ultra-high performance human brain PET/CT scanner which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation. TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters ×106 FLOPs ×109 Inference Time ms Memory GB Avg. Trans. Avg. Rot. In this paper we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data-driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model s ability to track the motion by establishing'}, {'rank': 5, 'score': 0.07181611323798981, 'id': '2510.11073v1', 'title': 'ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer', 'text': 'must also be transparent clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically challenges include seamless integration with existing hospital IT systems like PACS and EMR and the need 9 for sufficient computational resources e.g. GPU clusters to process image data at scale. Fourth in future we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models 135 141. In summary we have substantiated the effectiveness of the proposed ROFI technique across various applications including clinical diagnosis privacy protection compatibility to medical AI models and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People s Hospital Approval No.SH9H-2022-T380-1 Eye Center of Xiangya Hospital of Central South University Approval No.202407131 and Renmin Hospital of Wuhan University Approval No.WDRY2024-K238. All patients agreed to participate in the prospective study at the ROFI Program either by themselves or via their legal guidance. For the publication of identifiable images the written informed consent was obtained from the parents or legal guardians. This'}]",These findings offer a compelling pathway for deploying fair and efficient medical AI models in real-world clinical settings where computational constraints are often a major barrier. A critical insight is the model s sensitivity to computational resources which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints.
Why is preprocessing important for Ethic-BERT's performance?,2510.12850v1,2510.12850v1,True,"['2510.12850v1', '2510.12850v1', '2510.12850v1', '2510.10729v1', '2510.07320v1']","[0.35407406116220214, 0.09035686006859177, 0.07456333249221747, 0.05782483171682662, 0.056058967561588256]","['Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children']","[{'rank': 1, 'score': 0.35407406116220214, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification. Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT a BERT-based model for ethical content classification across four domains Commonsense Justice Virtue and Deontology. Leveraging the ETHICS dataset our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities alongside advanced fine-tuning strategies like full model unfreezing gradient accumulation and adaptive learning rate scheduling. To evaluate robustness we employ an adversarially filtered Hard Test split isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT s superiority over baseline models achieving 82.32% average accuracy on the standard test with notable improvements in Justice and Virtue. In addition the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions it is crucial to ensure they operate within the bounds of ethical principles. However encoding the complexity of human moral reasoning into AI systems poses significant challenges requiring innovative approaches to'}, {'rank': 2, 'score': 0.09035686006859177, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been widely explored using traditional ML models and DL approaches. Similarly Maqsood et al. proposed a hybrid approach combining Random Forest Multinomial Naive Bayes and SVM with CNNs observing that SVM outperformed other traditional ML models while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework integrating classifiers such as Logistic Regression Random Forest K-Nearest Neighbors and SVM. Their results using datasets like UCI s Spambase and the Kaggle Spam Filter Dataset demonstrated that BERT significantly improved spam classification achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile Labonne and Moran explored Large Language Models LLMs in spam detection developing Spam-T5 a fine-tuned version of Flan-T5. Their work showed that Spam-T5 performed exceptionally well in low-data settings surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification achieving superior performance in handling contextual variations. Similarly Zhang et al. integrated BERT with large LLMs in a hybrid approach improving sentiment intensity prediction and aspect extraction. In the domain of'}, {'rank': 3, 'score': 0.07456333249221747, 'id': '2510.12850v1', 'title': 'Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification', 'text': 'study on deep learning models for hate speech detection concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi-task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detection concluding that while specialized embeddings enhanced detection of coded hate 3 speech pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis highlighting machine learning deep learning and large language models. They explore applications datasets challenges and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study comparing six shallow learning algorithms with three deep models found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification emphasizing the need for improved multilingual detection. They introduce Deep-BERT a model combining CNN and BERT which enhances accuracy in identifying offensive content across different languages. Also spam detection has been'}, {'rank': 4, 'score': 0.05782483171682662, 'id': '2510.10729v1', 'title': 'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning', 'text': 'These include Sentiment Analysis This module employs VADER or BERT-based sentiment analysis models to capture the emotional polarity of a sentence. Sarcasm often involves polarity flips where positive sentiment is expressed with a negative undertone or vice versa. VADER with its rule-based sentiment scoring excels in social media text while BERT captures deeper contextual sentiment shifts. Contextual Embedding Powered by BERT this module encodes the input sentence into high-dimensional vectors that reflect contextual meaning. Unlike traditional embeddings e.g. Word2Vec BERT dynamically adjusts word meanings based on their sentence context which is crucial in understanding nuanced sarcasm. Linguistic Features This component utilizes Spa Cy and custom NLP rules to extract syntactic and semantic cues such as punctuation usage exaggerated expressions all caps and interjections e.g. Yeah right. Fig. 1. Illustration of a Basic Neural Network Emotion Detection A CNN/LSTM hybrid model is used to detect underlying emotional tone such as frustration amusement or confusion. These emotions when mismatched with surface sentiment often signal sarcastic intent. B. Convolutional Neural Networks CNNs CNNs are specialized neural networks designed to process data with grid-like topology such as images or text matrices. They use convolutional layers to detect patterns like edges or textual features followed by pooling layers to reduce dimensionality. The outputs from these modules are concatenated into a unified feature vector followed by'}, {'rank': 5, 'score': 0.056058967561588256, 'id': '2510.07320v1', 'title': 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children', 'text': 'E.1 Loss Functions Classification Loss Cross-entropy for 4-class emotion recognition N X 4 X LCE 1 IV. RESULTS AND DISCUSSION j 1 yi j log pi j 8 N The experimental evaluation was conducted to assess the effectiveness of autoencoder preprocessing on emotion recognition performance using ASD children datasets. Two state-ofthe-art deep learning architectures Xception and Inception V3 were evaluated under both baseline conditions without preprocessing and enhanced conditions with autoencoder preprocessing. The results demonstrate substantial and statistically significant improvements across all performance metrics. i 1 Total Loss Ltotal LAE + αLCE E.2 Optimization Adam optimizer with adaptive moments to address the loss function regarding classification loss mt β1mt 1 + 1 β1 gt vt β2vt 1 + 1 β2 g2 t θt θt 1 α vt + ε ˆmt 9 A. Performance Comparison Analysis Table I presents the comprehensive performance comparison between baseline and autoencoder-enhanced models. The reParameters α 0.001 β1 0.9 β2 0.999 ε 10 8. C. Error Rate Reduction Analysis sults reveal consistent and substantial improvements across both architectures when autoencoder preprocessing was integrated into the pipeline. A particularly noteworthy finding was the substantial reduction in classification errors. The Xception model achieved a 48.0% reduction in error rate from 27.7% to 14.4% while Inception V3 demonstrated a 44.1% reduction from 29.0% to 16.2%. TABLE I PERFORMANCE COMPARISON'}]",These findings contribute to performance improvement and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification.
What are the main strengths of using an LSTM model for real-time sign language translation?,2510.13137v1,2510.13137v1,True,"['2510.13137v1', '2510.13137v1', '2510.13137v1', '2510.13137v1', '2510.13137v1']","[0.2780875830064769, 0.19817694184472515, 0.17793207992428076, 0.17441918306932383, 0.1508227797223933]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN']","[{'rank': 1, 'score': 0.2780875830064769, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a'}, {'rank': 2, 'score': 0.19817694184472515, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between'}, {'rank': 3, 'score': 0.17793207992428076, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': '2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big gap because automated translation tech isn t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision-based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory LSTM networks which are proficient in modeling the temporal sequence of hand movements and 3D Convolutional Neural Networks 3D CNNs which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language ASL gestures into text. We look at how accurate they are how much computing power they need what s required to train them and how well they work in real-time situations. Using Media Pipe for reliable hand tracking we ve F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. generalization. A key insight is'}, {'rank': 4, 'score': 0.17441918306932383, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches including 3D CNNs and LSTMs for sign language recognition. The paper systematically compares model performance across public benchmarks highlighting strengths e.g. 3D CNN accuracy on spatial features and limitations e.g. LSTM dependency on sequential data. A critical takeaway is the discussion on open challenges such as real-time deployment and dataset scarcity which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques including 3D CNNs Transformers and hybrid models for sign language recognition. Published in CMES 2024 their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer. Sign language is the primary way that people in the Deaf and Hard of Hearing DHH community communicate yet there s still a big'}, {'rank': 5, 'score': 0.1508227797223933, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'evaluating spatial-temporal architectures for region-specific sign languages. In their innovative work Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at IEEE ICCE-Asia 2022 their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions reducing noise from irrelevant background motions. A key insight is the attention mechanism s ability to boost interpretability while maintaining real-time performance 45ms latency on GPU. This work aligns with our exploration of 3D-CNNs strengths in spatial modeling while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN-LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction their high complexity challenges real-time deployment. LSTMs conversely efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al. s work by quantifying this accuracy-efficiency dichotomy particularly for edge-device scenarios and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal'}]",CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures.
How does model selection affect responsiveness in real-time applications?,2510.13137v1,2510.13137v1,True,"['2510.13137v1', '2510.08770v1', '2510.13137v1', '2510.13050v1', '2510.13137v1']","[0.1277252627215386, 0.10340558874246286, 0.10335766823085299, 0.08109430220854139, 0.07255190560102227]","['Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN']","[{'rank': 1, 'score': 0.1277252627215386, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V. Conclusion In conclusion this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures making it highly suitable for real-time applications especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames offering slightly higher classification accuracy in offline scenarios. However the 3D CNN comes with a'}, {'rank': 2, 'score': 0.10340558874246286, 'id': '2510.08770v1', 'title': 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform', 'text': 'test set and real-time input. The same hardware was used to train the model as was used to collect data A Lenovo Legion Pro 7i with NVIDIA RTX 4080. Data used and real-time testing area were homogeneous across room and liquid types for this comparison For the training strategy the last 5 layers were fine-tuned an RMSprop optimizer was used lr 1e-5 a binary crossentropy loss function was used and early stopping was used with patience 5. A batch size of 8 was used for training and validation while a batch size of 2 was used for test. The model was trained with up to 50 epochs however this was never reached due to the application of the early stopping mechanism. For data augmentation images were randomly flipped horizontally slightly rotated and had small variations in contrast during training The random rotation and contrast had factor 0.01. The standard VGG preprocessing pipeline was then used. The results are found in Table I. Testing The results show that Combined although accurate on the test dataset is not effective in real-time testing. RGB although effective in real-time testing and near-perfect in the test set is slower for inferences and has a model size three times that of thermal. Thermal performs with 100% accuracy on the test set and real-time testing along with'}, {'rank': 3, 'score': 0.10335766823085299, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'as input e.g. 30 RGB frames of 128×128 pixels preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames e.g. 30×128×128×3 LSTM Explicitly designed for sequential data making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students Applied Science Engineering Project1 ASEP1 Paper SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology Pune INDIA. Additionally while the 3D CNN was slightly more accurate in offline evaluation it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong learns from raw images Temporal Modelling Limited no texture or shape info User testing and qualitative observations reinforced these results the LSTM model demonstrated higher responsiveness and robustness in live video input making it preferable for interactive applications such as assistive communication tools. Meanwhile the 3D CNN although precise in controlled environments lacked the adaptability and responsiveness required for real-time translation. Overall this comparative analysis underscores that while 3D CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking +'}, {'rank': 4, 'score': 0.08109430220854139, 'id': '2510.13050v1', 'title': 'An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting', 'text': 'Auxiliary IMERG Final 0.1 × 0.1 1800 × 3600 Dense globally Auxiliary 3.1.2. Training Input Features Unlike the training targets that do not need to be available in real-time during operations the training features should be available at least within a few hours of inference time to be relevant to the predictions. Table 2 outlines the datasets we use as gridded inputs along with their real-time latencies. These latencies are baked into the training dataset by fetching older data corresponding to b 15 consecutive orbital swaths sampled by GPM CORRA demonstrating the spatial footprint observed by the satellite over the course of a full day. a Radar coverage globally Saltikoff et al. 2019. This figure is for illustration purposes as noted by Saltikoff et al. in Figure 1 of their paper and may not be up-to-date. Figure 8 Global data coverage maps for the training targets. its real-time latency than the one at the initialization time of each training example. In the table we also mention how many historical timestamps we use for each of the inputs. Table 2 Input Datasets. Dataset Spatial Resolution Real-time Latency channels historical timestamps Ground Radars 0.05 × 0.05 30 mins 1 6 timestamps 15 mins apart Geostationary Satellite Mosaics 0.05 × 0.05 60 mins 17 3 timestamps 30 mins apart HRES atmospheric variables 0.1'}, {'rank': 5, 'score': 0.07255190560102227, 'id': '2510.13137v1', 'title': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN', 'text': 'Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 200 ASL signs across 50 classes comparing their accuracy computational efficiency and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN-LSTM model shows decent performance which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. In their study Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model s ability to process both spatial hand shapes and temporal gesture motion information simultaneously achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between'}]",CNNs offer higher classification accuracy LSTM models strike a better balance between accuracy speed and computational efficiency thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate using 3D convolution Preprocessing Hand tracking + landmark extraction Strong using LSTM layers Cropping resizing normalization Computation Low lightweight real-time friendly High GPU required Training Data Works with smaller datasets Requires larger labeled video datasets Real-Time Capability Excellent Limited depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High coordinate-based decisions Low complex visual features V.
