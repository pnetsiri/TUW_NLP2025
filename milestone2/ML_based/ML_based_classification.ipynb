{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "829813ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install verbatim-rag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "import_libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "from verbatim_rag.document import Document, Chunk, ProcessedChunk, DocumentType, ChunkType\n",
                "from verbatim_rag.ingestion import DocumentProcessor \n",
                "from verbatim_rag.vector_stores import LocalMilvusStore\n",
                "from verbatim_rag import VerbatimIndex\n",
                "from verbatim_rag.embedding_providers import SpladeProvider\n",
                "from collections import defaultdict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "b478416c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading 20 papers...\n"
                    ]
                }
            ],
            "source": [
                "documents_for_index = [] \n",
                "\n",
                "corpus_path = Path(\"../../corpus_json/corpus.json\")\n",
                "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
                "    corpus = json.load(f)\n",
                "\n",
                "print(f\"Loading {len(corpus)} papers...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "preview_data",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.microsoft.datawrangler.viewer.v0+json": {
                            "columns": [
                                {
                                    "name": "index",
                                    "rawType": "int64",
                                    "type": "integer"
                                },
                                {
                                    "name": "id",
                                    "rawType": "object",
                                    "type": "string"
                                },
                                {
                                    "name": "title",
                                    "rawType": "object",
                                    "type": "string"
                                },
                                {
                                    "name": "text",
                                    "rawType": "object",
                                    "type": "string"
                                }
                            ],
                            "ref": "9d912178-4c98-441f-8374-234237bdd9b0",
                            "rows": [
                                [
                                    "0",
                                    "2509.20913v1",
                                    "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales",
                                    "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles and Philadelphia. We employ crime incident data obtained from each city s police department combined with sociodemographic data from the American Community Survey and human mobility data from Advan collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles 0.2 sq. kms and used to train our deep learning forecasting model a Convolutional Long ShortTerm Memory Conv LSTM network which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models logistic regression random forest and standard LSTM. Incorporating mobility features improves predictive performance especially when using shorter input sequences. Noteworthy however the best results are obtained when both mobility and sociodemographic features are used together with our deep learning model achieving the highest recall precision and F1 score in all four cities outperforming alternative methods. With this configuration longer input sequences enhance predictions for violent crimes while shorter sequences are more effective for property crimes. These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting mobility included. They also highlight the advantages and limits of deep learning when dealing with fine-grained spatial and temporal scales. The increasing availability of data coupled with advancements in analytical techniques and the progressive democratization of programming languages has contributed to facilitate the computational study of criminal phenomena. One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime which is particularly important for theory and policy testing and development as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime. The spatial study of crime has its roots in the first half of the 19th century 4 6 and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place first introduced by which focuses on the dynamics of crime in microgeographic units within cities such as addresses facilities street segments or small clusters of street segments. Since then the growing interest in these micro-dynamics has led to practical applications in crime prevention such as hot spots policing. Along with the spatial study of crime another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime. The two dimensions space and time are intrinsically interconnected both components of a crime event are fundamental due to their correlation as time constraints result in space constraints. Given this fundamentally indissoluble link crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However beyond the spatial and temporal aspects of crime many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty for instance often fosters conditions that increase the likelihood of crime by limiting people s access to opportunities and resources 13 16. Meanwhile neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face. Besides these socio-economic dimensions another central phenomenon impacting the spatiotemporal concentration of crime is human mobility as suggested by various theories uncovering its link to crime incidents. For example areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes. Similarly shifts in population dynamics such as the influx of new residents or temporary workers can disrupt established community networks creating opportunities for criminal behavior. Due to the nonlinear and multidimensional spatiotemporal dynamics that often characterize crime patterns quantitative criminologists along with scholars from other fields 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods these algorithms have in fact often shown to be more effective in capturing complex patterns outplaying less flexible methods in prediction and forecasting tasks. However despite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime as of the current date there exist very few studies on crime forecasting employing deep learning models and combining historical crime data with human mobility data. Moreover such studies use relatively large geographic units of analysis e.g. census tract or police beats or have a large temporal granularity i.e. weeks months or years resulting in predictions that cannot easily yield actionable recommendations. Additionally these studies often focus on only one city or employ just one year of historical data leading to models with low generalizability to different contexts 22 25. To address these gaps in this research we propose a deep learning framework based on Convolutional Long Short-Term Memory Conv LSTM layers to perform spatiotemporal crime forecasting in microgeographic units utilizing historical crime data in conjunction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically the main contribution of this work is that to the best of our knowledge this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions 0.077 sq. miles and 12 hours respectively covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis allowing for a more indepth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This in turn advances our understanding of crime both theoretically and practically while further investigating whether and how artificial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort focused on hourly time scales and fine-grained spatial resolution is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution but also to generate practical knowledge that can inform policy decisions and situational interventions. These may include for example shift-by-shift allocation of patrol officers but also extend to non-policing measures such as adjusting lighting in high-risk areas modifying urban infrastructure to reduce anonymity or escape routes deploying community outreach teams during peak risk hours or temporarily altering the accessibility of specific spaces e.g. parks transit hubs based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2 we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4 we describe the datasets used the extensive data processing carried out and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5 we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly in Section 6 we discuss the results highlight the limitations of this study and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concentration proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that for a defined measure of crime at a specific microgeographic unit the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime. This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city. The law of crime concentration one of the most universally accepted features of crime as a human phenomenon is among the many applications of the Pareto principle which posits that while numerous factors may contribute to an observed outcome a small subset of these factors often accounts for the majority of the observations. This principle has been shown to govern many social phenomena. For instance it appears in education particularly in the dynamics of classroom disruption in network theory applied to social influence in online communities in the field of economics specifically regarding wealth distribution and in other areas of criminology for example asserting that most crimes in a population are committed by a small number of individuals. We frame the present study starting from the law of crime concentration because if crime is clustered in space it follows that there are statistical patterns to be inferred. Learning these patterns along with their sources of variation is the key goal of our forecasting framework. However while the law of crime concentration represents the fundamental intuition behind this work there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and importantly how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns population flows gentrification and urban sprawl these fluid phenomena are crucial to capturing crime patterns making it imperative to incorporate such dynamic elements into the study of crime especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon human mobility both influences and is influenced by the environments people engage with. These dynamics also play a critical role in the occurrence of criminal events which arise from the interplay between offenders potential targets and their surroundings. Certain conditions such as a lack of guardianship transient populations or economic disadvantage can amplify the likelihood of crime occurring. Therefore to better understand these processes researchers have proposed theories that examine how human mobility social structures and neighborhood characteristics contribute to the spatial and temporal distribution of crime. For instance crimes are more likely to occur when offenders and potential targets converge in environments that lack capable guardianship. This concept is central to Routine Activity Theory RAT introduced by. According to RAT offenders act when they possess both the motivation and the capacity to commit a crime often selecting victims either deliberately based on characteristics like identity or possessions or opportunistically based on accessibility or vulnerability. Guardians such as police officers family members or institutions like law enforcement serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime leading individuals to form awareness spaces consisting of their main routine activity nodes. Hence the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise. RAT is therefore important for crime forecasting and for this study specifically as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activities e.g. population density and business hours that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and human mobility have been backed by extensive empirical evidence 24 36 38. Benefiting from the growing integration between traditional data and more innovative sources for a review see these studies usually include mobility features in the form of footfall1 showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns however are also closely linked to the socioeconomic conditions of neighborhoods. Communities with high levels of poverty residential instability and ethnic diversity often experience weakened social networks and limited informal social control. These conditions form the basis of the Social Disorganization Theory SDT developed by the Chicago School. Social disorganization fosters environments where subcultures of crime and deviance flourish further contributing to higher crime rates. Despite some challenges and inconsistencies in its empirical research SDT remains highly relevant as it provides a lens through which we can investigate how human mobility neighborhood dynamics and social structures intersect to influence crime rates. Hence using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends as well as changes in these patterns and trends occurring in urban environments. Leveraging both dimensions we argue would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies each designed to address specific contexts and objectives. A review of the past decade s literature reveals five core elements in study design. Here we outline our contributions and innovations with respect to each of these elements. Element 1 What urban focus Firstly most studies focus on a single city in the U.S. such as New York City or Chicago 43 46. A smaller number of studies aim for broader comparability. For example analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is which examined eventlevel predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches our study investigates four U.S. cities to enhance generalizability of our results seeking to underscore common patterns and potential differences across diverse urban areas. Element 2 What geographical scale Secondly the choice of geographic unit which reflects a fundamental trade-off between spatial detail and computational accuracy has mostly focused on relatively large areas of 1 km2 or larger. Despite this there are some studies that used very fine geographic units of analysis. For instance used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock AR and used a grid of cells measuring 183 × 183 meters approximately 0.122 km2 to crime incidents in Portland OR. However in such studies the corresponding temporal granularity was relatively large i.e. six months and two weeks respectively. Our analysis adopts a spatial unit of 0.077 sq. miles 2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable also avoiding excessive sparsity which would have rendered any forecasting exercise theoretically and practically useless. 5 Element 3 What temporal granularity As anticipated in the previous paragraph the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions while some explore larger granularities. For instance employed a coarser approach generating yearly forecasts for gun violence. Similarly examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity we aim to provide more actionable insights for real-time resource allocation and urban safety measures underscoring not only the potential of this method but also its limits. Element 4 What temporal focus Temporal coverage of the historical data is another critical dimension influencing the ability to capture trends and variability. Many studies rely just on one year of data focusing on short-term predictions while few others try to capture long-term patterns by extending their analysis to eight years or more. Other researchers including and settled on datasets spanning five years finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus leveraging five years of data to identify meaningful patterns. Element 5 How many data sources Lastly recent advancements in crime forecasting have increasingly emphasized the importance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data more recent approaches highlight the added value of contextual information as well. In general the use of mobility data has increased substantially in the past years in criminology and has played a major role in the computational social science revolution. Mobility is measured through various sources including transportation data GPS tracking and social media or other types of digital traces and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this growing interest and the evident potential of mobility data to enrich crime analysis their role in short-term crime forecasting remains underexplored and largely unquantified. In particular we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur an insight that would be highly valuable to both scholars and practitioners. For this reason our study embraces a multidimensional perspective integrating crime mobility and sociodemographic data to capture the complex interplay of social spatial and temporal dynamics in crime occurrence in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore by building on these five elements our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research we analyzed four U.S. cities Baltimore MD Bal Chicago IL Chi Los Angeles CA Las and Philadelphia PA Phi. These cities were chosen due to their high crime rates and their good coverage of mobility data the specific criteria we defined and used are explained in detail in Section 3.3. In addition their diversity with respect to location urban structure and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance Baltimore is a port city focused on shipping and trade with a significant African American majority. In contrast Los Angeles is nearly six times larger characterized by a sprawling car-dependent structure and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables the date and time of the crime incident the category of the crime incident and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city we first selected those that were common across all four settings and among these the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of underreporting which would have introduced non-trivial issues to our forecasting exercises 64 66. Specifically we selected the following crime categories burglary motor vehicle theft MVT assault homicide and robbery. The first two are property crimes while the remaining three are violent crimes. Our analysis is thus three-fold we developed models forecasting all crime together as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study we were aware that not only the number of POIs was relevant but also the relationship POIs had with population size and the city s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason we defined a variable called the people-to-POI ratio representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density hence indicating the number of people per POI within 1 km2. Therefore since this is a normalized value it offers a fairer way to compare the cities particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list we focused on cities with high crime rates verified that a each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and b each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection i.e. Baltimore Chicago Los Angeles and Philadelphia. Table 3 presents the people-to-POI ratio for each of the four cities with a lower ratio indicating better POI coverage. In Appendix B we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2 930 28 218 21.78 Chi 591 4 581 95 049 28.46 Las 1 216 3 273 102 478 38.80 Phi 347 4 518 55 496 28.25 people visited each POI category in each cell during each time window thus representing the underlying mobility characteristics of all our cities mapping potential time-specific patterns at the daily level i.e. dynamics changing between day and night as well as meso-level i.e. different dynamics on different days of the week and macro-level e.g. structural changes due to gentrification seasonality and trends. Second we calculated the POI category diversity using the Shannon Diversity Index7 defined as n O H i 1 pi ln pi 1 where pirepresents the proportion of each POI category iwithin each spatial unit of analysis and nis the total number of POI categories 11 in our case. We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out hypothesizing that this information can be valuable in understanding its potential crime attractors or targets as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically we used the ACS 5-year estimates for 2019 2020 and 202189. These datasets contain more than 20 000 variables related to the social economic demographic and housing characteristics of the U.S. population for different geographic granularities e.g. region county county subdivision and block group. Firstly we chose the block group as our geographic level since it is the smallest granularity available. Afterward we selected 26 variables related to gender age race/ethnicity employment income education and marital status features that have been identified as correlated with crime in the scientific literature 69 71. The specific variables are listed in Appendix E. Their raw values in the case of medians or percentages in the case of counts were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis we used a grid-based approach which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study s time span. Therefore the problem definition becomes as follows Consider that we divide a city into an N× Ngrid with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles hence into a matrix of the form G gi j 0 i j N. Then for each grid cell gi j we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes where we have that the crime feature at the 12-hour block tis represented by Ct i j ct T i j ct T+1 i j... ct 1 i j where Tis our look-back period hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features which are denoted as Mt i j mt T yi j mt T+1 yi j... mt 1 yi j for each mobility feature ywith y 1 2... 12 and for each 12-hour block t. Lastly we have the 26 sociodemographic features hence for each sociodemographic feature zwith z 1 2... 26 and each 12-hour block t we get the features St zi j st T zi j st T+1 zi j... st 1 zi j 10. Consequently and borrowing this example from computer vision each frame can be interpreted as an N×Nimage with 39 channels where each channel represents a different feature see Figure 2. In other words our forecasting task can be thought of as similar to predicting the next frame in a video given a series of previous frames each characterized by a specific set of features what will the next frame look like These frames are then grouped into sequences with the length determined by the look-back LB period T. Each feature within these sequences is then normalized using min-max scaling ensuring consistent value ranges across the dataset. Also to maintain uniform grid sizes across all four cities and to augment the number of samples available we randomly sampled five M× Msubsections for each sequence of frames keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study we set M 16 which covers approximately 19.31 sq. miles 50 km2 and used two LB periods of T 28 and T 4 equivalent to 14 days and 2  Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature t2 t3 t4 t3 t4 t5 t4 t5 t6 t5 t6 t7 t6 t7 t8 t7 t8 t9 and t8 t9 t10. Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next to avoid data leakage we remove the last Tsequences of the training set since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset when T 14 this procedure resulted in training and test sets containing 3 222 and 362 sequences respectively which would increased to 16 110 and 1 810 samples after extracting the five subgrid sections. However sequences with no crime events were discarded leading to the loss of some samples. Additionally to keep the same sample size for all data configurations i.e. for the four cities and three crime aggregations we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12 546 sequences and a test set size of 1 510 sequences. Lastly before using these datasets for our analysis we applied a spatial mask to exclude all cells that met at least one of the following criteria i they are located outside city borders ii they consist entirely of water or iii they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers proposed by author. This type of neural network consists of two main components Long Short-Term Memory LSTM layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance they have been used to predict daily streamflow through weekly forecast horizons for speech recognition by modeling temporal dependencies in audio signals and for determining the probability that certain social media content will gain popularity. They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state c a hidden state h and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft the input gate it and the output gate ot. Their corresponding equations for the multivariate version of the LSTM i.e. the fully connected LSTM or FC-LSTM where both inputs and outputs are 1-dimensional vectors are as follows ft σ Wxfxt+ Whfht 1 + Wci ct 1 + bf 2 it σ Wxixt+ Whiht 1 + Wcf ct 1 + bi 3 ot σ Wxoxt+ Wxcht 1 + Wco ct+ bo 4 where ht 1 is the previous hidden state xtis the current input Ware the weight matrices where the subscripts indicate the two variables that are connected by this matrix bxis the 14 bias for that gate and σrepresents the sigmoid function. The goal of the forget gate is to determine which information from the previous cell state ct 1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht ct ftct 1 + ittanh Wxcxt+ Whcht 1 + bc 5 ht ot tanh ct. 6 These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently the cell state and the output gate are employed to obtain the hidden state which summarizes all the information learned. Lastly the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor the recurrent neural networks 77 79. However while LSTMs are effective at capturing temporal information they are not engineered to account for spatial information. To address this Conv LSTMs extend traditional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors where the first dimension represents time and the other two are the spatial dimensions the number of rows and the number of columns of the input frame.Therefore the matrix multiplications in the FC-LSTM equations used in both the input-to-state and state-tostate transitions are replaced by convolutional operations and the weight matrices are replaced by convolutional kernels. To illustrate how the introduction of convolutional operations allows the model to preserve spatial information consider a simple example where the input xtis a singlechannel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence the operation results in a 9 × 1 vector which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore we can see that the fully connected operation disregards the spatial structure of xt. In contrast for Conv LSTM assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid followed by summing the results. This operation is repeated for the rest of the grid with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input zero-padding12 is applied. Therefore this operation results in a 3 × 3 matrix as output preserving the spatial dependencies of the input and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv LSTMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting using past radar sequences to predict future radar frames. Since then they have been applied in various contexts such as quantifying the contributions of climate change and human activities to vegetation change and predicting short-term traffic flow. Therefore this type of neural network is typically used for video frame prediction as it accounts for both the temporal and spatial components of the data making it ideal for the data structure we constructed. While this is usually performed with either grayscale images 1 channel or RGB images 3 channels 13 we adjusted the model architecture to train on images with 39 channels corresponding to our 39 features and output a 1-channel image. In order to engineer our model we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters see Appendix G for the specific values used until we obtained the model architecture depicted in 123 and 999 to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First the presence of crime in a location affects the nearby areas and the same occurs with police presence and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second given obfuscated georeferenced crime data crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O oi j 1 i j Mdenote the ground truth crime occurrence matrix and P pi j 1 i j Mthe predicted crime occurrence matrix where oi j pi j 0 1 and Mis the size of the matrix as defined in Section 4.1. Next let Ni j O represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell i j O i.e. its nearest neighbors. Using these three elements the classification outcomes are defined as follows True negatives tn Cells where both oi j 0 and pi j 0 True positives tp Cells where both oi j 1 and pi j 1 False negatives fn Cells where oi j 1 and pi j 0 Cell Classification fpnn if i j Ni j O such that oi j 1 neighboring false positive False positives fp otherwise standard false positive 7 Therefore the modified metrics become Recall NN tp+ fpnn tp+ fpnn+ fn Precision NN tp+ fpnn tp+ fpnn+ fp. These modified metrics called NN an acronym for nearest neighbor are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes if the cells share an entire border or 8 minutes if they only share a corner walking or 2.5 and 4 minutes running respectively to go from the center of one cell to the center of a neighboring cell assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly to determine the effectiveness of our proposed model we compare it against three baseline models chosen to provide a comprehensive evaluation by spanning statistical methods traditional machine learning techniques and deep learning approaches 1 Logistic Regression LR a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable 2 Random Forest RF a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships and 3 LSTM a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly we assessed the model s performance against the three chosen baselines LR RF and LSTM by considering all five crime categories together to predict whether there will be a crime in the next time window without discriminating by type. On the one hand this approach has the advantage that it contains more crime incidents thereby making the dataset relatively more balanced and stabilizing the learning process. On the other hand combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify recall for our Conv LSTM model is always higher than 0.65 across all cities meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low being consistently below 10% meaning that only one in ten forecasts of crime presence are actually true positives. However when considering the modified precision metric performance increases substantially on average when we forecast the presence of crime for a given spatial cell there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days the modified metrics. Although LSTM is the second-best model for recall its precision is lower than that of LR highlighting the importance of the spatial patterns captured by Conv LSTM s convolutional operations for improving both recall and precision. It is also worth noting that despite LSTM s comparable performance it has approximately ten times more parameters than Conv LSTM leading to higher computational costs and an increased risk of overfitting particularly with limited data. Lastly the comparison of the LB periods reveals mixed results across models. For Conv LSTM the 2-day LB period typically shows slightly lower performance compared to the 14-day period although it achieves higher modified precision in Baltimore Chicago and Philadelphia. In contrast LSTM generally performs better with the 2-day LB period. Meanwhile LR demonstrates similar performance across both LB periods while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores we observe that both LB periods have similar performance with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods see Appendix J shows that the shorter LB period performs similarly or worse except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes i.e. assault homicide and robbery. Although this dataset contains fewer crime incidents it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city as shown in below 0.2. RF continues to be the worst-performing model generally exhibiting very low recall and precision although its performance improves in Chicago Los Angeles and Philadelphia compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance as indicated by the F1 score compared to its application to all crimes or violent crimes alone. Finally the Conv LSTM model remains the best-performing model maintaining strong recall values but showing a decrease in precision a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB 14 days LB 2 days see Appendix J. These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models general performance we sought to assess the impact of including mobility features in our deep learning framework. Thus we retrained the best-performing model Conv LSTM using various feature sets CMS CM CS and C where C represents crime features M represents mobility features and S represents sociodemographic features. This retraining was conducted using the best configuration alone hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days the first observation is the considerable variation in standard recall within each city depending on the feature set used for training. Looking more closely we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall based on both the standard and modified recall and precision values the setup incorporating all three feature types consistently performs best as expected. Specifically among the 24 configurations seen in Figure 8 the C setup achieves the highest performance in 8.3% of cases the CM setup in 8.3% CS in 29.2% and CMS in 54.2%. Therefore the results indicate that incorporating additional feature types alongside crime data improves the model s ability to forecast effectively. Conversely when analyzing the results with an LB period of 2 days performance consistently decreases slightly when using the standard metrics. However Chicago and Los Angeles remain the cities with the lowest performance and the CMS configuration again emerges as the best overall. In this case the C setup does not achieve the highest performance in any instance while the CM setup outperforms others in 12.5% of cases CS in 29.2% and CMS in 58.3%. Thus while shorter LB periods lead to a slight reduction in overall performance the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward to further understand the impact of mobility features on predictive performance we calculated the percentage difference between the results of CS and CMS to assess the effect of incorporating mobility features and CM and CMS to evaluate the effect of sociodemographic features for both LB periods. As shown in Table 4 the results generally indicate a slight decrease in recall when a third feature set is added both when going from CM to CMS and from CS to CMS resulting in a negative percentage point difference. In contrast precision increases in all configurations with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance F1 score compared to the 14-day period suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally comparing the F1 scores between CMS-CM and CMS-CS we can see that sociodemographic features have a greater impact than mobility features in increasing performance however both contribute to enhancing the model s ability to forecast crime with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB 14 days LB 2 days Firstly we used five years of human mobility data in combination with crime and sociodemographic data from four U.S. cities to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal granularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complexity involved in building it. Lastly we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM consistently outperformed the three baseline models particularly in recall but struggled with precision especially when using standard metrics. LSTM showed the closest performance in both recall and precision though it remained slightly below Conv LSTM. LR while competitive in precision suffered from low recall and RF failed to capture meaningful patterns across all data configurations. Therefore on the one hand the effort involved in building such an elaborate model was worthwhile as the other models either struggled with high-dimensional data or as in the case of LSTM had to be highly complex to achieve comparable performance. Moreover it should be noted that Conv LSTM s high recall values suggest it can predict most instances where at least one crime is likely to occur minimizing false negatives as intended. However the low precision results highlight how the extremely unbalanced nature of our forecasting tasks a byproduct of the fine-grained spatial and temporal scales of the study makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types we found that our model performed best when using all crime types together rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset hence leading to more stable training. Additionally the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features we were able to determine that the overall best performance was achieved when using all three feature sets together i.e. crime mobility and sociodemographic data while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources in combination with crime data allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover the percentage difference between CS and CMS indicates that mobility data improve performance especially when using shorter LB periods stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime but are also relevant to policymakers. In fact they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies 3 86 89. Although offering an exhaustive systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work our results underscore that not even extremely sophisticated algorithms can reach high performance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding detected for all cities and crime types bears important implications. Relying on the predictions of our models for instance would certainly allow first responders and law enforcement to predict practically all actual offenses but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse unbalanced and volatile phenomenon as crime. 6.1 Limitations This work despite its promising results is not without limitations. First the use of a gridbased approach while being very common in the literature see among others imposes an arbitrary division on our space and it has been shown that network-based models can outperform grid-based models for crime prediction. Furthermore an arbitrary division of urban space may also pose a problem for instance in terms of administrative or police precinct boundaries since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns which limits how finely we can define our spatial unit of analysis. However our cell size is still considerably smaller than the typical block size in American cities see for some estimates including those analyzed in this research. Additionally given the low precision obtained with our current grid cell size we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore we argue that obfuscation does not constitute a major issue in our current design as it is not as micro in scale. Finally arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features such as mobility flows have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information instead they represent aggregate mobility information at the premise level further aggregated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic cultural or institutional factors potentially introducing biases into the analysis. Nonetheless we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue as well as crimes that are less prone to be the subject of specific policing decision-making processes such as those concerning drug-related offenses. Additionally our model s statistical results particularly its low precision raise significant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas as false positives are pervasive across cities and crime types possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the theoretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting rather than to serve as direct recommendations for enforcement actions. Future research should focus on addressing these ethical concerns improving model precision and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design."
                                ],
                                [
                                    "1",
                                    "2509.23158v1",
                                    "Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization",
                                    "Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization. Early detection of cognitive impairment is critical for timely diagnosis and intervention yet infrequent clinical assessments often lack the sensitivity and temporal resolution to capture subtle cognitive declines in older adults. Passive smartphone sensing has emerged as a promising approach for naturalistic and continuous cognitive monitoring. Building on this potential we implemented a Long Short-Term Memory LSTM model to detect cognitive impairment from sequences of daily behavioral features derived from multimodal sensing data collected in an ongoing one-year study of older adults. Our key contributions are two techniques to enhance model generalizability across participants 1 routine-aware augmentation which generates synthetic sequences by replacing each day with behaviorally similar alternatives and 2 demographic personalization which reweights training samples to emphasize those from individuals demographically similar to the test participant. Evaluated on 6-month data from 36 older adults these techniques jointly improved the Area Under the Precision-Recall Curve AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 highlighting the potential of scalable monitoring of cognitive impairment in aging populations with passive sensing. Cognitive decline associated with aging can significantly impair processing speed working memory and executive function thereby reducing quality of life. Early detection of cognitive dysfunction is crucial for timely diagnosis and intervention. However clinical assessment instruments such as the Montreal Cognitive Assessment Mo CA are typically administered infrequently failing to capture fluctuations influenced by mood fatigue medication or environment and potentially painting an incomplete or misleading representation of cognitive status. Participants Routine-Aware Augmentation which expands the training data by generating synthetic sequences in which each day is replaced with behaviorally similar alternatives. Demographic Personalization which re-weights training samples to emphasize those from individuals demographically similar to the test subject. We systematically evaluated the model and techniques on 6-month data from 36 participants. These techniques jointly increased the AUPRC of the model trained on sensing and demographic features from 0.637 to 0.766 under the leave-one-participant-out LOPO cross-validation scheme. rather than raw sensor signals. Specifically we leverage participants daily routines to generate synthetic trajectories by replacing each day with behaviorally similar alternatives. Personalization improves model performance by tailoring it to individual participants. A common strategy is to introduce a portion of the test participant s data into the training set. Yu et al. further fine-tuned a participant-independent model using a small amount of data from the test subject for wellbeing prediction. While effective these approaches violate subject-level independence and undermine LOPO evaluation s goal of assessing model generalizability to unseen individuals. Moreover they require access to ground truth health outcomes for the test subject posing challenges for cognitive impairment detection. Whereas wellbeing scores can be conveniently obtained via surveys or Ecological Momentary Assessments EMAs determining cognitive status requires time-consuming formal assessments. Therefore models intended for scalable cognitive impairment detection should avoid relying on ground truth labels from the test participant. An alternative approach trains models on a subset of participants similar to the test subject based on personalization metrics e.g. demographics and mental health scores. However this reduces the amount of training data which may be suboptimal for studies with relatively small cohorts. To address these limitations in detecting cognitive impairment our personalization strategy leverages instance weighting to emphasize training samples from participants with demographic profiles similar to the test subject. This approach preserves subject-level independence and utilizes all available training data. II. A. Digital Phenotyping for Cognitive Impairment Digital phenotyping studies have investigated multidimensional behavioral signatures of cognitive impairment. To illustrate Park analyzed smartphone typing dynamics and found that longer keystroke hold times and transition times between consecutive keypresses were associated with poorer cognitive performance. Muurling et al. characterized social engagement from phone calls app usage and location data. They found that cognitively impaired individuals exhibited more repetitive social behaviors specifically calling the same contacts more frequently. A large-scale longitudinal study tracked over 20 000 participants for two years using smartphones and wearables with preliminary findings supporting the feasibility of detecting cognitive impairment through smartphone-based interactive assessments. Furthermore the RADAR-AD study developed machine learning models to differentiate stages of cognitive decline using various smartphoneand wearable-based remote monitoring technologies. Similarly Chen et al. trained XGBoost classifiers to detect cognitive impairment from 12 weeks of multimodal sensing data. Our work builds upon these efforts by leveraging deep learning to model fine-grained behavioral patterns across diverse domains for characterizing cognitive impairment. III. A. Study Protocol B. Deep Learning for Time Series in Health Sensing Our one-year prospective observational study recruits community-dwelling older adults aged 65 years and above. At enrollment participants provided informed consent and installed a custom-built smartphone app for passive sensing. Cognitive assessments from Version 3 of the Uniform Data Set by the National Alzheimer s Coordinating Center were administered remotely every 6 months to evaluate participants cognitive performance at baseline 6 months and 12-month study exit. Demographically adjusted assessment results were analyzed by a neuropsychologist in the study team to determine whether participants exhibited cognitive impairment. As of May 2025 the study is still actively recruiting and monitoring current participants. This manuscript focuses on the baseline cognitive performance of participants enrolled between May 2023 and December 2024. Compared to classical machine learning models deep learning approaches such as RNNs can capture nuanced temporal dynamics and behavioral patterns from frequently sampled sensing data. Among them LSTM has been widely used due to its lightweight architecture and competitive performance in time series modeling. For instance Hong et al. used an LSTM to predict cognitive impairment from daily sleep variables collected via wearables over several months. Umematsu et al. and Yu et al. built LSTMs to forecast future wellbeing of college students based on time series derived from smartphone and wearable sensing. More recent efforts have explored large language models LLMs to infer wellbeing from behavioral time series. While these approaches show promise modeling the complex behavioral phenotypes of cognitive decline can be more challenging. B. Smartphone Sensing Application C. Data Augmentation and Model Personalization Data augmentation is a widely used technique to increase training data size and enhance the performance of deep learning models. Um et al. applied various signal transformations to augment wearable accelerometer time series for monitoring Parkinson s disease. In contrast our augmentation strategy operates on daily behavioral features For data collection we developed an i OS smartphone application that continuously captures multimodal data in the background without requiring any active user interaction. The app utilizes various i OS frameworks to record inertial measurement unit IMU readings infer physical activities track step counts sense geolocations and retrieve metrics from i Phone s built-in Health app. In particular it leverages the i OS Sensor Kit framework only available to research studies reviewed and approved by Apple to collect detailed smartphone interaction data while preserving user privacy. These interactions include smartphone and app usage keyboard typing dynamics and metadata from phone calls and text messages. The app transmits collected data to a secure remote server when the phone is connected to Wi-Fi and is either charging or has at least 50% of battery remaining. if its maximum distance to any other sample recorded within a 10-minute window was less than 200 meters. From these samples we computed measures to quantify various aspects of participants daily movement. Spatial variability was assessed using location variance defined as the logarithm of the sum of variances in latitude and longitude. Spatial extent was characterized by the total distance traveled and geometric properties of the convex hull the smallest polygon enclosing all recorded locations including its area perimeter and Gravelius compactness. To capture temporal characteristics we extracted stationary and moving durations along with the earliest time of movement. Furthermore we assessed movement patterns with respect to the significant places participants visited. These places were identified by clustering stationary samples with the DBSCAN algorithm. The cluster with the longest total stay between midnight and 6 a.m. was designated as the home location. To characterize general mobility patterns we extracted the number of clusters and the time spent across all clusters and specifically at home. We also computed the maximum distance between any pair of clusters as well as between home and other clusters to capture spatial relationships among significant locations. The radius of gyration defined as the average deviation of each cluster from the centroid of all clusters was used to quantify spatial dispersion. Lastly we calculated location entropy based on the distribution of time spent across clusters and extracted the time of day when participants were farthest from home to capture temporal aspects of their trajectories. 4 Smartphone and App Usage We first extracted the total number of unlocks and unlock duration to assess overall smartphone usage. To protect user privacy Sensor Kit did not record the names of third-party i OS apps but logged the usage time for each of 29 predefined app categories e.g. games news lifestyle. We consolidated these categories into 6 broader types productivity information social life health and other and computed the proportion of usage time for each type to reflect detailed usage patterns. 5 Typing Sensor Kit did not log any content typed by users. Instead it recorded metadata from typing events and keystrokes. To reduce variability introduced by keyboard layout we excluded all typing sessions in landscape orientation. We then extracted total typing duration and numbers of typing sessions and typed words as aggregate measures of overall typing activity. Additionally we computed the frequency of various typing events such as taps deletes altered words corrections and pauses relative to the word count to reflect participants typing dynamics. Beyond these aggregate features we derived keystrokelevel metrics potentially indicative of fine motor control and cognitive function. Specifically we extracted the hold time of character keys and estimated typing speed using the transition time between consecutive character inputs. We also obtained the transition time between character keys and deletes to capture self-correction behaviors. Typing accuracy was quantified by the spatial distance between each C. Passive Sensing Features From the raw sensor data we extracted 147 features to comprehensively characterize participants daily behaviors organized into 6 major categories described below. We first inferred participants timezones from their location data and partitioned the raw data into daily data frames. Behavioral features of each day were then computed from these data frames. As some participants traveled during the study period we excluded all days with multiple inferred timezones to avoid biasing the daily activity estimates. 1 Activity The i OS Core Motion framework recognizes activities including walking running cycling and automotive travel every few seconds. From these activity inferences we summarized the total daily duration of each activity to capture participants overall activeness. 2 Pedometer and Gait We extracted both high-level and granular features from the i Phone pedometer data. Daily total step count and walking distance were computed to quantify overall activity levels while we used the time of day when the first step was taken to reflect the timing of physical movement. To characterize participants walking patterns in detail we used the step timestamps to identify continuous walking periods of at least 10 seconds with more than 10 steps taken and calculated statistics for the step count distance cadence steps/second and pace seconds/meter across all such periods during each day. The statistics including the mean selected percentiles 5th 25th 50th 75th and 95th and median absolute deviation provided robust representations of the feature distributions. Furthermore we obtained the daily minimum average and maximum of several gait metrics from the built-in Health app including walking speed step length asymmetry and double support time. These features complemented the statistics derived from continuous walking periods to capture more nuanced aspects of naturalistic walking. Specifically walking asymmetry measures the proportion of steps with asymmetric speeds and double support time represents the percentage of the gait cycle with both feet on the ground. 3 Location To preserve privacy raw location coordinates were shifted to obfuscate participants true positions. Following established practices in location feature extraction we excluded low-quality samples recorded under unreliable signal conditions and classified the remaining ones as either stationary or moving. Specifically samples with an accuracy over 100 meters or an instantaneous speed exceeding 180 km/h were removed. A sample was considered stationary character keystroke and the center of the corresponding key. To construct interpretable daily features we applied the same set of summary statistics used in pedometer feature extraction to aggregate these keystroke-level measurements. 6 Communication As a privacy safeguard Sensor Kit does not collect the actual content of phone calls or text messages nor any identifiable information about contacts e.g. names or phone numbers. Therefore we summarized the number of incoming and outgoing calls and text messages total call duration and the number of unique contacts involved in these communications to examine participants social engagement. a bidirectional LSTM layer with 256 hidden units to produce a 512-dimensional representation for each day. The daily representations are then averaged across the time axis to obtain a global representation of the entire sequence. This global vector is passed through a Re LU-activated fully connected layer with 256 units and 0.2 dropout. Finally a classification head outputs the probability of cognitive impairment. C. Routine-Aware Augmentation Our data augmentation strategy leverages participants routines to generate synthetic day sequences in which each day is replaced with behaviorally similar alternatives. Specifically for each pair of days i j from a participant we computed the Euclidean distance Dij between their standardized sensing IV. A. Dataset Preparation Our goal was to develop a deep learning model to detect cognitive impairment based on participants behavioral trajectories derived from passive sensing. Similar to prior study window slicing was used to capture diverse temporal patterns while reducing variability from short-term events e.g. travel. Specifically we applied a 30-day sliding window to construct sequences of daily behavioral features and advanced the window by one day to maximize the number of available sequences. Participant-level estimates were then obtained by averaging probability predictions across all sequences from each participant. To ensure the features accurately reflected daily behavior we defined a valid day as one with at least 14 hours of sensing coverage between 6 a.m. and midnight. Sensing duration was also included in the feature set. Features were extracted only for valid days and a sequence was retained if it contained at least 23 valid days. We also excluded participants with fewer than 5 sequences for robust predictions. Missing feature values were imputed as zero after standardization. To align with the timing of cognitive assessments we focused on data collected during each participant s first 6 months of enrollment through March 2025. In total we constructed 3 351 sequences covering 5 115 unique days from 36 participants 12 of whom had cognitive impairment at baseline age 75.5 5.2 years education 18.2 1.5 years 6 females and contributed 981 sequences covering 1 595 days. The remaining 24 individuals were cognitively normal age 75.4 5.4 years education 16.3 1.9 years 14 females and contributed 2 370 sequences from 3 520 days. features vectors xi xj Rd Dij q Pd k 1 xi k xj k 2. For each day i we identified its 5 closest neighbors as replacement candidates Ci. To avoid substituting atypical days that deviate from routines with behaviorally dissimilar neighbors only neighbors with distances below a threshold τ were retained. We set τ as the 10th percentile of all pairwise distances Dij i j. Synthetic sequences were then generated by randomly sampling replacement days from Ci for each day i in the original sequence. Days without any valid replacements i.e. no candidates with distances below τ or sufficient sensing coverage were left unchanged. D. Demographic Personalization We developed a personalization method that preserves subject-level independence while utilizing data from all training participants. Specifically it reweights training samples based on demographic similarities between training and test participants. Each participant was represented by a standardized three-dimensional demographic vector d from their age sex and years of education. We then computed Euclidean distances Sij between di of the test participant i and dj of each training participant j. All training samples from participant j were assigned a weight wj using a softmax over the inverse distances to the test participant wj e1/Sij PM k 1 e1/Sik N where M is the number of training participants and N is the total number of training samples. This weighting scheme prioritizes training samples from participants demographically similar to the test subject while preserving the average weight of one across all samples to ensure comparability to uniform weighting. We further applied a softmax over the sample weights within each training batch to more effectively capture their relative importance. B. Classification Model E. Experiments We conducted a series of experiments to systematically evaluate the LSTM classifier and quantify the benefits of routine-aware augmentation and demographic personalization under a LOPO evaluation scheme. Model performance was assessed using both Area Under the ROC Curve AUC and Area Under the Precision-Recall Curve AUPRC for Fig. 1. Overall architecture of the LSTM model for detecting cognitive impairment from 30-day sequences of daily passive sensing features. We used an LSTM for binary classification. As illustrated in Figure 1 it first processes the 30-day input sequence using comparability with prior study. AUPRC emphasizes accurate predictions of the minority class and is therefore well suited for our imbalanced dataset which includes fewer participants with cognitive impairment i.e. the positive class. As a demographic baseline we fit a logistic regression on participants age sex and years of education. An XGBoost model was trained on summary statistics mean SD min max of the 147-dimensional passive sensing features computed over each 30-day sequence as a non-deep learning baseline. For the LSTM models we optimized the balanced cross-entropy loss using an Adam optimizer with a learning rate of 5 × 10 6 and a batch size of 128. To improve generalizability label smoothing with a factor of 0.1 was applied. The base LSTM was trained for 30 epochs. To evaluate the effect of routine-aware augmentation we generated 5 synthetic sequences for each real sequence increasing the training data size by 5 times. An LSTM model was then trained on the augmented dataset for 5 epochs to match the total number of optimization steps in the base setting for a fair comparison. We further trained an LSTM on the augmented dataset with demographic personalization to assess its additional contribution to model performance. In this case the final loss of a batch was computed as the sum of balanced cross-entropy losses per included sample each weighted by its personalization weight. To examine the impact of directly incorporating demographic context all three LSTM settings were repeated on a fused feature set where age sex and education were added as static inputs to each timestep of the passive sensing sequence. We reported both sequence-level and participant-level performance for the XGBoost and LSTM models. The deterministic logistic regression was trained with a single random seed while the others were trained with 10 different seeds. We used the same set of seeds across experiments to ensure fair comparison and reported the mean SD across seeds as a robust estimate of model performance. 0.660 to 0.671 and AUPRC from 0.604 to 0.623. More notably demographic personalization led to a substantial performance gain boosting AUC to 0.756 and AUPRC to 0.689. All improvements in AUC and AUPRC from the baselines to LSTM and with augmentation and personalization are statistically significant p.001 except for the increase of AUC from the demographic baseline to LSTM p 0.26. The benefits of augmentation and personalization were even more pronounced when sensing features were fused with demographic variables to train LSTMs. Augmentation improved participant-level AUC and AUPRC of the base model from 0.702 to 0.709 and from 0.637 to 0.654 respectively. Further personalization led to the best-performing model across all experiments achieving an AUC of 0.780 and an AUPRC of 0.766. To put this result in context Chen et al. reported an AUPRC of 0.701 using XGBoost classifiers trained on combined sensing and demographic features. Our models that incorporated demographic information also outperformed their counterparts trained on sensing features alone demonstrating the value of demographic context in detecting cognitive impairment. Again all performance improvements reported here are statistically significant. We further used the Gradient Explainer from Shapley Additive Explanations SHAP to identify important features utilized by the best-performing LSTM model for detecting cognitive impairment. Key contributors included higher education level longer character key hold and transition times during typing also reported in prior studies more smartphone unlocks and slower walking speed. B. Visualization of Participant Routines V. RESULTS A. Overall Performance Table I summarizes the classification performance across different combinations of feature sets and training settings. We used one-sided one-sample t-tests to compare model performance against the demographic baseline and one-sided paired t-tests to assess performance differences between other models. The models produced comparable results at the sequence and participant levels. At the participant level the demographic baseline achieved an AUC of 0.656 and AUPRC of 0.473 both exceeding the expected performance of random guessing with 0.5 for AUC and 0.33 i.e. prevalence of the positive class for AUPRC. The LSTM model trained on passive sensing features significantly outperformed the demographic and non-deep learning baselines in identifying participants with cognitive impairment yielding an average AUPRC of 0.604. This demonstrates its effectiveness in modeling fine-grained behavioral trajectories. Routine-aware augmentation further increased its AUC from Fig. 2. t-SNE visualization of participants daily passive sensing features from days with sufficient sensing coverage color-coded by participant ID. To visualize participants daily routines we obtained 4 384 unique days with sufficient sensing coverage from the 30-day sequences used in model development. Principal Component Analysis PCA was applied to the standardized daily features to retain 54 components that explained 95% of the total variance. We then used t-Distributed Stochastic Neighbor Embedding t-SNE to project these components into a two-dimensional space. Figure 2 illustrates the resulting embeddings color-coded by participant ID. The visualization revealed clearly identifiable participant clusters indicating the presence of routine behaviors across days. Specifically many participants exhibited distinct routines as reflected by their well-separated clusters. Others showed more similar behavioral patterns with clusters located TABLE I LOPO PERFORMANCE ACROSS DIFFERENT COMBINATIONS OF MODELS FEATURE SETS AND TRAINING SETTINGS. Aug DENOTES ROUTINE-AWARE AUGMENTATION AND Per INDICATES DEMOGRAPHIC PERSONALIZATION. BEST VALUES FOR EACH METRIC ARE BOLDED. Model Feature Set Setting AUC AUPRC Sequences Participants Sequences Participants Logistic Regression Demographics Base 0.656 0.473 XGBoost Sensing Base 0.518 0.030 0.505 0.034 0.331 0.031 0.389 0.037 LSTM Sensing Base 0.697 0.011 0.660 0.016 0.606 0.014 0.604 0.020 Base + Aug 0.701 0.011 0.671 0.015 0.612 0.013 0.623 0.021 Base + Aug + Per 0.814 0.010 0.756 0.010 0.727 0.031 0.689 0.026 LSTM Sensing + Demographics Base 0.735 0.023 0.702 0.025 0.603 0.023 0.637 0.025 Base + Aug 0.738 0.024 0.709 0.030 0.607 0.026 0.654 0.031 Base + Aug + Per 0.832 0.016 0.780 0.021 0.786 0.033 0.766 0.035 closer to each other near the center of the plot. Moreover atypical days that deviated from routines appeared as outliers relative to their corresponding clusters. These observations justified the design of our routine-aware augmentation which only replaced routine days with behaviorally similar alternatives when generating synthetic day sequences. They also provided empirical support for the effectiveness of this strategy in increasing the diversity of training data and enhancing model generalizability to unseen participants. leveraged demographic information by emphasizing behavioral patterns from individuals similar to the test participant. As described in Section IV-D the strategy employs a participant-level softmax and a batch-level softmax to derive sample weights from demographic similarity. In practice we found it critical to have both components to achieve the substantial performance improvement reported. While removing either softmax retained more than half of the original gain in AUC hardly any improvement was observed for AUPRC. This suggests that both demographicbased participant importance and the relevance of samples within each batch were effectively utilized through softmax normalization to adaptively prioritize more informative training samples especially for identifying participants with cognitive impairment i.e. the minority class. C. Demographic Analysis A. Future Directions We identified several directions for future research. First this work used behavioral features aggregated at the day level. Building on this foundation future work could examine behavioral trajectories at finer temporal scales. For example app usage is summarized every 15 minutes and physical activity is inferred every few seconds. Leveraging these higher-resolution time series may allow models to capture more nuanced behavioral signatures of cognitive decline. Second we required sufficient sensing coverage within each day and across the 30-day windows to ensure reliable daily feature extraction. However this criterion excluded several participants with inconsistent data collection. Notably since smartphone use can be cognitively demanding such inconsistencies may themselves carry information about cognitive function. Future research could explore event-based modeling approaches that do not rely on continuous sensing. For instance pedometer and typing data can be analyzed at the event level e.g. continuous walking periods or typing sessions enabling model development from collections of discrete behavioral episodes. Lastly it is essential to validate our modeling approach on both future participants from this ongoing study and independent external cohorts to establish its potential for real-world clinical deployment. Fig. 3. Scatter plots of age and education for male and female participants color-coded by cognitive status. The two participant groups were roughly matched in age and gender while those with cognitive impairment had approximately two more years of education on average. As reported in Section V-A the demographic baseline outperformed random guessing in detecting cognitive impairment and combining demographic variables with sensing features improved model performance. These findings suggest that demographic characteristics provide complementary information for detecting cognitive impairment. To further explore potential mechanisms underlying the performance gains from demographic personalization we visualized participants age and education stratified by sex and color-coded by cognitive status in B. Conclusion In this work we collected passive smartphone sensing data from older adults and extracted multimodal features to comprehensively characterize their daily behaviors. We then developed an LSTM classification model to detect cognitive impairment based on 30-day behavioral trajectories from 36 participants. To improve model generalizability and tailor it to individual-specific behavioral patterns we introduced two strategies routine-aware augmentation and demographic personalization. Evaluated with LOPO cross-validation these techniques jointly increased the participant-level AUPRC from 0.604 to 0.689 for the LSTM trained on sensing features alone and from 0.637 to 0.766 for the model trained on fused sensing and demographic features. Visualizations of participant routines and demographics provided additional empirical support for the effectiveness of the proposed strategies."
                                ],
                                [
                                    "2",
                                    "2510.05163v1",
                                    "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches",
                                    "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems enabling higher accuracy resilience to spoofing and seamless integration with hardware-based solutions. At the same time smart card technologies have evolved to include on-chip biometric verification cryptographic processing and secure storage thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work 2019 2025 at the intersection of deep learning biometrics and smart card technologies for MFA. We analyze biometric modalities face fingerprint iris voice review hardware-based approaches smart cards NFC TPMs secure enclaves and highlight integration strategies for real-world applications such as digital banking healthcare IoT and critical infrastructure. Furthermore we discuss the major challenges that remain open including usability security tradeoffs adversarial attacks on deep learning models privacy concerns surrounding biometric data and the need for standardization in MFA deployment. By consolidating current advancements limitations and research opportunities this survey provides a roadmap for designing secure scalable and user-friendly authentication frameworks. In today s hyperconnected digital environment safeguarding user identity has become more critical than ever. With increasing reports of data breaches phishing scams and account hijacking traditional password-based authentication has proven insufficient. Passwords are not only prone to being forgotten or reused but are also vulnerable to brute-force attacks phishing and database leaks. These vulnerabilities have driven the widespread adoption of Multi-Factor Authentication MFA as a more robust alternative. MFA relies on the combination of independent identity factors something the user knows e.g. a password something they have e.g. a smart card and something they are e.g. a biometric trait. This layered approach significantly improves security by ensuring that compromising one factor does not grant access to protected systems. According to recent surveys modern digital services including financial platforms compliant with regulations such as PSD2 increasingly require MFA to mitigate risks associated with single-factor systems. Privacy-preserving MFA approaches leveraging deep learning and biometrics have also been proposed. 1 Biometric authentication including modalities such as face fingerprint iris and voice recognition has emerged as a popular inherence factor in MFA systems. Biometrics offer nontransferable user-specific traits improving usability and reducing reliance on memory. Yet they raise unique challenges in terms of spoofing privacy and demographic bias. Recent advances in deep learning DL have significantly improved the accuracy and reliability of biometric systems. Convolutional Neural Networks CNNs and other deep architectures have shown strong performance in extracting robust features from noisy or occluded biometric data enabling real-time and multimodal biometric authentication. DL techniques also power liveness detection domain adaptation and template security enhancing resilience against spoofing and adversarial attacks. In parallel possession-based factors have evolved. Smart cards Trusted Platform Modules TPMs and secure enclaves now provide cryptographic computation biometric match-on-card and tamper-resistant credential storage. Recent advances in biometric payment cards and NFCenabled devices illustrate how hardware tokens can securely integrate with DL-based biometric authentication to form compact and user-friendly MFA schemes. These solutions reduce fraud while ensuring compliance with data protection requirements. Despite this progress several open challenges remain ensuring usability without degrading security defending against presentation attacks preserving biometric privacy and ensuring interoperability across vendors and regulatory frameworks. Objectives. This survey provides a comprehensive overview of recent research 2019 2025 on DL-based MFA systems integrating biometrics and smart cards. Specifically it Reviews deep learning methods applied to biometric authentication within MFA frameworks. Examines smart card and hardware-based approaches for secure factor integration. Compares architectures fusion strategies datasets and benchmarks employed in state-ofthe-art systems. Analyzes threat models and countermeasures against adversarial and spoofing attacks. Identifies open research questions and outlines future directions for scalable privacypreserving and user-friendly authentication. 2 Background MFA Biometrics and Smart Cards Multi-Factor Authentication MFA is a security mechanism that requires users to verify their identity using a combination of independent factors typically categorized as knowledge something the user knows possession something the user has and inherence something the user is. The rationale behind MFA is that compromising multiple independent factors is significantly harder for attackers thus enhancing overall system security. Traditionally authentication relied heavily on passwords. However due to their susceptibility to guessing phishing and large-scale leaks passwords alone have become increasingly risky. In response MFA has been adopted across domains handling sensitive data particularly digital banking e-government and healthcare Io T. Biometric authentication represents the inherence factor leveraging physiological traits e.g. face fingerprint iris or behavioral patterns e.g. gait keystroke dynamics. Unlike passwords or tokens biometric traits are intrinsic to individuals and difficult to replicate. However they raise challenges such as privacy concerns irrevocability and vulnerability to spoofing attacks. To mitigate these risks standardization bodies e.g. ISO/IEC 30107 have proposed presentation attack detection PAD guidelines and researchers are increasingly focused on fairness and robustness across demographics. 2 Smart cards typically associated with the possession factor are tamper-resistant hardware devices capable of securely storing credentials cryptographic keys and even performing biometric matching. In the banking sector EMV-compliant cards enable secure offline authentication through digital signatures. When combined with biometrics smart cards can implement matchon-card verification ensuring that sensitive templates never leave the card s secure chip. Beyond traditional smart cards Trusted Platform Modules TPMs and Secure Enclaves extend these guarantees to general-purpose devices such as smartphones and laptops. These components isolate sensitive data and computations enabling secure biometric enrollment inference and key storage and underpin modern standards such as FIDO2 and Web Authn. In practice effective MFA requires balancing usability cost and risk. A typical modern system may combine a fingerprint scan inherence a smartphone secure enclave or biometric smart card possession and a PIN or behavioral pattern knowledge/behavior. The integration of deep learning into biometric systems coupled with trusted hardware marks the next evolution in MFA explored in detail in the following sections. Deep learning DL has fundamentally transformed biometric authentication by enabling endto-end learning robust feature extraction and scalability across diverse modalities. Traditional biometric systems relied on handcrafted features which often lacked generalizability across populations or environmental conditions. DL models particularly Convolutional Neural Networks CNNs Recurrent Neural Networks RNNs and Transformers now power state-of-the-art systems for face fingerprint iris voice and behavioral biometrics. 3.1 Facial Recognition and Anti-Spoofing Facial recognition has rapidly advanced with architectures such as Face Net Arc Face and Cos Face which learn highly discriminative embeddings from images. Wang and Deng survey modern DL-based face recognition. However face systems remain vulnerable to spoofing 2D photos replay videos 3D masks. Liveness detection networks address this by analyzing texture motion cues or physiological signals e.g. eye blinking r PPG. Guo et al. proposed CNN-based liveness detection while recent works integrate anti-deepfake detection. 3.2 Fingerprint and Iris Recognition DL enhances fingerprint authentication by improving minutiae detection ridge classification and partial print matching. Zahid et al. show CNNs outperform traditional Gabor-based methods under noisy conditions. Similarly iris recognition benefits from CNNs and attentionbased models trained on datasets such as CASIA-Iris and ND-Iris robust to illumination and pupil dilation. 3.3 Voice and Behavioral Biometrics DL also advances speaker verification via spectrogram-based CNNs and LSTM embeddings. Combining voice with face audiovisual biometrics strengthens robustness for remote banking authentication. Behavioral biometrics keystroke gait touchscreen mouse movement enable continuous MFA. Verma et al. demonstrate smartphone motion-based DL models for adaptive MFA. 3 algorithms. The synergy between biometric recognition and possession-based hardware is foundational to secure MFA. 4.1 Smart Cards in MFA Systems Smart cards have long been used in authentication due to their ability to securely store user credentials and perform local computations. In modern MFA biometric smart cards BSCs integrate fingerprint or facial recognition sensors directly into the card or terminal. There are two main architectures Match-on-Card Mo C Biometric matching is performed entirely on the card s chip and the template never leaves the card. This ensures maximum privacy. Match-off-Card The biometric is matched externally with the template read from the card. This mode is more flexible but less private. Mo C provides superior privacy and is increasingly supported by commercial products such as Idemia s biometric payment cards and Gemalto s biometric EMV solutions with technical standards maintained by EMVCo. 4.4 Smart Card + DL System Architectures Recent architectures combine deep learning with secure hardware to enhance biometric verification DL on-chip Miniaturized CNNs embedded in smart cards or tokens. Secure template fusion Combining multiple traits e.g. fingerprint + iris with fused templates stored in secure elements. On-device adaptation DL models fine-tuned per user during enrollment stored within TEEs. Tani et al. 2025 validated the feasibility of such architectures for real-world banking authentication. 4.5 Challenges in Hardware-Based MFA Despite their advantages hardware-integrated MFA systems face challenges Cost and Scalability Biometric smart cards are more expensive than traditional tokens. Hardware Standardization Fragmentation across vendors complicates integration. Energy Constraints DL inference on low-power chips requires lightweight models and quantization. Ongoing research explores energy-efficient DL models and secure co-processors to address these limitations. The integration of deep learning DL based biometric authentication with traditional MFA systems introduces several design options depending on modality fusion user experience UX requirements and environmental constraints. This section discusses core system-level integration strategies such as fusion techniques adaptive MFA policies and UX considerations like latency. 5.1 Fusion Techniques in DL-Based MFA To leverage the strengths of multiple authentication factors especially in multimodal biometrics systems often use fusion strategies to combine different sources of evidence. Fusion can occur at various levels Sensor-Level Fusion Raw biometric signals e.g. fingerprint + face are captured and preprocessed jointly. Feature-Level Fusion Deep embeddings from CNN/RNN models are concatenated before classification. Score-Level Fusion Independent DL models output match scores that are weighted and combined. Decision-Level Fusion Each modality votes independently a decision is made based on predefined logic e.g. majority voting. Score-level fusion offers a balance between flexibility and performance and is widely used in commercial systems. Decision-level fusion is preferred in scenarios with hardware heterogeneity or legacy compatibility. 7 6.5 Summary DL-based MFA improves identity assurance but still faces challenges in adversarial robustness deepfake resistance privacy and bias mitigation. The future of trustworthy MFA depends on standardized testing open datasets and secure hardware-software co-design. 6.6 Standardization and Interoperability DL-MFA systems often combine heterogeneous sensors inference engines and secure hardware. Without standards integration is fragile and error-prone. Key standards include FIDO2/Web Authn ISO/IEC 30107 PAD and EMVCo for biometric payment cards. Future work must emphasize pluggable frameworks formal verification of workflows and government-led certification e.g. e IDAS NIST 800-63. 6.7 Summary While DL-based MFA significantly strengthens digital identity protection its adoption introduces challenges in robustness privacy fairness and interoperability. Addressing these threats requires cross-disciplinary efforts spanning ML research hardware security regulatory compliance and usability studies. 7 Datasets Benchmarks and Metrics Robust evaluation of DL-based MFA systems requires standardized biometric datasets and consistent performance metrics. This section presents widely used benchmarks for facial fingerprint iris and multimodal biometrics and outlines key evaluation criteria such as FAR FRR and EER. 7.1 Benchmark Datasets for Biometric MFA Research in DL-powered biometric authentication relies on curated datasets that represent different modalities under diverse conditions. The quality and bias of these datasets significantly influence model generalizability. Well-established datasets underpin biometric research. Examples include LFW VGGFace2 Age DB CASIA-Iris V4 and FVC2004 which have become de facto benchmarks for DL-based evaluation. Many of these datasets include variations in lighting pose aging and acquisition devices to simulate real-world conditions. Public availability supports benchmarking and fair comparison across DL architectures. 7.2 Performance Metrics in MFA Systems Accurate evaluation of DL-based MFA systems requires standardized biometric metrics and protocols as defined in ISO/IEC 19795 and NIST SP 800-63B. These standards ensure comparability across algorithms datasets and deployment environments. False Acceptance Rate FAR Probability that an impostor is incorrectly accepted as a genuine user. FAR is critical for measuring system security and is often reported at operating points such as FAR 10 3 or 10 4. False Rejection Rate FRR Probability that a legitimate user is incorrectly rejected. FRR reflects system usability and user experience. Equal Error Rate (EER): The rate at which FAR and FRR are equal, often used as a single scalar indicator of performance. Lower EER implies a more accurate system. Receiver Operating Characteristic (ROC): A curve plotting the trade-off between FAR and True Positive Rate (1–FRR). Widely used to visualize model discriminability. Detection Error Tradeoff (DET): A log-scaled version of the ROC curve emphasizing low-error regions, recommended by ISO/IEC 19795 for biometric evaluations. Failure to Enroll (FTE) / Failure to Acquire (FTA): Rates at which biometric data cannot be captured or enrolled successfully, critical for deployment evaluation. Authentication Latency: The time required to complete an MFA process (biometric + token verification). Recommended latency for practical systems is below 1.5 seconds. In multimodal DL-MFA, researchers also evaluate: Fusion Gain: Improvement in EER or accuracy when combining multiple modalities compared to single-modality baselines. Robustness to Noise and Spoofing: Performance degradation under environmental noise, aging, or adversarial conditions. Template Security Impact: Trade-offs between encryption, privacy-preserving operations, and system accuracy. Public benchmarks such as FVC2004, CASIA-IrisV4, and AgeDB are typically evaluated using these metrics under standard protocols. As digital ecosystems expand and cyber threats evolve, MFA has become a central pillar of secure access control. This survey reviewed how deep learning, biometric modalities, and hardware tokens such as smart cards and secure enclaves can converge to build the next generation of MFA systems. Improvements include accuracy, liveness detection, and multimodal fusion, while challenges include robustness, privacy, fairness, and interoperability."
                                ],
                                [
                                    "3",
                                    "2510.05736v1",
                                    "Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes",
                                    "Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes. The identification of γ-rays from the predominant hadronic-background is a key aspect in their ground-based detection using Imaging Atmospheric Cherenkov Telescopes IACTs. While current methods are limited in their ability to exploit correlations in complex data deep learning-based models offer a promising alternative by directly leveraging image-level information. However several challenges involving the robustness and applicability of such models remain. Designing model architectures with inductive biases relevant for the task can help mitigate the problem. Three such deep learning-based models are proposed trained and evaluated on simulated data 1 a hybrid convolutional and graph neural network model CNN-GNN using both image and graph data 2 an enhanced CNN-GNN variant that incorporates additional reconstructed information within the graph construction and 3 a graph neural network GNN model using image moments serving as a baseline. The new combined convolution and graph-based approach demonstrates improved performance over traditional methods and the inclusion of reconstructed information offers further potential in generalization capabilities on real observational data. The field of very-high-energy VHE γ-ray astronomy has evolved significantly over the past three decades driven largely due to observations from ground-based imaging telescopes. These Imaging Atmospheric Cherenkov Telescopes IACTs capture Cherenkov light produced by highly energetic particles interacting with the Earth s atmosphere. Each camera image represents a two-dimensional projection of the Cherenkov light pool from the telescope s position and each event typically consists of multiple images taken simultaneously across an array of telescopes. This stereoscopic information is used to reconstruct the energy direction and type of the primary particle initiating the air shower. Event statistics however are dominated by hadron-induced air showers which can outnumber γ-ray air showers by up to a factor of 104. This makes the task of identifying γ-ray events from the hadronic background a central challenge for IACT-based observations. Hadronic air showers are fundamentally different from those initiated by γ-rays which are electromagnetic in nature. This difference in shower development leads to small but significant differences in the camera images which form the basis for separating the two event classes. Currentgeneration IACTs typically rely on Boosted Decision Trees BDTs trained on parameterized image features or goodness-of-fit parameters for this task 3 5. Consequently a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Multiple studies have explored deep learning methods for identifying γ-rays and demonstrated exceptional performance on simulated data 6 10. Most model architectures use convolutional neural networks CNNs for extracting information from camera images and recurrent neural networks RNNs for its aggregation across an event. Graph neural networks GNNs applied on images represented as point clouds have also been established as a viable approach for the same task. Despite their potential a complete deployment of deep learning-based models on IACT data remains non-trivial due to a variety of issues ranging from observational systematics to discrepancies between simulations and real-world data. The construction of models that can generalize to unseen situations is a long-standing problem in deep learning. The use of network architectures with inductive biases suitable for the given task can lead to improved generalizations by guiding the learning process towards more physically meaningful representations. For example translational invariance in CNNs temporal dependence in RNNs and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to thus contributing to their success in their domains. In the context of IACTs as each image is a projection of the same event there is no inherent ordering between them making them permutation equivariant. This motivates a shift towards exploring GNNs for aggregating information across multiple telescopes. To that end this work introduces a combined convolutional and graph neural network CNN-GNN based approach for γ/hadron separation in IACTs. Three models with two distinct training strategies are proposed and evaluated on simulated data. These include a CNN-GNN model trained on image and graph data an enhanced CNN-GNN variant with additional reconstructed event information incorporated into the graph structure and a baseline GNN model utilizing image moments serving as a reference for existing methods. The models were trained on simulations of the High Energy Stereoscopic System H.E.S.S. located in Go llschau Namibia. The H.E.S.S. array consists of four 12-meter telescopes CT1-4 arranged in a square of side 120 meters with an additional 28-meter telescope CT5 at its center. Diffuse proton and γ-ray events simulated at a zenith of 20 and with a maximum view cone of 5 were chosen as the two event classes for this task. Similar models were also trained on the simulations of the upcoming Cherenkov Telescope Array Observatory the results of which are not presented here. A custom data processing pipeline based on ctapipe and Py Torch was developed for the subsequent analysis of IACT data. The framework within the ctapipe v0.19.3 package was used to handle the low-level data processing tasks such as image calibration and cleaning while the construction and training of the convolution and graph-based models were implemented using Py Torch v2.0.1 and Py Torch Geometric v2.4.0 respectively. γ-ray or hadron-induced based on the event images and information. To this end the aim of the convolutional CNN half of the model remains to extract relevant image-level features from each telescope while the graph-based GNN half is given additional contextual information described in Section 3 to better learn the classification task. The output of the CNN also forms part of the input to the GNN allowing the model to combine image features with telescope-level information during training. A key advantage of the graph representation in GNNs is its flexibility in handling varying numbers and types of telescopes present in IACT arrays. This is also relevant for when subsets of the array operate due to technical or observational constraints. Additionally the graph approach also allows for inclusion of telescope-specific information such as position reconstructed parameters temporal relationships making the system more adaptable to the particularities of real-world data. The CNN component is a simple implementation inspired from the Inception module employing convolution filters of multiple sizes in parallel to learn features at different spatial scales. Five convolutions with kernel sizes ranging from 1 to 15 are applied to each image and the output subsequently passed through two additional convolutional layers with max-pooling. The result is then flattened and processed through two fully connected layers with dropout regularization applied before the final layer. The network operates on images from each telescope independently and returns a fixed-dimensional embedding for each telescope. The feature vectors extracted by the CNN are subsequently passed to the GNN component which consists of a multi-layer Edge Conv architecture with residual concatenation across layers following the original design. After message passing the learned node representations are aggregated globally using a combination of mean max and sum pooling operations to form a graph-level event embedding which is then used for classification. Alternative GNN layers such as GCNConv and GATConv were also explored but no significant difference in performance was observed. A key aim of the work was to examine if the inclusion of reconstructed information in the model training contributed to improvements in performance. Consequently three models with different GNN components were developed a Fast CNN-GNN a Strong GNN and a Hillas GNN. The specifics followFast CNN-GNN Combines features extracted from camera images with telescope positions in the ground frame total image and maximum pixel intensity per image. Strong CNN-GNN An enhanced version which incorporates the reconstructed core position of the air shower. This is implemented by shifting the telescope positions in the tilted ground frame1 by centering the origin at the reconstructed impact point Figure 2. Hillas GNN A simplified GNN trained solely on Hillas parameters extracted from the camera images serving as the baseline model. 1The tilted ground frame is the ground frame transformed according to the telescope s pointing direction and is used for reconstructing the shower core position. corresponding to the area under curve AUC values in each case. All models are successfully able to learn to classify events between γ-rays and protons and the CNN-GNN models outperform the Hillas-based GNN model as well. Upon testing on a dataset with an additional local distance cut on images as in the CNN-GNN approach matches the classification performance of the previous models. Similar results were also seen when the same approach was applied and evaluated on simulations of the CTAO array. where a higher Δ AUC fscore indicates a greater importance of that feature to the model s decision process. The results of this computation for the Hillas GNN n 50 and the Fast CNN-GNN n 10 model trained via the split-training approach are shown. The applicability of deep learning-based models on IACT observational data remains a key challenge for several reasons. The observational conditions under which such data is taken are far more diverse than those that can be simulated. Additionally, physical uncertainties introduce further discrepancies between simulated and actual observations, challenging the ability of deep learning models to generalize under such conditions. This work addresses these issues by exploring model architectures that include inductive biases aligned with the physical structure of IACT data. Convolution and graph-based methods are therefore explored for the classification of events between gamma rays and the hadronic background. The proposed hybrid CNN-GNN models show improvements in classification over parametrized approaches and match the performance of previous deep learning models, establishing the technique as a viable approach. Furthermore, the inclusion of reconstructed information within the graph representation appears beneficial and shows potential in enhancing both performance and generalization. The natural next step is to evaluate these models on real observations, which will provide a more definitive test of their viability and robustness with real-world IACT data. Such evaluation will offer insight into how effectively inductive-bias-driven architectures can bridge the gap between simulations and real observations, contributing to next-generation gamma/hadron separation strategies."
                                ],
                                [
                                    "4",
                                    "2510.07320v1",
                                    "Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children",
                                    "Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children. Autism Spectrum Disorder ASD significantly influences the communication abilities learning processes behavior and social interactions of people. Although early intervention and customized educational strategies are critical to improving outcomes there is a pivotal gap in understanding and addressing nuanced behavioral patterns and emotional identification in autistic children prior to skill development. This extended research delves into the foundational step of recognizing and mapping these patterns as a prerequisite to improving learning and soft skills. Using a longitudinal approach to monitor emotions and behaviors this study aims to establish a baseline understanding of the unique needs and challenges faced by autistic students particularly in the Information Technology domain where opportunities are markedly limited. Through a detailed analysis of behavioral trends over time we propose a targeted framework for developing applications and technical aids designed to meet these identified needs. Our research underscores the importance of a sequential and evidence-based intervention approach that prioritizes a deep understanding of each child s behavioral and emotional landscape as the basis for effective skill development. By shifting the focus toward early identification of behavioral patterns we aim to foster a more inclusive and supportive learning environment that can significantly improve the educational and developmental trajectory of children with ASD. Emotion recognition has emerged as a critical research domain in artificial intelligence with particular complexity arising in Autism Spectrum Disorder ASD contexts due to unique emotional expression characteristics in this population. Traditional Machine Learning Era Early emotion recognition systems relied on conventional methods including Support Vector Machines decision trees and random forests achieving moderate success with 75-85% accuracy on standard datasets. However these approaches fundamentally depend on handengineered features creating significant limitations when processing high-dimensional facial data particularly problematic for ASD applications where emotional expressions deviate from typical patterns. Deep Learning Revolution Convolutional Neural Networks revolutionized the field by enabling automatic feature extraction and hierarchical learning. Stateof-the-art CNN architectures consistently achieve 90-95% accuracy on neurotypical datasets FER-2013 CK+ through effective spatial hierarchy capture in facial expressions establishing CNNs as the standard approach for emotion classification. ASD-Specific Challenges Despite deep learning success in general emotion recognition ASD applications face substantial obstacles. Children with ASD exhibit significantly. Autism Spectrum Disorder ASD is a developmental condition that significantly affects a person s ability to communicate interact socially and express or interpret emotions. For children with ASD these challenges often manifest as atypical facial expressions and unusual behavioral patterns making it especially difficult for caregivers educators and even advanced computational systems to accurately recognize their emotional states. Yet effective emotion recognition is vital it underpins tailored interventions social support and the overall quality of life for children on the spectrum. Recent advances in artificial intelligence have led to the development of deep learning models particularly the Xception and Inception architectures which have shown remarkable performance in facial emotion detection. These models are highly effective in extracting and analyzing subtle features from facial images enabling more precise classification of emotional expressions higher variability in emotional expression quantified through facial expression variance motion dynamics and temporal inconsistencies. This complexity causes dramatic performance degradation with CNN accuracy dropping from 95% on neurotypical data to 70-75% on ASD-specific datasets. Transfer Learning Solutions To address small ASD dataset limitations researchers extensively explored transfer learning using pre-trained models VGG16 Res Net Inception architectures leveraging large-scale image dataset knowledge for specialized ASD tasks. These approaches demonstrate 5-10% accuracy improvements over training from scratch. Advanced architectures like Xception utilizing depthwise separable convolutions for computational efficiency and Inception V3 enabling multiscale feature extraction have gained prominence for their sophisticated feature extraction capabilities. Autoencoder Integration Autoencoders have emerged as powerful dimensionality reduction and feature extraction tools demonstrating effectiveness in filtering irrelevant features such as background noise and lighting variations. Their ability to learn compressed representations while preserving essential spatial information makes them particularly suitable for preprocessing heterogeneous datasets. Multiple studies have shown autoencoder frameworks improve model robustness in challenging environments with their capacity to standardize input data while preserving critical facial features being especially beneficial for ASD emotion recognition applications. Research Gaps and Future Directions Despite significant technological progress substantial gaps remain in ASD-specific applications. The heterogeneity within ASD populations challenges generalized recognition system development with most existing studies focusing on neurotypical datasets and limited research addressing unique ASD emotional expression characteristics. The lack of standardized ASD emotion datasets complicates comparative methodology evaluation. Contemporary trends indicate growing interest in personalized recognition systems adapting to individual expression patterns with integration of multiple preprocessing techniques and advanced CNN architectures representing promising directions for improved ASD emotion recognition accuracy. The literature establishes a clear evolutionary trajectory from traditional machine learning through deep learning advances to contemporary hybrid systems with autoencoder-enhanced preprocessing emerging as a mathematically sound approach to addressing the unique challenges of ASD emotion recognition. Fig. 1. Structure of a Auto Encorder B. Dataset The dataset comprises facial expressions of ASD children collected from clinical settings educational environments and specialized databases. Images exhibit dimensional variability with aspect ratios ranging from 0.75 to 1.33 significantly deviating from Image Net standards. C. Autoencoder Preprocessing Architecture To address the size mismatch we implement an autoencoder that maps variable-sized inputs to the required 299×299×3 format while preserving facial features essential for emotion recognition. C.1 Mathematical Formulation Encoder Maps variable input to fixed latent representation z fθ x σe Wex + be 1 Decoder Reconstructs standardized 299×299×3 output ˆx gφ z σd Wdz + bd 2 C.2 Loss Function The autoencoder employs composite loss balancing reconstruction and spatial preservation LAE T x ˆx 2 2 + λ1 V GGj T x V GGj ˆx 2 2 + λ2 Flandmark T x Flandmark ˆx 2 2 3 where T x is ground truth resized image V GGj extracts perceptual features and Flandmark preserves facial landmarks. D. Classification Models D.1 Xception Architecture Utilizes depthwise separable convolutions decomposing standard convolution into Depthwise Convolution III. Y dw i j c X m n W dw m n c Xi+m 1 j+n 1 c 4 A. Problem Statement Pointwise Convolution Xception and Inception V3 models pre-trained on Image Net dataset require fixed input dimensions of 299×299×3 pixels.ASD children s facial expression datasets contain variablesized images 150×200 to 800×600 pixels creating a fundamental mismatch that degrades model performance through aspect ratio distortion and information loss. c W pw c k Y dw i j c 5 Yi j k X This reduces computational complexity from O H W C K2 F 6a F. Experimental Setup to O H W C K2 + F 6b F.1 Training Protocol The training process employs a two-stage strategy to optimize both pre-processing and classification components. During the first stage the autoencoder undergoes pretraining for 100 epochs with a batch size of 32 learning to map variable-sized inputs to standardized 299×299×3 outputs while preserving essential facial features. The second stage involves end-to-end fine-tuning where the complete pipeline undergoes joint optimization allowing the autoencoder and classification models to adapt collaboratively for optimal emotion recognition performance. F.2 Comparative Analysis The effectiveness of the proposed approach is evaluated through rigorous comparison between baseline and enhanced methodologies. The baseline approach applies direct resizing transformation xbaseline resize x 299 299 to convert variable-sized images to the required input dimensions. In contrast the enhanced approach utilizes autoencoder preprocessing xenhanced gφ fθ x to generate standardized inputs that preserve spatial relationships and semantic content. Performance evaluation encompasses accuracy precision recall and F1-score metrics across both approaches with statistical significance assessed through paired t-tests to validate improvement claims. F.3 Implementation Details The experimental implementation utilizes NVIDIA V100 32GB GPU hardware to handle the computational demands of training both autoencoder and classification components. The framework employs TensorFlow/Keras for model implementation and training orchestration. Data augmentation strategies include random rotation within 15 degrees and brightness/contrast adjustments within 0.2 range to improve model generalization. Training stability is maintained through early stopping mechanisms that monitor validation loss plateaus preventing overfitting while ensuring optimal convergence. This comprehensive methodology systematically addresses the fundamental dimensional mismatch between variable-sized ASD emotion datasets and the fixed-input requirements of Image Net-trained deep learning models. The approach enables improved classification accuracy through learned preprocessing techniques while preserving the semantic facial features essential for accurate emotion recognition in children with autism spectrum disorders. Fig. 2. Xception Model Structure D.2 Inception V3 Architecture Employs multi-scale feature extraction through parallel convolutions Finception Concat F1×1 F3×3 F5×5 Fpool 7 Factorized convolutions improve efficiency 5 × 5 convolutions replaced by two 3 × 3 operations and asymmetric factorization n × n 1 × n + n × 1. Fig. 3. Inception V3 Model Structure E. Training Framework E.1 Loss Functions Classification Loss Cross-entropy for 4-class emotion recognition N X 4 X LCE 1 IV. RESULTS AND DISCUSSION j 1 yi j log pi j 8 N The experimental evaluation was conducted to assess the effectiveness of autoencoder preprocessing on emotion recognition performance using ASD children datasets. Two state-ofthe-art deep learning architectures Xception and Inception V3 were evaluated under both baseline conditions without preprocessing and enhanced conditions with autoencoder preprocessing. The results demonstrate substantial and statistically significant improvements across all performance metrics. i 1 Total Loss Ltotal LAE + αLCE E.2 Optimization Adam optimizer with adaptive moments to address the loss function regarding classification loss mt β1mt 1 + 1 β1 gt vt β2vt 1 + 1 β2 g2 t θt θt 1 α vt + ε ˆmt 9 A. Performance Comparison Analysis Table I presents the comprehensive performance comparison between baseline and autoencoder-enhanced models. The reParameters α 0.001 β1 0.9 β2 0.999 ε 10 8. C. Error Rate Reduction Analysis sults reveal consistent and substantial improvements across both architectures when autoencoder preprocessing was integrated into the pipeline. A particularly noteworthy finding was the substantial reduction in classification errors. The Xception model achieved a 48.0% reduction in error rate from 27.7% to 14.4% while Inception V3 demonstrated a 44.1% reduction from 29.0% to 16.2%. TABLE I PERFORMANCE COMPARISON BETWEEN XCEPTION AND INCEPTIONV3 MODELS WITH AND WITHOUT AUTOENCORDERS Model Xception Inception V3 Accuracy Baseline 72.3% 71.0% Accuracy Autoencoder 85.6% 83.8% Precision 0.82 0.81 Recall 0.84 0.82 F1-Score 0.83 0.82 The Xception model achieved the highest overall performance with 85.6% accuracy after autoencoder preprocessing representing a 13.3 percentage point improvement over the baseline 72.3%. Similarly Inception V3 demonstrated significant enhancement reaching 83.8% accuracy compared to the baseline performance of 71.0% corresponding to a 12.8 percentage point improvement. Fig. 5. Statistical significance analysis showing very large effect sizes and substantial error rate reductions. These reductions represent significant practical improvements in model reliability for emotion recognition in ASD children. D. Confidence Interval Analysis The 95% confidence intervals for accuracy improvements provide robust bounds on the true performance enhancements. For Xception the confidence interval 10.6% 16.0% and for Inception V3 10.1% 15.5% both exclude zero with substantial margins confirming the reliability of the observed improvements. E. Consistency Across Architectures Fig. 4. Performance Comparison Baseline vs Autoencoder-Enhanced Models. Table III presents the comprehensive analysis of improvement consistency across different neural network architectures demonstrating the robustness of the autoencoder preprocessing approach. B. Statistical Significance Evaluation The statistical rigor of the observed improvements was validated through multiple complementary analyses. Table II summarizes the comprehensive statistical evaluation confirming the significance and magnitude of the performance enhancements. TABLE III COMPREHENSIVE PERFORMANCE IMPROVEMENT ANALYSIS Metric Xception Inception V3 TABLE II STATISTICAL SIGNIFICANCE ANALYSIS OF MODEL IMPROVEMENTS Mean Improvement 13.3% 12.8% Standard Deviation 0.004 0.004 Coefficient of Variation % 2.7 2.7 Effect Size Cohen s d 2.66 2.56 Statistical Power % 99.9 99.9 NNT samples 7.5 7.8 Model Accuracy Improvement % Error Reduction % Xception 13.3 18.4 2.66 Very Large 0.001 10.6 16.0 48.0 Inception V3 12.8 18.0 2.56 Very Large 0.001 10.1 15.5 44.1 Relative Improvement % Cohen s d Effect Size p-value CI Lower % CI Upper % The Cohen s d values of 2.66 and 2.56 for Xception and Inception V3 respectively indicate very large effect sizes substantially exceeding Cohen s threshold for large effects d 0.8. The p-values 0.001 demonstrate highly significant differences with statistical power exceeding 99.9% for both models. The extremely low coefficient of variation 2.7% indicates remarkable consistency in improvement magnitude across different architectures suggesting that the benefits of autoencoder preprocessing are architecture-independent. F. Magnitude and Significance of Improvements emotion classification is achieved compared to the baseline system. This represents significant practical value in clinical and educational settings where accurate emotion recognition is crucial for effective intervention strategies. The 95% confidence intervals provide clinicians and researchers with reliable bounds on expected performance improvements. Even the lower confidence bounds 10.1-10.6% substantially exceed typical minimum clinically important differences 2-5% for machine learning applications in healthcare. The experimental results provide compelling evidence for the effectiveness of autoencoder preprocessing in enhancing emotion recognition performance for ASD children. The observed improvements of 13.3% Xception and 12.8% Inception V3 in accuracy represent substantial enhancements that significantly exceed typical performance variations in deep learning models. The Cohen s d effect sizes 2.66 and 2.56 are exceptionally large by conventional standards placing these improvements among the most substantial reported in emotion recognition literature. These effect sizes indicate that the differences between baseline and enhanced models are not only statistically significant but represent practically meaningful improvements with real-world implications for ASD emotion recognition systems. J. Computational Efficiency Considerations The cost-benefit analysis reveals an exceptional efficiency ratio of 9 1 where the 10-20% increase in computational overhead from autoencoder preprocessing yields an 18% improvement in accuracy. This favorable trade-off makes the approach highly practical for real-world deployment in resourceconstrained environments. The preprocessing standardization also contributes to improved system reliability by ensuring consistent input formatting reducing variability due to image quality differences lighting conditions and background clutter that commonly affect emotion recognition systems in natural settings. G. Statistical Robustness and Reliability The statistical analysis demonstrates overwhelming evidence for the significance of the observed improvements. The p-values 0.001 indicate that the probability of observing such large improvements by chance alone is less than 0.1% providing strong evidence against the null hypothesis of no improvement. The Mc Nemar s test results χ2 98.01 p 0.001 further confirm the statistical significance when comparing the paired predictions of baseline versus enhanced models. This non-parametric test is particularly appropriate for comparing the accuracy of two classification models on the same dataset providing robust validation of the improvements. The high statistical power 99.9% ensures that the study design was capable of detecting meaningful differences eliminating concerns about insufficient sample sizes or inadequate experimental design. This study rigorously evaluated the impact of autoencoderbased preprocessing on emotion recognition systems for children with Autism Spectrum Disorder ASD leveraging two advanced convolutional neural network architectures Xception and Inception V3. The integration of autoencoders as a preprocessing step resulted in substantial and consistent improvements across all key performance metrics including accuracy precision recall and F1-score. Quantitatively the observed effect sizes Cohen s d 2.5 absolute accuracy increases of over 13 percentage points and error rate reductions approaching 50% collectively underscore a practical and statistically significant enhancement in model performance. H. Mechanistic Understanding of Enhancement The consistent improvement pattern across different architectures coefficient of variation 2.7% suggests that autoencoder preprocessing addresses fundamental challenges in emotion recognition rather than architecture-specific limitations. The mathematical model of improvement A. Generalizability and Robustness Enhanced Accuracy Baseline Accuracy + δ 10 A major strength of these findings lies in their demonstrated generalizability. The consistency of improvements across two distinctly different architectures Xception s depthwise separable convolutions and Inception V3 s multi-scale processing offers compelling evidence that the benefits of autoencoder preprocessing extend beyond the idiosyncrasies of specific models. The low variance in accuracy improvement standard deviation 0.004 further implies that these enhancements are robust and likely applicable to a diverse range of convolutional neural network structures. The additive mathematical model supporting these results suggests that autoencoder preprocessing resolves universal challenges inherent to emotion recognition rather than exploiting architecturedependent features indicating strong potential for broad applicability including domains beyond ASD. where δ 0.131 0.004 provides a quantitative framework for understanding the additive nature of the enhancement. The substantial error rate reductions 44 48% indicate that autoencoder preprocessing effectively filters noise and irrelevant background information allowing the deep learning models to focus on critical facial features essential for emotion recognition. This noise reduction mechanism is consistent with the theoretical framework of autoencoders as optimal lossy compression algorithms that preserve task-relevant information while discarding extraneous details. I. Practical Implications for ASD Applications The Number Needed to Treat NNT values of 7.5-7.8 samples indicate that for approximately every 8 children processed through the enhanced system one additional correct B. Comparison with Literature Y. Zhang A better autoencoder for image Convolutional autoencoder. The improvement metrics registered in this study particularly the exceptionally large effect sizes are among the highest reported to date in emotion recognition research. Most comparable works report only modest gains Cohen s d 0.2 0.5 whereas the present work achieved much greater impact. Furthermore the cross-architecture validation available here addresses a common gap in the literature where such findings are typically reported for a single deep learning model. This strengthens the external validity and distinguishes this work in the field. VI. FUTURE WORK Despite the strong results several limitations warrant attention. The statistical analyses are based on assumptions regarding data distribution and sample size thus confirmatory studies utilizing larger and more heterogeneous datasets are recommended to further substantiate these findings. Although the study included two prominent neural network architectures investigating additional frameworks such as Vision Transformers and Res Net families would further confirm the generalizability and scalability of the approach. Future research should also explore the customization and optimization of autoencoder architectures specific to emotion recognition with the presented quantitative model especially the improvement parameter δ offering a solid starting point for such endeavors. A. Clinical Translation Potential The magnitude and consistency of the enhancements combined with high computational efficiency and rigorous statistical validation suggest strong potential for clinical translation. The consistent and predictable gains across architectures minimal additional computational cost and robust statistical performance collectively lower the risk for clinical implementation. If deployed the method could meaningfully improve emotion recognition support systems for children with ASD with significant implications for intervention effectiveness and broader applications in affective computing. In summary autoencoderbased preprocessing constitutes a transformative strategy for elevating the accuracy reliability and robustness of emotion recognition models in challenging real-world scenarios. The clear statistically robust improvements from this work provide a methodological and practical foundation for further research and real-world deployment in healthcare and other emotionsensitive domains."
                                ]
                            ],
                            "shape": {
                                "columns": 3,
                                "rows": 5
                            }
                        },
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>title</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>2509.20913v1</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2509.23158v1</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2510.05163v1</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>2510.05736v1</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2510.07320v1</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "             id                                              title  \\\n",
                            "0  2509.20913v1  Deep Learning for Crime Forecasting: The Role ...   \n",
                            "1  2509.23158v1  Deep Learning-Based Detection of Cognitive Imp...   \n",
                            "2  2510.05163v1  Deep Learning-Based Multi-Factor Authenticatio...   \n",
                            "3  2510.05736v1  Convolution and Graph-based Deep Learning Appr...   \n",
                            "4  2510.07320v1  Deep Learning Based Approach to Enhanced Recog...   \n",
                            "\n",
                            "                                                text  \n",
                            "0  Deep Learning for Crime Forecasting: The Role ...  \n",
                            "1  Deep Learning-Based Detection of Cognitive Imp...  \n",
                            "2  Deep Learning-Based Multi-Factor Authenticatio...  \n",
                            "3  Convolution and Graph-based Deep Learning Appr...  \n",
                            "4  Deep Learning Based Approach to Enhanced Recog...  "
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# checking the corpus\n",
                "df = pd.DataFrame(corpus)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "828da49c",
            "metadata": {},
            "source": [
                "### Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "b1a05a32",
            "metadata": {},
            "outputs": [],
            "source": [
                "# replicates the private method '_add_document_metadata' from the repo\n",
                "def create_enhanced_content(text, doc):\n",
                "    parts = [text, \"\", \"---\"]\n",
                "    parts.append(f\"Document: {doc.title or 'Unknown'}\")\n",
                "    parts.append(f\"Source: {doc.source or 'Unknown'}\")\n",
                "    for key, value in doc.metadata.items():\n",
                "         parts.append(f\"{key}: {value}\")\n",
                "    return \"\\n\".join(parts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "a8dbcbcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We initialize the processor and use its 'chunker_provider'\n",
                "processor = DocumentProcessor()\n",
                "\n",
                "for paper in corpus:\n",
                "    # Create the shell Document object\n",
                "    doc_obj = Document(\n",
                "        title=paper['title'],\n",
                "        source=\"json_corpus\", \n",
                "        content_type=DocumentType.TXT, \n",
                "        raw_content=paper['text'],\n",
                "        metadata={\n",
                "            \"id\": paper['id'],\n",
                "            \"title\": paper['title']\n",
                "        }\n",
                "    )\n",
                "    # Manually Chunk the text using the processor's tool\n",
                "    # This breaks the text into semantic pieces\n",
                "    chunk_tuples = processor.chunker_provider.chunk(paper['text'])\n",
                "\n",
                "    # Build Chunk objects\n",
                "    for i, (raw_text, struct_enhanced) in enumerate(chunk_tuples):\n",
                "        \n",
                "        # Create the footer/header info\n",
                "        enhanced_content = create_enhanced_content(struct_enhanced, doc_obj)\n",
                "\n",
                "        # Create the Basic Chunk\n",
                "        doc_chunk = Chunk(\n",
                "            document_id=doc_obj.id,\n",
                "            content=raw_text,\n",
                "            chunk_number=i,\n",
                "            chunk_type=ChunkType.PARAGRAPH,\n",
                "        )\n",
                "\n",
                "        # Create the Processed Chunk (The part that gets embedded)\n",
                "        processed_chunk = ProcessedChunk(\n",
                "            chunk_id=doc_chunk.id,\n",
                "            enhanced_content=enhanced_content,\n",
                "        )\n",
                "\n",
                "        # Link them\n",
                "        doc_chunk.add_processed_chunk(processed_chunk)\n",
                "        doc_obj.add_chunk(doc_chunk)\n",
                "\n",
                "    documents_for_index.append(doc_obj)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e53f784",
            "metadata": {},
            "source": [
                "### Building the Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "9db33200",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-19 21:44:44,516 - INFO - Connected to Milvus Lite: ./milvus_final.db\n",
                        "2025-11-19 21:44:44,579 - INFO - PyTorch version 2.9.1 available.\n",
                        "2025-11-19 21:44:44,932 - INFO - Load pretrained SparseEncoder: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n",
                        "2025-11-19 21:44:47,887 - INFO - Loaded SPLADE model: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Database file found.\n",
                        "Index is already populated. SKIPPING ingestion.\n"
                    ]
                }
            ],
            "source": [
                "DB_FILE = \"./milvus_final.db\"\n",
                "\n",
                "db_exists = os.path.exists(DB_FILE)\n",
                "\n",
                "# Setup Store\n",
                "# we explicitly tell the store we are using Sparse only to save memory\n",
                "store = LocalMilvusStore(DB_FILE, enable_sparse=True, enable_dense=False)\n",
                "\n",
                "# we use a standard SPLADE model that works well on CPUs\n",
                "sparse_embedder = SpladeProvider(\n",
                "    model_name=\"opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\",\n",
                "    device=\"cpu\"\n",
                ")\n",
                "index = VerbatimIndex(vector_store=store, sparse_provider=sparse_embedder)\n",
                "\n",
                "if db_exists:\n",
                "    # The file exists on disk, so we check if Milvus can read it\n",
                "    print(\"Database file found.\")\n",
                "    try:\n",
                "        # We use a valid filter 'id != \"\"' instead of empty string\n",
                "        res = store.client.query(store.collection_name, filter='id != \"\"', limit=1)\n",
                "        if len(res) > 0:\n",
                "            print(\"Index is already populated. SKIPPING ingestion.\")\n",
                "        else:\n",
                "            print(\"Database exists but seems empty. Adding documents...\")\n",
                "            index.add_documents(documents_for_index)\n",
                "    except Exception as e:\n",
                "        print(f\"Database seems corrupted: {e}\")\n",
                "        print(\"deleting and rebuilding...\")\n",
                "        store.client.drop_collection(store.collection_name)\n",
                "        index.add_documents(documents_for_index)\n",
                "else:\n",
                "    print(\"New Database. Indexing documents...\")\n",
                "    index.add_documents(documents_for_index)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c45a916c",
            "metadata": {},
            "source": [
                "### Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "528a1e9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_best_paper(query_text, top_k=5):\n",
                "    print(f\"Querying: '{query_text}'\")\n",
                "    \n",
                "    results = index.query(query_text, k=top_k)\n",
                "    \n",
                "    if not results:\n",
                "        print(\"No matches found.\")\n",
                "        return None\n",
                "\n",
                "    # Dictionary to accumulate REAL scores\n",
                "    # {'Paper Title': 14.53}\n",
                "    paper_scores = defaultdict(float)\n",
                "    \n",
                "    print(f\"\\n--- Top {top_k} Chunks & Actual Similarity Scores ---\")\n",
                "    \n",
                "    for i, res in enumerate(results):\n",
                "        # 1. Get Metadata (Title)\n",
                "        meta = getattr(res, 'metadata', {}) or {}\n",
                "        if not meta and hasattr(res, 'get'): meta = res.get('metadata', {})\n",
                "        title = meta.get('title', meta.get('id', 'Unknown'))\n",
                "        id = meta.get('id', 'Unknown')\n",
                "        \n",
                "        # 2. EXTRACT THE REAL SCORE\n",
                "        # We try common attribute names used by Milvus wrappers\n",
                "        score = getattr(res, 'score', None)\n",
                "        \n",
                "        # If .score is missing, sometimes it is called .distance\n",
                "        if score is None:\n",
                "            score = getattr(res, 'distance', 0.0)\n",
                "            \n",
                "        # 3. Add to Total\n",
                "        paper_scores[id] += score\n",
                "        \n",
                "        # 4. Print Result\n",
                "        # We print the score to 4 decimal places\n",
                "        snippet = getattr(res, 'text', getattr(res, 'content', ''))[:40].replace('\\n', '')\n",
                "        print(f\"Rank {i+1}: Score {score:.4f} | Paper: {id} [{title[:20]}...] | Text: {snippet}...\")\n",
                "\n",
                "    # 5. Winner\n",
                "    sorted_papers = sorted(paper_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "    winner, total_score = sorted_papers[0]\n",
                "    \n",
                "    print(\"\\n--- Classification Result ---\")\n",
                "    print(f\"Predicted Paper: {winner}\")\n",
                "    print(f\"Total Similarity Score: {total_score:.4f}\")\n",
                "\n",
                "    return winner # id of the paper"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b8239d7",
            "metadata": {},
            "source": [
                "### Enter Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "fa7e3546",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"How can we detect sarcasm using deep learning?\"\n",
                "\n",
                "#predicted_paper = find_best_paper(query)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rag",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
