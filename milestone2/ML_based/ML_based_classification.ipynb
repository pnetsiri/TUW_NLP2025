{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73a1cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  #1 time installation\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "749f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ipywidgets import Widget, Text, Button, VBox, Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a2fa700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents from ../../corpus_json/corpus.json\n",
      "Document-level SVM model trained on 20 documents\n"
     ]
    }
   ],
   "source": [
    "candidate_paths = [\n",
    "    Path(\"corpus_json/corpus.json\"),\n",
    "    Path(\"../corpus_json/corpus.json\"),\n",
    "    Path(\"../../corpus_json/corpus.json\"),\n",
    "]\n",
    "for p in candidate_paths:\n",
    "    if p.exists():\n",
    "        corpus_path = p\n",
    "        break\n",
    "\n",
    "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "doc_ids = [doc[\"id\"] for doc in corpus]\n",
    "doc_titles = [doc.get(\"title\", \"\") for doc in corpus]\n",
    "doc_texts = [doc[\"text\"] for doc in corpus]\n",
    "\n",
    "# Train document-level SVM classifier\n",
    "doc_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),   # unigrams + bigrams\n",
    "    max_df=0.9,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "doc_model = make_pipeline(\n",
    "    doc_vectorizer,\n",
    "    SVC(C=1.0, kernel=\"sigmoid\", probability=True, random_state=42)\n",
    ")\n",
    "doc_model.fit(doc_texts, doc_ids)\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents from {corpus_path}\")\n",
    "print(f\"Document-level SVM model trained on {len(doc_texts)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bbe99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50406\n",
      "Sample features: ['00' '00 00' '00 15' '00 20' '00 40' '00 75' '00 95' '00 99' '00 aigc'\n",
      " '00 cm']\n",
      "\n",
      "Top 10 TF-IDF terms in Document 0:\n",
      "                   term     tfidf\n",
      "1406              crime  0.671330\n",
      "3568           mobility  0.209394\n",
      "986              cities  0.138597\n",
      "1484             crimes  0.134266\n",
      "2412        forecasting  0.129443\n",
      "3175                 lb  0.125604\n",
      "3756                 nn  0.106497\n",
      "1437  crime forecasting  0.090954\n",
      "863                cell  0.088116\n",
      "4189                poi  0.086623\n"
     ]
    }
   ],
   "source": [
    "# Display model information\n",
    "feature_names = doc_vectorizer.get_feature_names_out()\n",
    "print(\"Number of features:\", len(feature_names))\n",
    "print(\"Sample features:\", feature_names[:10])\n",
    "\n",
    "# Get feature importance from first document\n",
    "first_doc_vector = doc_vectorizer.transform([doc_texts[0]]).toarray()[0]\n",
    "nonzero_idx = first_doc_vector.nonzero()[0]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"term\": [feature_names[i] for i in nonzero_idx],\n",
    "    \"tfidf\": [first_doc_vector[i] for i in nonzero_idx]\n",
    "}).sort_values(\"tfidf\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF terms in Document 0:\")\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d6924ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-level retrieval function using SVM\n",
    "def retrieve_svm_docs(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Returns top-k documents using SVM classification.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Get probability predictions\n",
    "    proba = doc_model.predict_proba([query])[0]\n",
    "    classes = doc_model.classes_\n",
    "\n",
    "    # Get top-k indices\n",
    "    topk_idx = np.argsort(proba)[::-1][:k]\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(topk_idx, start=1):\n",
    "        doc_idx = doc_ids.index(classes[idx])\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(proba[idx]),\n",
    "            \"id\": doc_ids[doc_idx],\n",
    "            \"title\": doc_titles[doc_idx],\n",
    "            \"text\": doc_texts[doc_idx]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b23cb033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d523de0c5197474daef4b2995633b338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter your question:'), Text(value='', placeholder='Type your question here...'), …"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = Label(\"Enter your question:\")\n",
    "txt = Text(placeholder=\"Type your question here...\")\n",
    "btn = Button(description=\"Submit\")\n",
    "\n",
    "def on_click(b):\n",
    "    global query\n",
    "    query = txt.value\n",
    "    \n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "VBox([label, txt, btn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "833a66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we detect sarcasm via deep learning?\n"
     ]
    }
   ],
   "source": [
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1c4b428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can we detect sarcasm via deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] 2510.05163v1  (SVM probability=0.0507)\n",
      "Paper title: Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent break ...\n",
      "\n",
      "[2] 2510.05736v1  (SVM probability=0.0507)\n",
      "Paper title: Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes\n",
      "--------------------------------------------------------------------------------\n",
      "Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes. The identification of γ-rays from the predominant hadronic-background is a key aspect in their ground-based detection using Imaging Atmospheric Cherenkov Telescopes IACTs. While current methods are limited in their ability to exploit correlations in complex data deep learning-based models offer a promising alternative by directly leveraging image-level information. Howeve ...\n",
      "\n",
      "[3] 2510.07320v1  (SVM probability=0.0507)\n",
      "Paper title: Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children. Autism Spectrum Disorder ASD significantly influences the communication abilities learning processes behavior and social interactions of people. Although early intervention and customized educational strategies are critical to improving outcomes there is a pivotal gap in understanding and addressing nuanced behavioral patterns and emotional identification in autistic children prior to s ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results at title level\n",
    "results = retrieve_svm_docs(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] {r['id']}  (SVM probability={r['score']:.4f})\")\n",
    "    print(f\"Paper title: {r['title']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d2206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=220, overlap=40):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    chunk_size: target words per chunk\n",
    "    overlap: how many words to overlap between consecutive chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(words)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        start = end - overlap  \n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "752cc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_texts = []   \n",
    "passage_meta = []   \n",
    "\n",
    "for doc in corpus:\n",
    "    doc_id = doc[\"id\"]\n",
    "    title = doc.get(\"title\", \"\")\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size=220, overlap=40)\n",
    "    start_word = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        end_word = start_word + len(chunk.split())\n",
    "        passage_texts.append(chunk)\n",
    "        passage_meta.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"start_word\": start_word,\n",
    "            \"end_word\": end_word,\n",
    "        })\n",
    "        start_word = end_word - 40  # keep aligned with overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2a853e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n",
      "Number of passages:  491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of passages:  {len(passage_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcfe3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage-level SVM model trained on 491 passages\n"
     ]
    }
   ],
   "source": [
    "# Train passage-level SVM classifier\n",
    "passage_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.9,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "# Get labels for passages\n",
    "passage_labels = [meta[\"doc_id\"] for meta in passage_meta]\n",
    "\n",
    "passage_model = make_pipeline(\n",
    "    passage_vectorizer,\n",
    "    SVC(C=1.0, kernel=\"linear\", probability=True, random_state=42)\n",
    ")\n",
    "passage_model.fit(passage_texts, passage_labels)\n",
    "\n",
    "print(f\"Passage-level SVM model trained on {len(passage_texts)} passages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b827e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_svm_chunks(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages (chunks) using SVM classification.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Get probability predictions for each document class\n",
    "    proba = passage_model.predict_proba([query])[0]\n",
    "    classes = passage_model.classes_\n",
    "\n",
    "    # Get top-k document IDs\n",
    "    topk_idx = np.argsort(proba)[::-1][:k]\n",
    "    topk_doc_ids = [classes[idx] for idx in topk_idx]\n",
    "\n",
    "    # For each top document, find the best matching passage from that document\n",
    "    query_vec = passage_vectorizer.transform([query])\n",
    "    results = []\n",
    "    \n",
    "    for rank, doc_id in enumerate(topk_doc_ids, start=1):\n",
    "        # Get all passages from this document\n",
    "        doc_passage_indices = [i for i, label in enumerate(passage_labels) if label == doc_id]\n",
    "        \n",
    "        if doc_passage_indices:\n",
    "            # Find the passage with highest cosine similarity to query\n",
    "            doc_passage_vectors = passage_vectorizer.transform([passage_texts[i] for i in doc_passage_indices])\n",
    "            sims = cosine_similarity(query_vec, doc_passage_vectors)[0]\n",
    "            best_passage_idx = doc_passage_indices[np.argmax(sims)]\n",
    "            \n",
    "            meta = passage_meta[best_passage_idx]\n",
    "            doc_prob = proba[classes.tolist().index(doc_id)]\n",
    "            results.append({\n",
    "                \"rank\": rank,\n",
    "                \"score\": float(doc_prob),\n",
    "                \"text\": passage_texts[best_passage_idx],\n",
    "                \"doc_id\": meta[\"doc_id\"],\n",
    "                \"title\": meta[\"title\"],\n",
    "                \"chunk_id\": meta[\"chunk_id\"],\n",
    "                \"start_word\": meta[\"start_word\"],\n",
    "                \"end_word\": meta[\"end_word\"],\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8af5a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can we detect sarcasm via deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] SVM probability=0.4198\n",
      "Paper: 2510.10729v1 — Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "Chunk: 2510.10729v1_chunk_2 (words 360–580)\n",
      "--------------------------------------------------------------------------------\n",
      "text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must infer sarcasm from textual patterns alone. This paper explores a conceptual solution using DCNNs combined with contextual embedding models to understand sarcasm s complex indicators such as irony sentiment contradiction  ...\n",
      "\n",
      "[2] SVM probability=0.0682\n",
      "Paper: 2509.20913v1 — Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales\n",
      "Chunk: 2509.20913v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles an ...\n",
      "\n",
      "[3] SVM probability=0.0496\n",
      "Paper: 2510.11073v1 — ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer\n",
      "Chunk: 2510.11073v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_svm_chunks(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] SVM probability={r['score']:.4f}\")\n",
    "    print(f\"Paper: {r['doc_id']} — {r['title']}\")\n",
    "    print(f\"Chunk: {r['chunk_id']} (words {r['start_word']}–{r['end_word']})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
