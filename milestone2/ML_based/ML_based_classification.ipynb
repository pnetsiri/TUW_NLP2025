{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "73a1cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  #1 time installation\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "749f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ipywidgets import Widget, Text, Button, VBox, Label\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "93ab8bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents from ../../corpus_json/corpus.json\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT-LEVEL: Simple TF-IDF Vectorizer\n",
      "================================================================================\n",
      "Document-level TF-IDF vectorizer fitted on 20 documents\n",
      "Number of features: 5,000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "candidate_paths = [\n",
    "    Path(\"corpus_json/corpus.json\"),\n",
    "    Path(\"../corpus_json/corpus.json\"),\n",
    "    Path(\"../../corpus_json/corpus.json\"),\n",
    "]\n",
    "for p in candidate_paths:\n",
    "    if p.exists():\n",
    "        corpus_path = p\n",
    "        break\n",
    "\n",
    "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "doc_ids = [doc[\"id\"] for doc in corpus]\n",
    "doc_titles = [doc.get(\"title\", \"\") for doc in corpus]\n",
    "doc_texts = [doc[\"text\"] for doc in corpus]\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents from {corpus_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT-LEVEL: Simple TF-IDF + Cosine Similarity (No Grid Search Needed)\n",
    "# ============================================================================\n",
    "# Document-level is just for initial ranking, not classification\n",
    "# We use simple TF-IDF + cosine similarity \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENT-LEVEL: Simple TF-IDF Vectorizer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simple TF-IDF vectorizer for document-level retrieval\n",
    "doc_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,  # Require words to appear in at least 2 documents\n",
    "    max_df=0.8,  # Ignore very common words\n",
    "    max_features=5000  # Limit feature space to prevent overfitting\n",
    ")\n",
    "\n",
    "# Fit on all documents\n",
    "doc_vectors = doc_vectorizer.fit_transform(doc_texts)\n",
    "print(f\"Document-level TF-IDF vectorizer fitted on {len(doc_texts)} documents\")\n",
    "print(f\"Number of features: {len(doc_vectorizer.get_feature_names_out()):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1004bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-level retrieval function using TF-IDF + Cosine Similarity\n",
    "def retrieve_docs_tfidf(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Returns top-k documents using TF-IDF + cosine similarity.\n",
    "    Simple and fast - no SVM needed for document-level ranking.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    # Transform query to TF-IDF vector\n",
    "    query_vec = doc_vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(query_vec, doc_vectors)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    topk_idx = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    results = []\n",
    "    for rank, idx in enumerate(topk_idx, start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(similarities[idx]),\n",
    "            \"id\": doc_ids[idx],\n",
    "            \"title\": doc_titles[idx],\n",
    "            \"text\": doc_texts[idx]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7bbe99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT-LEVEL MODEL INFORMATION\n",
      "================================================================================\n",
      "Number of features: 5,000\n",
      "Feature reduction: From ~50,000+ to 5,000 features\n",
      "Reduction ratio: 90.0%\n",
      "\n",
      "Sample features: ['00' '00 00' '000' '000 images' '001' '006' '007' '009' '01' '010']\n",
      "\n",
      "Top 10 TF-IDF terms in Document 0:\n",
      "             term     tfidf\n",
      "961      mobility  0.420206\n",
      "645   forecasting  0.259764\n",
      "1013           nn  0.213715\n",
      "254          cell  0.176828\n",
      "1206       recall  0.162278\n",
      "277          city  0.160442\n",
      "901          lstm  0.158118\n",
      "1384      spatial  0.149042\n",
      "375          conv  0.132621\n",
      "1132    precision  0.125659\n",
      "\n",
      "✓ Document-level uses simple TF-IDF + cosine similarity\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display model information\n",
    "feature_names = doc_vectorizer.get_feature_names_out()\n",
    "print(\"=\"*80)\n",
    "print(\"DOCUMENT-LEVEL MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of features: {len(feature_names):,}\")\n",
    "print(f\"Feature reduction: From ~50,000+ to {len(feature_names):,} features\")\n",
    "print(f\"Reduction ratio: {(1 - len(feature_names)/50000)*100:.1f}%\")\n",
    "print(f\"\\nSample features: {feature_names[:10]}\")\n",
    "\n",
    "# Get feature importance from first document\n",
    "first_doc_vector = doc_vectorizer.transform([doc_texts[0]]).toarray()[0]\n",
    "nonzero_idx = first_doc_vector.nonzero()[0]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"term\": [feature_names[i] for i in nonzero_idx],\n",
    "    \"tfidf\": [first_doc_vector[i] for i in nonzero_idx]\n",
    "}).sort_values(\"tfidf\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF terms in Document 0:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n✓ Document-level uses simple TF-IDF + cosine similarity\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b23cb033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9700a1e7f14124941a91ec7e90474f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter your question:'), Text(value='', placeholder='Type your question here...'), …"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = Label(\"Enter your question:\")\n",
    "txt = Text(placeholder=\"Type your question here...\")\n",
    "btn = Button(description=\"Submit\")\n",
    "\n",
    "def on_click(b):\n",
    "    global query\n",
    "    query = txt.value\n",
    "    \n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "VBox([label, txt, btn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "833a66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how can we detect sarcasm using deep learning?\n"
     ]
    }
   ],
   "source": [
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f1c4b428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how can we detect sarcasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] 2510.10729v1  (similarity=0.0342)\n",
      "Paper title: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "--------------------------------------------------------------------------------\n",
      "Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through  ...\n",
      "\n",
      "[2] 2510.08770v1  (similarity=0.0255)\n",
      "Paper title: Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform\n",
      "--------------------------------------------------------------------------------\n",
      "Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform. This paper presents a real-time spill detection system that utilizes pretrained deep learning models with RGB and thermal imaging to classify spill vs. no-spill scenarios across varied environments. Using a balanced binary dataset 4 000 images our experiments demonstrate the advantages of thermal imaging in inference speed accuracy and model size. We achieve up to 100% accuracy using lightweight mode ...\n",
      "\n",
      "[3] 2510.13137v1  (similarity=0.0175)\n",
      "Paper title: Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 20 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results at title level\n",
    "results = retrieve_docs_tfidf(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] {r['id']}  (similarity={r['score']:.4f})\")\n",
    "    print(f\"Paper title: {r['title']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d2206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=220, overlap=0):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    chunk_size: target words per chunk\n",
    "    overlap: how many words to overlap between consecutive chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(words)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        start = end - overlap  \n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "752cc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_texts = []   \n",
    "passage_meta = []   \n",
    "\n",
    "for doc in corpus:\n",
    "    doc_id = doc[\"id\"]\n",
    "    title = doc.get(\"title\", \"\")\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size=220, overlap=40)\n",
    "    start_word = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        end_word = start_word + len(chunk.split())\n",
    "        passage_texts.append(chunk)\n",
    "        passage_meta.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"start_word\": start_word,\n",
    "            \"end_word\": end_word,\n",
    "        })\n",
    "        start_word = end_word - 40  # keep aligned with overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f2a853e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n",
      "Number of passages:  491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of passages:  {len(passage_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4a82d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Neuer Split mit sklearn (shuffle=False):\n",
      "Training:   281 Chunks (60%)\n",
      "Validation: 103 Chunks (20%)\n",
      "Test:       107 Chunks (20%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train passage-level SVM classifier with validation and test set\n",
    "# Get labels for passages\n",
    "passage_labels = [meta[\"doc_id\"] for meta in passage_meta]\n",
    "\n",
    "X_train_pass, X_val_pass, X_test_pass = [], [], []\n",
    "y_train_pass, y_val_pass, y_test_pass = [], [], []\n",
    "\n",
    "unique_docs = list(set(passage_labels))\n",
    "\n",
    "for doc_id in unique_docs:\n",
    "    indices = [i for i, x in enumerate(passage_labels) if x == doc_id]\n",
    "    doc_texts = [passage_texts[i] for i in indices]\n",
    "    doc_labels = [passage_labels[i] for i in indices]\n",
    "    \n",
    "    if len(doc_texts) < 3:\n",
    "        X_train_pass.extend(doc_texts)\n",
    "        y_train_pass.extend(doc_labels)\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    texts_temp, texts_test, labels_temp, labels_test = train_test_split(\n",
    "        doc_texts, doc_labels, test_size=0.20, shuffle=False\n",
    "    )\n",
    "    \n",
    "\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
    "        texts_temp, labels_temp, test_size=0.25, shuffle=False\n",
    "    )\n",
    "\n",
    "    X_train_pass.extend(texts_train)\n",
    "    y_train_pass.extend(labels_train)\n",
    "    \n",
    "    X_val_pass.extend(texts_val)\n",
    "    y_val_pass.extend(labels_val)\n",
    "    \n",
    "    X_test_pass.extend(texts_test)\n",
    "    y_test_pass.extend(labels_test)\n",
    "\n",
    "#check split\n",
    "print(\"=\"*60)\n",
    "print(f\"Neuer Split mit sklearn (shuffle=False):\")\n",
    "print(f\"Training:   {len(X_train_pass)} Chunks (60%)\")\n",
    "print(f\"Validation: {len(X_val_pass)} Chunks (20%)\")\n",
    "print(f\"Test:       {len(X_test_pass)} Chunks (20%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fcfe3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING PASSAGE-LEVEL MODEL WITH GRID SEARCH\n",
      "================================================================================\n",
      "Performing grid search for passage-level model (this may take a while)...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters: {'svc__C': 0.8, 'svc__kernel': 'linear', 'tfidfvectorizer__max_df': 0.7, 'tfidfvectorizer__max_features': 100, 'tfidfvectorizer__min_df': 5}\n",
      "Best cross-validation score: 0.8718\n",
      "\n",
      "Passage-level validation accuracy: 0.7087\n",
      "Training set: 281 passages, Validation set: 103 passages\n",
      "Number of features: 100\n",
      "\n",
      "Validation classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "2509.20913v1       0.80      0.80      0.80        10\n",
      "2509.23158v1       0.31      0.80      0.44         5\n",
      "2510.05163v1       1.00      1.00      1.00         3\n",
      "2510.05736v1       0.00      0.00      0.00         2\n",
      "2510.07320v1       1.00      0.50      0.67         4\n",
      "2510.08116v1       1.00      0.83      0.91         6\n",
      "2510.08411v1       1.00      0.75      0.86         4\n",
      "2510.08662v1       0.50      1.00      0.67         4\n",
      "2510.08770v1       0.50      0.25      0.33         4\n",
      "2510.09187v1       0.60      0.75      0.67         4\n",
      "2510.10729v1       0.00      0.00      0.00         2\n",
      "2510.10822v1       0.67      1.00      0.80         4\n",
      "2510.11073v1       1.00      0.70      0.82        10\n",
      "2510.11301v1       0.67      0.50      0.57         4\n",
      "2510.12758v1       1.00      1.00      1.00        10\n",
      "2510.12850v1       0.00      0.00      0.00         4\n",
      "2510.13050v1       1.00      0.38      0.55         8\n",
      "2510.13137v1       0.75      0.75      0.75         4\n",
      "2510.13937v1       1.00      1.00      1.00         5\n",
      "2510.14855v1       0.50      1.00      0.67         6\n",
      "\n",
      "    accuracy                           0.71       103\n",
      "   macro avg       0.66      0.65      0.63       103\n",
      "weighted avg       0.75      0.71      0.69       103\n",
      "\n",
      "\n",
      "Passage-level TEST accuracy: 0.6729\n",
      "Test set: 107 passages\n",
      "Test set classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "2509.20913v1       0.85      1.00      0.92        11\n",
      "2509.23158v1       0.36      0.80      0.50         5\n",
      "2510.05163v1       0.67      0.67      0.67         3\n",
      "2510.05736v1       1.00      1.00      1.00         2\n",
      "2510.07320v1       0.60      0.75      0.67         4\n",
      "2510.08116v1       0.86      1.00      0.92         6\n",
      "2510.08411v1       0.67      0.50      0.57         4\n",
      "2510.08662v1       0.27      0.80      0.40         5\n",
      "2510.08770v1       0.00      0.00      0.00         4\n",
      "2510.09187v1       0.50      0.50      0.50         4\n",
      "2510.10729v1       0.00      0.00      0.00         3\n",
      "2510.10822v1       1.00      0.80      0.89         5\n",
      "2510.11073v1       1.00      0.20      0.33        10\n",
      "2510.11301v1       0.75      0.75      0.75         4\n",
      "2510.12758v1       1.00      0.90      0.95        10\n",
      "2510.12850v1       0.33      0.50      0.40         4\n",
      "2510.13050v1       1.00      0.75      0.86         8\n",
      "2510.13137v1       1.00      1.00      1.00         4\n",
      "2510.13937v1       1.00      1.00      1.00         5\n",
      "2510.14855v1       0.33      0.17      0.22         6\n",
      "\n",
      "    accuracy                           0.67       107\n",
      "   macro avg       0.66      0.65      0.63       107\n",
      "weighted avg       0.72      0.67      0.66       107\n",
      "\n",
      "\n",
      "Training accuracy: 0.9893\n",
      "Validation accuracy: 0.7087\n",
      "Difference (overfitting indicator): 0.2806\n",
      "WARNING: Large gap between train and validation accuracy suggests overfitting!\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL ON ALL DATA\n",
      "================================================================================\n",
      "Passage-level SVM model trained on 491 passages (all data)\n",
      "Final model features: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING PASSAGE-LEVEL MODEL WITH GRID SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define parameter grid for passage-level model\n",
    "passage_param_grid = {\n",
    "    'tfidfvectorizer__min_df': [3, 5, 7],  # BALANCED: Moderate filtering (was too high at 15)\n",
    "    'tfidfvectorizer__max_df': [0.7, 0.8, 0.9],\n",
    "    'tfidfvectorizer__max_features': [50, 80, 100],\n",
    "    'svc__C': [0.01, 0.1, 0.8],  # BALANCED: Moderate regularization \n",
    "    'svc__kernel': ['linear', 'rbf'] \n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "passage_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2)),\n",
    "    SVC(probability=True, random_state=42)\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Performing grid search for passage-level model (this may take a while)...\")\n",
    "passage_grid_search = GridSearchCV(\n",
    "    passage_pipeline,\n",
    "    passage_param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "passage_grid_search.fit(X_train_pass, y_train_pass)\n",
    "\n",
    "print(f\"\\nBest parameters: {passage_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {passage_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "passage_model = passage_grid_search.best_estimator_\n",
    "passage_vectorizer = passage_model.named_steps['tfidfvectorizer']\n",
    "\n",
    "# Validate on validation set\n",
    "val_pred_pass = passage_model.predict(X_val_pass)\n",
    "val_acc_pass = accuracy_score(y_val_pass, val_pred_pass)\n",
    "print(f\"\\nPassage-level validation accuracy: {val_acc_pass:.4f}\")\n",
    "print(f\"Training set: {len(X_train_pass)} passages, Validation set: {len(X_val_pass)} passages\")\n",
    "print(f\"Number of features: {len(passage_vectorizer.get_feature_names_out())}\")\n",
    "print(\"\\nValidation classification report:\")\n",
    "print(classification_report(y_val_pass, val_pred_pass, zero_division=0))\n",
    "\n",
    "# Test on held-out test set\n",
    "test_pred_pass = passage_model.predict(X_test_pass)\n",
    "test_acc_pass = accuracy_score(y_test_pass, test_pred_pass)\n",
    "print(f\"\\nPassage-level TEST accuracy: {test_acc_pass:.4f}\")\n",
    "print(f\"Test set: {len(X_test_pass)} passages\")\n",
    "print(\"Test set classification report:\")\n",
    "print(classification_report(y_test_pass, test_pred_pass, zero_division=0))\n",
    "\n",
    "# Check for overfitting: compare train vs validation accuracy\n",
    "train_pred_pass = passage_model.predict(X_train_pass)\n",
    "train_acc_pass = accuracy_score(y_train_pass, train_pred_pass)\n",
    "print(f\"\\nTraining accuracy: {train_acc_pass:.4f}\")\n",
    "print(f\"Validation accuracy: {val_acc_pass:.4f}\")\n",
    "print(f\"Difference (overfitting indicator): {train_acc_pass - val_acc_pass:.4f}\")\n",
    "if train_acc_pass - val_acc_pass > 0.1:\n",
    "    print(\"WARNING: Large gap between train and validation accuracy suggests overfitting!\")\n",
    "\n",
    "# Train final model on all data for production use (using best parameters)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON ALL DATA\")\n",
    "print(\"=\"*80)\n",
    "final_passage_model = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=passage_grid_search.best_params_['tfidfvectorizer__min_df'],\n",
    "        max_df=passage_grid_search.best_params_['tfidfvectorizer__max_df'],\n",
    "        max_features=passage_grid_search.best_params_['tfidfvectorizer__max_features']\n",
    "    ),\n",
    "    SVC(\n",
    "        C=passage_grid_search.best_params_['svc__C'],\n",
    "        kernel=passage_grid_search.best_params_['svc__kernel'],\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "final_passage_model.fit(passage_texts, passage_labels)\n",
    "passage_model = final_passage_model\n",
    "passage_vectorizer = passage_model.named_steps['tfidfvectorizer']\n",
    "print(f\"Passage-level SVM model trained on {len(passage_texts)} passages (all data)\")\n",
    "print(f\"Final model features: {len(passage_vectorizer.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b827e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_svm_chunks(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages (chunks) using SVM classification.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    # Get probability predictions for each document class\n",
    "    proba = passage_model.predict_proba([query])[0]\n",
    "    classes = passage_model.classes_\n",
    "\n",
    "    # Get top-k document IDs\n",
    "    topk_idx = np.argsort(proba)[::-1][:k]\n",
    "    topk_doc_ids = [classes[idx] for idx in topk_idx]\n",
    "\n",
    "    # For each top document, find the best matching passage from that document\n",
    "    query_vec = passage_vectorizer.transform([query])\n",
    "    results = []\n",
    "    \n",
    "    for rank, doc_id in enumerate(topk_doc_ids, start=1):\n",
    "        # Get all passages from this document\n",
    "        doc_passage_indices = [i for i, label in enumerate(passage_labels) if label == doc_id]\n",
    "        \n",
    "        if doc_passage_indices:\n",
    "            # Find the passage with highest cosine similarity to query\n",
    "            doc_passage_vectors = passage_vectorizer.transform([passage_texts[i] for i in doc_passage_indices])\n",
    "            sims = cosine_similarity(query_vec, doc_passage_vectors)[0]\n",
    "            best_passage_idx = doc_passage_indices[np.argmax(sims)]\n",
    "            \n",
    "            meta = passage_meta[best_passage_idx]\n",
    "            doc_prob = proba[classes.tolist().index(doc_id)]\n",
    "            results.append({\n",
    "                \"rank\": rank,\n",
    "                \"score\": float(doc_prob),\n",
    "                \"text\": passage_texts[best_passage_idx],\n",
    "                \"doc_id\": meta[\"doc_id\"],\n",
    "                \"title\": meta[\"title\"],\n",
    "                \"chunk_id\": meta[\"chunk_id\"],\n",
    "                \"start_word\": meta[\"start_word\"],\n",
    "                \"end_word\": meta[\"end_word\"],\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8af5a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how can we detect sarcasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] SVM probability=0.2004\n",
      "Paper: 2510.14855v1 — A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution\n",
      "Chunk: 2510.14855v1_chunk_26 (words 4680–4811)\n",
      "--------------------------------------------------------------------------------\n",
      "class imbalance through augmentation or resampling may improve fairness across lesion types. A promising next step is generating simulated lesion images using GAN-based methods to complement feature-space evolution and provide more intuitive visual feedback. Incorporating expert-reviewed or longitudinal data would also enable supervised learning of the E component and improve clinical realism. In summary, we developed a deep learning model to classify skin lesions, interpret predictions using AB ...\n",
      "\n",
      "[2] SVM probability=0.1025\n",
      "Paper: 2509.20913v1 — Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales\n",
      "Chunk: 2509.20913v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles an ...\n",
      "\n",
      "[3] SVM probability=0.0934\n",
      "Paper: 2510.08662v1 — DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops\n",
      "Chunk: 2510.08662v1_chunk_3 (words 540–760)\n",
      "--------------------------------------------------------------------------------\n",
      "unprecedented challenges to food conventional GS models often inadequately capture complex non-additive genetic effects which limits their prediction accuracy and robustness. Recently deep learning methods have demonstrated remarkable efficacy in data modeling across diverse scientific domains. Their capacity to automatically learn complex features enables the effective modeling of non-linear relationships between genotype and phenotype rendering them highly suitable for genomic prediction. Geno ...\n",
      "\n",
      "[4] SVM probability=0.0693\n",
      "Paper: 2510.05163v1 — Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches\n",
      "Chunk: 2510.05163v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches. In the era of pervasive cyber threats and exponential growth in digital services the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication MFA which combines knowledge-based factors passwords PINs possessionbased factors smart cards tokens and inherence-based factors biometric traits has emerged as a robust defense mechanism. Recent break ...\n",
      "\n",
      "[5] SVM probability=0.0650\n",
      "Paper: 2510.12850v1 — Ethic-BERT: An Enhanced Deep Learning Model for Ethical and Non-Ethical Content Classification\n",
      "Chunk: 2510.12850v1_chunk_3 (words 540–760)\n",
      "--------------------------------------------------------------------------------\n",
      "we address these key challenges by the following key contributions fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios improving data quality. By advancing the interplay between ethical reasoning and AI our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation hate speech and ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_svm_chunks(query, k=5)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] SVM probability={r['score']:.4f}\")\n",
    "    print(f\"Paper: {r['doc_id']} — {r['title']}\")\n",
    "    print(f\"Chunk: {r['chunk_id']} (words {r['start_word']}–{r['end_word']})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
