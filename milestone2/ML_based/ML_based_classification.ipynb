{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "829813ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install verbatim-rag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "import_libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from verbatim_rag.document import Document, Chunk, ProcessedChunk, DocumentType, ChunkType\n",
                "from verbatim_rag.ingestion import DocumentProcessor \n",
                "from verbatim_rag.vector_stores import LocalMilvusStore\n",
                "from verbatim_rag import VerbatimIndex\n",
                "from verbatim_rag.embedding_providers import SpladeProvider"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus_path = Path(\"../../corpus_json/corpus.json\")\n",
                "\n",
                "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
                "    corpus = json.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "preview_data",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>title</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>2509.20913v1</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2509.23158v1</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2510.05163v1</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>2510.05736v1</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2510.07320v1</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "             id                                              title  \\\n",
                            "0  2509.20913v1  Deep Learning for Crime Forecasting: The Role ...   \n",
                            "1  2509.23158v1  Deep Learning-Based Detection of Cognitive Imp...   \n",
                            "2  2510.05163v1  Deep Learning-Based Multi-Factor Authenticatio...   \n",
                            "3  2510.05736v1  Convolution and Graph-based Deep Learning Appr...   \n",
                            "4  2510.07320v1  Deep Learning Based Approach to Enhanced Recog...   \n",
                            "\n",
                            "                                                text  \n",
                            "0  Deep Learning for Crime Forecasting: The Role ...  \n",
                            "1  Deep Learning-Based Detection of Cognitive Imp...  \n",
                            "2  Deep Learning-Based Multi-Factor Authenticatio...  \n",
                            "3  Convolution and Graph-based Deep Learning Appr...  \n",
                            "4  Deep Learning Based Approach to Enhanced Recog...  "
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Convert to DataFrame for easier handling\n",
                "df = pd.DataFrame(corpus)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "828da49c",
            "metadata": {},
            "source": [
                "### Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "b1a05a32",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This replicates the private method '_add_document_metadata' from the repo\n",
                "def create_enhanced_content(text, doc):\n",
                "    parts = [text, \"\", \"---\"]\n",
                "    parts.append(f\"Document: {doc.title or 'Unknown'}\")\n",
                "    parts.append(f\"Source: {doc.source or 'Unknown'}\")\n",
                "    for key, value in doc.metadata.items():\n",
                "         parts.append(f\"{key}: {value}\")\n",
                "    return \"\\n\".join(parts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8dbcbcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "documents_for_index = []\n",
                "\n",
                "# We initialize the processor and use its 'chunker_provider'\n",
                "processor = DocumentProcessor()\n",
                "\n",
                "for paper in corpus:\n",
                "    # Create the shell Document object\n",
                "    doc_obj = Document(\n",
                "        title=paper['title'],\n",
                "        source=\"json_corpus\", \n",
                "        content_type=DocumentType.TXT, \n",
                "        raw_content=paper['text'],\n",
                "        metadata={\n",
                "            \"id\": paper['id'],\n",
                "            \"title\": paper['title']\n",
                "        }\n",
                "    )\n",
                "    # Manually Chunk the text using the processor's tool\n",
                "    # This breaks the text into semantic pieces\n",
                "    chunk_tuples = processor.chunker_provider.chunk(paper['text'])\n",
                "\n",
                "    # Build Chunk objects\n",
                "    for i, (raw_text, struct_enhanced) in enumerate(chunk_tuples):\n",
                "        \n",
                "        # Create the footer/header info\n",
                "        enhanced_content = create_enhanced_content(struct_enhanced, doc_obj)\n",
                "\n",
                "        # Create the Basic Chunk\n",
                "        doc_chunk = Chunk(\n",
                "            document_id=doc_obj.id,\n",
                "            content=raw_text,\n",
                "            chunk_number=i,\n",
                "            chunk_type=ChunkType.PARAGRAPH,\n",
                "        )\n",
                "\n",
                "        # Create the Processed Chunk (The part that gets embedded)\n",
                "        processed_chunk = ProcessedChunk(\n",
                "            chunk_id=doc_chunk.id,\n",
                "            enhanced_content=enhanced_content,\n",
                "        )\n",
                "\n",
                "        # Link them\n",
                "        doc_chunk.add_processed_chunk(processed_chunk)\n",
                "        doc_obj.add_chunk(doc_chunk)\n",
                "\n",
                "    documents_for_index.append(doc_obj)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e53f784",
            "metadata": {},
            "source": [
                "### Building the Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9db33200",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-19 15:40:15,934 - INFO - Connected to Milvus Lite: ./milvus_classifier.db\n",
                        "2025-11-19 15:40:15,938 - INFO - Load pretrained SparseEncoder: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n",
                        "2025-11-19 15:40:18,841 - INFO - Loaded SPLADE model: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Adding 20 documents to Milvus.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.69it/s]s]\n",
                        "2025-11-19 15:40:19,150 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:19,151 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.81it/s] 3.31it/s]\n",
                        "2025-11-19 15:40:19,401 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:19,402 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.12it/s] 3.68it/s]\n",
                        "2025-11-19 15:40:19,640 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:19,641 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.26it/s] 3.89it/s]\n",
                        "2025-11-19 15:40:19,923 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:19,923 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.20it/s] 3.74it/s]\n",
                        "2025-11-19 15:40:20,164 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:20,165 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.39it/s] 3.88it/s]\n",
                        "2025-11-19 15:40:20,425 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:20,426 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.10it/s] 3.86it/s]\n",
                        "2025-11-19 15:40:20,672 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:20,674 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.92it/s] 3.92it/s]\n",
                        "2025-11-19 15:40:20,941 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:20,942 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.64it/s] 3.85it/s]\n",
                        "2025-11-19 15:40:21,239 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:21,240 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.61it/s] 3.69it/s]\n",
                        "2025-11-19 15:40:21,496 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:21,497 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.88it/s]  3.75it/s]\n",
                        "2025-11-19 15:40:21,741 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:21,742 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.74it/s]  3.84it/s]\n",
                        "2025-11-19 15:40:22,001 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:22,002 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.78it/s]  3.84it/s]\n",
                        "2025-11-19 15:40:22,302 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:22,303 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.39it/s]  3.67it/s]\n",
                        "2025-11-19 15:40:22,562 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:22,563 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  8.61it/s]  3.72it/s]\n",
                        "2025-11-19 15:40:22,827 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:22,828 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.09it/s]  3.73it/s]\n",
                        "2025-11-19 15:40:23,075 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:23,076 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.46it/s]  3.82it/s]\n",
                        "2025-11-19 15:40:23,324 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:23,325 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.83it/s]  3.88it/s]\n",
                        "2025-11-19 15:40:23,616 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:23,617 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.45it/s]  3.73it/s]\n",
                        "2025-11-19 15:40:23,868 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:23,869 - INFO - Added 1 documents to Milvus\n",
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.67it/s]  3.80it/s]\n",
                        "2025-11-19 15:40:24,118 - INFO - Added 1 vectors to Milvus\n",
                        "2025-11-19 15:40:24,119 - INFO - Added 1 documents to Milvus\n",
                        "Adding documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.79it/s]\n"
                    ]
                }
            ],
            "source": [
                "# we explicitly tell the store we are using Sparse only to save memory\n",
                "store = LocalMilvusStore(\"./milvus_classifier.db\", enable_sparse=True, enable_dense=False)\n",
                "\n",
                "# we use a standard SPLADE model that works well on CPUs\n",
                "sparse_embedder = SpladeProvider(\n",
                "    model_name=\"opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\",\n",
                "    device=\"cpu\"\n",
                ")\n",
                "\n",
                "index = VerbatimIndex(vector_store=store, sparse_provider=sparse_embedder)\n",
                "\n",
                "print(f\"Adding {len(documents_for_index)} documents to Milvus.\")\n",
                "index.add_documents(documents_for_index)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c45a916c",
            "metadata": {},
            "source": [
                "### Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "528a1e9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "def find_best_paper(query_text, top_k=5):\n",
                "    \"\"\"\n",
                "    1. Searches for the top_k chunks matching the query.\n",
                "    2. Counts which paper appears most often.\n",
                "    3. Returns the winner.\n",
                "    \"\"\"\n",
                "    print(f\"ðŸ”Ž Querying: '{query_text}'\")\n",
                "    \n",
                "    # retrieval\n",
                "    # We ask for top_k chunks to get a good sample for voting\n",
                "    results = index.query(query_text, k=top_k)\n",
                "    \n",
                "    if not results:\n",
                "        print(\"No matches found in the index.\")\n",
                "        return None\n",
                "\n",
                "    # Extract Votes\n",
                "    votes = []\n",
                "    \n",
                "    print(f\"\\n--- Raw Retrieved Chunks (Top {top_k}) ---\")\n",
                "    for i, res in enumerate(results):\n",
                "        # extracting metadata\n",
                "        meta = getattr(res, 'metadata', None)\n",
                "        if meta is None and hasattr(res, 'get'):\n",
                "            meta = res.get('metadata')\n",
                "        if meta is None: \n",
                "            meta = {}\n",
                "\n",
                "        # get the paper title (Label)\n",
                "        paper_label = meta.get('title', meta.get('paper_title', meta.get('id', 'Unknown Source')))\n",
                "        votes.append(paper_label)\n",
                "        \n",
                "        if hasattr(res, 'text'):\n",
                "            raw_text = res.text\n",
                "\n",
                "        snippet = str(raw_text)[:100].replace('\\n', ' ')\n",
                "        print(f\"Rank {i+1}: [{paper_label}] ...{snippet}...\")\n",
                "\n",
                "    # count the votes\n",
                "    if not votes:\n",
                "        return \"No Metadata Found\"\n",
                "\n",
                "    vote_counts = Counter(votes)\n",
                "    \n",
                "    # Determine the Winner\n",
                "    winner, count = vote_counts.most_common(1)[0]\n",
                "    \n",
                "    print(\"\\n--- Classification Result ---\")\n",
                "    print(f\"Predicted Paper: {winner}\")\n",
                "    print(f\"Confidence: {count}/{len(results)} retrieved chunks belong to this paper.\")\n",
                "    print(f\"All candidates: {dict(vote_counts)}\")\n",
                "    \n",
                "    return winner"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b8239d7",
            "metadata": {},
            "source": [
                "### Enter Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "fa7e3546",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ”Ž Querying: 'How can we detect sarcasm using deep learning?'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.96it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Raw Retrieved Chunks (Top 5) ---\n",
                        "Rank 1: [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning] ...Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone an...\n",
                        "Rank 2: [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning] ...Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone an...\n",
                        "Rank 3: [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization] ...Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-A...\n",
                        "Rank 4: [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization] ...Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-A...\n",
                        "Rank 5: [From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis] ...From Detection to Mitigation Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis. Deep...\n",
                        "\n",
                        "--- Classification Result ---\n",
                        "Predicted Paper: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
                        "Confidence: 2/5 retrieved chunks belong to this paper.\n",
                        "All candidates: {'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning': 2, 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization': 2, 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis': 1}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "query = \"How can we detect sarcasm using deep learning?\"\n",
                "\n",
                "predicted_paper = find_best_paper(query)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
