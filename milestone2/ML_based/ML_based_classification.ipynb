{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 136,
            "id": "829813ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install verbatim-rag"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 137,
            "id": "import_libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "import os\n",
                "from pathlib import Path\n",
                "from verbatim_rag.document import Document, Chunk, ProcessedChunk, DocumentType, ChunkType\n",
                "from verbatim_rag.ingestion import DocumentProcessor \n",
                "from verbatim_rag.vector_stores import LocalMilvusStore\n",
                "from verbatim_rag import VerbatimIndex\n",
                "from verbatim_rag.embedding_providers import SpladeProvider\n",
                "from collections import defaultdict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 138,
            "id": "b478416c",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading 20 papers...\n"
                    ]
                }
            ],
            "source": [
                "documents_for_index = [] \n",
                "\n",
                "corpus_path = Path(\"../../corpus_json/corpus.json\")\n",
                "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
                "    corpus = json.load(f)\n",
                "\n",
                "print(f\"Loading {len(corpus)} papers...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 139,
            "id": "preview_data",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>id</th>\n",
                            "      <th>title</th>\n",
                            "      <th>text</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>2509.20913v1</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "      <td>Deep Learning for Crime Forecasting: The Role ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>2509.23158v1</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "      <td>Deep Learning-Based Detection of Cognitive Imp...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2510.05163v1</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "      <td>Deep Learning-Based Multi-Factor Authenticatio...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>2510.05736v1</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "      <td>Convolution and Graph-based Deep Learning Appr...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>2510.07320v1</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "      <td>Deep Learning Based Approach to Enhanced Recog...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "             id                                              title  \\\n",
                            "0  2509.20913v1  Deep Learning for Crime Forecasting: The Role ...   \n",
                            "1  2509.23158v1  Deep Learning-Based Detection of Cognitive Imp...   \n",
                            "2  2510.05163v1  Deep Learning-Based Multi-Factor Authenticatio...   \n",
                            "3  2510.05736v1  Convolution and Graph-based Deep Learning Appr...   \n",
                            "4  2510.07320v1  Deep Learning Based Approach to Enhanced Recog...   \n",
                            "\n",
                            "                                                text  \n",
                            "0  Deep Learning for Crime Forecasting: The Role ...  \n",
                            "1  Deep Learning-Based Detection of Cognitive Imp...  \n",
                            "2  Deep Learning-Based Multi-Factor Authenticatio...  \n",
                            "3  Convolution and Graph-based Deep Learning Appr...  \n",
                            "4  Deep Learning Based Approach to Enhanced Recog...  "
                        ]
                    },
                    "execution_count": 139,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# checking the corpus\n",
                "df = pd.DataFrame(corpus)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "828da49c",
            "metadata": {},
            "source": [
                "### Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 140,
            "id": "b1a05a32",
            "metadata": {},
            "outputs": [],
            "source": [
                "# replicates the private method '_add_document_metadata' from the repo\n",
                "def create_enhanced_content(text, doc):\n",
                "    parts = [text, \"\", \"---\"]\n",
                "    parts.append(f\"Document: {doc.title or 'Unknown'}\")\n",
                "    parts.append(f\"Source: {doc.source or 'Unknown'}\")\n",
                "    for key, value in doc.metadata.items():\n",
                "         parts.append(f\"{key}: {value}\")\n",
                "    return \"\\n\".join(parts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 141,
            "id": "a8dbcbcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We initialize the processor and use its 'chunker_provider'\n",
                "processor = DocumentProcessor()\n",
                "\n",
                "for paper in corpus:\n",
                "    # Create the shell Document object\n",
                "    doc_obj = Document(\n",
                "        title=paper['title'],\n",
                "        source=\"json_corpus\", \n",
                "        content_type=DocumentType.TXT, \n",
                "        raw_content=paper['text'],\n",
                "        metadata={\n",
                "            \"id\": paper['id'],\n",
                "            \"title\": paper['title']\n",
                "        }\n",
                "    )\n",
                "    # Manually Chunk the text using the processor's tool\n",
                "    # This breaks the text into semantic pieces\n",
                "    chunk_tuples = processor.chunker_provider.chunk(paper['text'])\n",
                "\n",
                "    # Build Chunk objects\n",
                "    for i, (raw_text, struct_enhanced) in enumerate(chunk_tuples):\n",
                "        \n",
                "        # Create the footer/header info\n",
                "        enhanced_content = create_enhanced_content(struct_enhanced, doc_obj)\n",
                "\n",
                "        # Create the Basic Chunk\n",
                "        doc_chunk = Chunk(\n",
                "            document_id=doc_obj.id,\n",
                "            content=raw_text,\n",
                "            chunk_number=i,\n",
                "            chunk_type=ChunkType.PARAGRAPH,\n",
                "        )\n",
                "\n",
                "        # Create the Processed Chunk (The part that gets embedded)\n",
                "        processed_chunk = ProcessedChunk(\n",
                "            chunk_id=doc_chunk.id,\n",
                "            enhanced_content=enhanced_content,\n",
                "        )\n",
                "\n",
                "        # Link them\n",
                "        doc_chunk.add_processed_chunk(processed_chunk)\n",
                "        doc_obj.add_chunk(doc_chunk)\n",
                "\n",
                "    documents_for_index.append(doc_obj)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e53f784",
            "metadata": {},
            "source": [
                "### Building the Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 142,
            "id": "9db33200",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-19 16:17:49,677 - INFO - Connected to Milvus Lite: ./milvus_final.db\n",
                        "2025-11-19 16:17:49,680 - INFO - Load pretrained SparseEncoder: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n",
                        "2025-11-19 16:17:52,362 - INFO - Loaded SPLADE model: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Database file found.\n",
                        "Index is already populated. SKIPPING ingestion.\n"
                    ]
                }
            ],
            "source": [
                "DB_FILE = \"./milvus_final.db\"\n",
                "\n",
                "db_exists = os.path.exists(DB_FILE)\n",
                "\n",
                "# Setup Store\n",
                "# we explicitly tell the store we are using Sparse only to save memory\n",
                "store = LocalMilvusStore(DB_FILE, enable_sparse=True, enable_dense=False)\n",
                "\n",
                "# we use a standard SPLADE model that works well on CPUs\n",
                "sparse_embedder = SpladeProvider(\n",
                "    model_name=\"opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\",\n",
                "    device=\"cpu\"\n",
                ")\n",
                "index = VerbatimIndex(vector_store=store, sparse_provider=sparse_embedder)\n",
                "\n",
                "if db_exists:\n",
                "    # The file exists on disk, so we check if Milvus can read it\n",
                "    print(\"Database file found.\")\n",
                "    try:\n",
                "        # We use a valid filter 'id != \"\"' instead of empty string\n",
                "        res = store.client.query(store.collection_name, filter='id != \"\"', limit=1)\n",
                "        if len(res) > 0:\n",
                "            print(\"Index is already populated. SKIPPING ingestion.\")\n",
                "        else:\n",
                "            print(\"Database exists but seems empty. Adding documents...\")\n",
                "            index.add_documents(documents_for_index)\n",
                "    except Exception as e:\n",
                "        print(f\"Database seems corrupted: {e}\")\n",
                "        print(\"deleting and rebuilding...\")\n",
                "        store.client.drop_collection(store.collection_name)\n",
                "        index.add_documents(documents_for_index)\n",
                "else:\n",
                "    print(\"New Database. Indexing documents...\")\n",
                "    index.add_documents(documents_for_index)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c45a916c",
            "metadata": {},
            "source": [
                "### Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "528a1e9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_best_paper(query_text, top_k=5):\n",
                "    print(f\"Querying: '{query_text}'\")\n",
                "    \n",
                "    results = index.query(query_text, k=top_k)\n",
                "    \n",
                "    if not results:\n",
                "        print(\"No matches found.\")\n",
                "        return None\n",
                "\n",
                "    # Dictionary to accumulate REAL scores\n",
                "    # {'Paper Title': 14.53}\n",
                "    paper_scores = defaultdict(float)\n",
                "    \n",
                "    print(f\"\\n--- Top {top_k} Chunks & Actual Similarity Scores ---\")\n",
                "    \n",
                "    for i, res in enumerate(results):\n",
                "        # 1. Get Metadata (Title)\n",
                "        meta = getattr(res, 'metadata', {}) or {}\n",
                "        if not meta and hasattr(res, 'get'): meta = res.get('metadata', {})\n",
                "        title = meta.get('title', meta.get('id', 'Unknown'))\n",
                "        \n",
                "        # 2. EXTRACT THE REAL SCORE\n",
                "        # We try common attribute names used by Milvus wrappers\n",
                "        score = getattr(res, 'score', None)\n",
                "        \n",
                "        # If .score is missing, sometimes it is called .distance\n",
                "        if score is None:\n",
                "            score = getattr(res, 'distance', 0.0)\n",
                "            \n",
                "        # 3. Add to Total\n",
                "        paper_scores[title] += score\n",
                "        \n",
                "        # 4. Print Result\n",
                "        # We print the score to 4 decimal places\n",
                "        snippet = getattr(res, 'text', getattr(res, 'content', ''))[:40].replace('\\n', '')\n",
                "        print(f\"Rank {i+1}: Score {score:.4f} | Paper: [{title[:20]}...] | Text: {snippet}...\")\n",
                "\n",
                "    # 5. Winner\n",
                "    sorted_papers = sorted(paper_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "    winner, total_score = sorted_papers[0]\n",
                "    \n",
                "    print(\"\\n--- Classification Result (Soft Voting) ---\")\n",
                "    print(f\"Predicted Paper: {winner}\")\n",
                "    print(f\"Total Similarity Score: {total_score:.4f}\")\n",
                "    print(f\"All Candidates: {dict(sorted_papers)}\")\n",
                "    \n",
                "    return winner"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b8239d7",
            "metadata": {},
            "source": [
                "### Enter Query"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 145,
            "id": "fa7e3546",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ”Ž Querying: 'How can we detect sarcasm using deep learning?'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.86it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Top 5 Chunks & Scores ---\n",
                        "Rank 1 (+5 pts): [Sarcasm Detection Using Deep C...] ...Sarcasm is a nuanced and often misinterpreted form...\n",
                        "Rank 2 (+4 pts): [Deep Learning-Based Detection ...] ...Deep Learning-Based Detection of Cognitive Impairm...\n",
                        "Rank 3 (+3 pts): [From Detection to Mitigation: ...] ...From Detection to Mitigation Addressing Bias in De...\n",
                        "Rank 4 (+2 pts): [Deep Learning Based Approach t...] ...Deep Learning Based Approach to Enhanced Recogniti...\n",
                        "Rank 5 (+1 pts): [Detecting spills using thermal...] ...Detecting spills using thermal imaging, pretrained...\n",
                        "\n",
                        "--- Classification Result ---\n",
                        "Predicted Paper: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
                        "Total Score: 5.0\n",
                        "All Scores: {'Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning': 5.0, 'Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization': 4.0, 'From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis': 3.0, 'Deep Learning Based Approach to Enhanced Recognition of Emotions and Behavioral Patterns of Autistic Children': 2.0, 'Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform': 1.0}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "query = \"How can we detect sarcasm using deep learning?\"\n",
                "\n",
                "predicted_paper = find_best_paper_weighted(query)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
