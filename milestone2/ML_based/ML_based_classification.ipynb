{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73a1cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets  #1 time installation\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "749f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from ipywidgets import Label, Text, Button, VBox\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "93ab8bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents from ../../corpus_json/corpus.json\n",
      "\n",
      "================================================================================\n",
      "DOCUMENT-LEVEL: Simple TF-IDF Vectorizer\n",
      "================================================================================\n",
      "Document-level TF-IDF vectorizer fitted on 20 documents\n",
      "Number of features: 5,000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "candidate_paths = [\n",
    "    Path(\"corpus_json/corpus.json\"),\n",
    "    Path(\"../corpus_json/corpus.json\"),\n",
    "    Path(\"../../corpus_json/corpus.json\"),\n",
    "]\n",
    "for p in candidate_paths:\n",
    "    if p.exists():\n",
    "        corpus_path = p\n",
    "        break\n",
    "\n",
    "with corpus_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "doc_ids = [doc[\"id\"] for doc in corpus]\n",
    "doc_titles = [doc.get(\"title\", \"\") for doc in corpus]\n",
    "doc_texts = [doc[\"text\"] for doc in corpus]\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents from {corpus_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DOCUMENT-LEVEL: Simple TF-IDF + Cosine Similarity (No Grid Search Needed)\n",
    "# ============================================================================\n",
    "# Document-level is just for initial ranking, not classification\n",
    "# We use simple TF-IDF + cosine similarity \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENT-LEVEL: Simple TF-IDF Vectorizer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simple TF-IDF vectorizer for document-level retrieval\n",
    "doc_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,  # Require words to appear in at least 2 documents\n",
    "    max_df=0.8,  # Ignore very common words\n",
    "    max_features=5000  # Limit feature space to prevent overfitting\n",
    ")\n",
    "\n",
    "# Fit on all documents\n",
    "doc_vectors = doc_vectorizer.fit_transform(doc_texts)\n",
    "print(f\"Document-level TF-IDF vectorizer fitted on {len(doc_texts)} documents\")\n",
    "print(f\"Number of features: {len(doc_vectorizer.get_feature_names_out()):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1004bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-level retrieval function using TF-IDF + Cosine Similarity\n",
    "def retrieve_docs_tfidf(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Returns top-k documents using TF-IDF + cosine similarity.\n",
    "    Simple and fast - no SVM needed for document-level ranking.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "    \n",
    "    # Transform query to TF-IDF vector\n",
    "    query_vec = doc_vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(query_vec, doc_vectors)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    topk_idx = np.argsort(similarities)[::-1][:k]\n",
    "    \n",
    "    results = []\n",
    "    for rank, idx in enumerate(topk_idx, start=1):\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(similarities[idx]),\n",
    "            \"id\": doc_ids[idx],\n",
    "            \"title\": doc_titles[idx],\n",
    "            \"text\": doc_texts[idx]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7bbe99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOCUMENT-LEVEL MODEL INFORMATION\n",
      "================================================================================\n",
      "Number of features: 5,000\n",
      "Feature reduction: From ~50,000+ to 5,000 features\n",
      "Reduction ratio: 90.0%\n",
      "\n",
      "Sample features: ['00' '00 00' '000' '000 images' '001' '006' '007' '009' '01' '010']\n",
      "\n",
      "Top 10 TF-IDF terms in Document 0:\n",
      "             term     tfidf\n",
      "961      mobility  0.420206\n",
      "645   forecasting  0.259764\n",
      "1013           nn  0.213715\n",
      "254          cell  0.176828\n",
      "1206       recall  0.162278\n",
      "277          city  0.160442\n",
      "901          lstm  0.158118\n",
      "1384      spatial  0.149042\n",
      "375          conv  0.132621\n",
      "1132    precision  0.125659\n",
      "\n",
      "✓ Document-level uses simple TF-IDF + cosine similarity\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display model information\n",
    "feature_names = doc_vectorizer.get_feature_names_out()\n",
    "print(\"=\"*80)\n",
    "print(\"DOCUMENT-LEVEL MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of features: {len(feature_names):,}\")\n",
    "print(f\"Feature reduction: From ~50,000+ to {len(feature_names):,} features\")\n",
    "print(f\"Reduction ratio: {(1 - len(feature_names)/50000)*100:.1f}%\")\n",
    "print(f\"\\nSample features: {feature_names[:10]}\")\n",
    "\n",
    "# Get feature importance from first document\n",
    "first_doc_vector = doc_vectorizer.transform([doc_texts[0]]).toarray()[0]\n",
    "nonzero_idx = first_doc_vector.nonzero()[0]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"term\": [feature_names[i] for i in nonzero_idx],\n",
    "    \"tfidf\": [first_doc_vector[i] for i in nonzero_idx]\n",
    "}).sort_values(\"tfidf\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF terms in Document 0:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n✓ Document-level uses simple TF-IDF + cosine similarity\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b23cb033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5153d88d76409b95aafe9854f679f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter your question:'), Text(value='', placeholder='Type your question here...'), …"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = Label(\"Enter your question:\")\n",
    "txt = Text(placeholder=\"Type your question here...\")\n",
    "btn = Button(description=\"Submit\")\n",
    "\n",
    "def on_click(b):\n",
    "    global query\n",
    "    query = txt.value\n",
    "    \n",
    "\n",
    "btn.on_click(on_click)\n",
    "\n",
    "VBox([label, txt, btn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "833a66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we detect sarcasm using deep learning?\n"
     ]
    }
   ],
   "source": [
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1c4b428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can we detect sarcasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] 2510.10729v1  (similarity=0.0342)\n",
      "Paper title: Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "--------------------------------------------------------------------------------\n",
      "Sarcasm is a nuanced and often misinterpreted form of communication especially in text where tone and body language are absent. This paper presents a proposed modular deep learning framework for sarcasm detection leveraging Deep Convolutional Neural Networks DCNNs and contextual models like BERT to analyze linguistic emotional and contextual cues. The system is conceptually designed to integrate sentiment analysis contextual embeddings linguistic feature extraction and emotion detection through  ...\n",
      "\n",
      "[2] 2510.08770v1  (similarity=0.0255)\n",
      "Paper title: Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform\n",
      "--------------------------------------------------------------------------------\n",
      "Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform. This paper presents a real-time spill detection system that utilizes pretrained deep learning models with RGB and thermal imaging to classify spill vs. no-spill scenarios across varied environments. Using a balanced binary dataset 4 000 images our experiments demonstrate the advantages of thermal imaging in inference speed accuracy and model size. We achieve up to 100% accuracy using lightweight mode ...\n",
      "\n",
      "[3] 2510.13137v1  (similarity=0.0175)\n",
      "Paper title: Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN\n",
      "--------------------------------------------------------------------------------\n",
      "Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN. This study investigates the performance of 3D Convolutional Neural Networks 3D CNNs and Long Short-Term Memory LSTM networks for real-time American Sign Language ASL recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1 20 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results at title level\n",
    "results = retrieve_docs_tfidf(query, k=3)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] {r['id']}  (similarity={r['score']:.4f})\")\n",
    "    print(f\"Paper title: {r['title']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d2206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=220, overlap=0):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    chunk_size: target words per chunk\n",
    "    overlap: how many words to overlap between consecutive chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(words)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        chunk = \" \".join(chunk_words)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        if end >= n:\n",
    "            break\n",
    "\n",
    "        start = end - overlap  \n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "752cc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_texts = []   \n",
    "passage_meta = []   \n",
    "\n",
    "for doc in corpus:\n",
    "    doc_id = doc[\"id\"]\n",
    "    title = doc.get(\"title\", \"\")\n",
    "    text = doc[\"text\"]\n",
    "\n",
    "    chunks = chunk_text(text, chunk_size=220, overlap=40)\n",
    "    start_word = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        end_word = start_word + len(chunk.split())\n",
    "        passage_texts.append(chunk)\n",
    "        passage_meta.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": f\"{doc_id}_chunk_{i}\",\n",
    "            \"start_word\": start_word,\n",
    "            \"end_word\": end_word,\n",
    "        })\n",
    "        start_word = end_word - 40  # keep aligned with overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f2a853e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 20\n",
      "Number of passages:  491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of passages:  {len(passage_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Sequential Split:\n",
      "Training:   281 chunks\n",
      "Validation: 103 chunks\n",
      "Test:       107 chunks\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get labels\n",
    "passage_labels = [meta[\"doc_id\"] for meta in passage_meta]\n",
    "unique_docs = list(set(passage_labels))\n",
    "\n",
    "X_train_pass, X_val_pass, X_test_pass = [], [], []\n",
    "y_train_pass, y_val_pass, y_test_pass = [], [], []\n",
    "\n",
    "# Iterate over each document to split it chronologically\n",
    "for doc_id in unique_docs:\n",
    "    # Find all chunk indices for this specific document\n",
    "    indices = [i for i, x in enumerate(passage_labels) if x == doc_id]\n",
    "    doc_texts = [passage_texts[i] for i in indices]\n",
    "    doc_labels = [passage_labels[i] for i in indices]\n",
    "    \n",
    "    # SKIP very small documents\n",
    "    if len(doc_texts) < 3:\n",
    "        X_train_pass.extend(doc_texts)\n",
    "        y_train_pass.extend(doc_labels)\n",
    "        continue\n",
    "\n",
    "    # SEQUENTIAL SPLIT \n",
    "    # We use shuffle=False to keep the order (Time-series split)\n",
    "    \n",
    "    # 1. Split off the Test set \n",
    "    texts_temp, texts_test, labels_temp, labels_test = train_test_split(\n",
    "        doc_texts, doc_labels, test_size=0.20, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 2. Split off the Validation set \n",
    "    # 0.25 of 80% is 20% of the total\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
    "        texts_temp, labels_temp, test_size=0.25, shuffle=False\n",
    "    )\n",
    "\n",
    "    X_train_pass.extend(texts_train)\n",
    "    y_train_pass.extend(labels_train)\n",
    "    \n",
    "    X_val_pass.extend(texts_val)\n",
    "    y_val_pass.extend(labels_val)\n",
    "    \n",
    "    X_test_pass.extend(texts_test)\n",
    "    y_test_pass.extend(labels_test)\n",
    "\n",
    "print(f\"New Sequential Split:\")\n",
    "print(f\"Training:   {len(X_train_pass)} chunks\")\n",
    "print(f\"Validation: {len(X_val_pass)} chunks\")\n",
    "print(f\"Test:       {len(X_test_pass)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fcfe3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING PASSAGE-LEVEL MODEL WITH GRID SEARCH\n",
      "================================================================================\n",
      "\n",
      "Best parameters: {'svc__C': 1, 'svc__kernel': 'linear', 'tfidfvectorizer__max_df': 0.8, 'tfidfvectorizer__max_features': 2000, 'tfidfvectorizer__min_df': 1}\n",
      "Best cross-validation score: 0.9750\n",
      "\n",
      "Passage-level validation accuracy: 0.9612\n",
      "Training set: 281 passages, Validation set: 103 passages\n",
      "Number of features: 2000\n",
      "\n",
      "Validation classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "2509.20913v1       0.71      1.00      0.83        10\n",
      "2509.23158v1       1.00      1.00      1.00         5\n",
      "2510.05163v1       1.00      1.00      1.00         3\n",
      "2510.05736v1       1.00      1.00      1.00         2\n",
      "2510.07320v1       1.00      1.00      1.00         4\n",
      "2510.08116v1       1.00      1.00      1.00         6\n",
      "2510.08411v1       1.00      1.00      1.00         4\n",
      "2510.08662v1       1.00      1.00      1.00         4\n",
      "2510.08770v1       1.00      0.75      0.86         4\n",
      "2510.09187v1       1.00      1.00      1.00         4\n",
      "2510.10729v1       1.00      1.00      1.00         2\n",
      "2510.10822v1       1.00      1.00      1.00         4\n",
      "2510.11073v1       1.00      1.00      1.00        10\n",
      "2510.11301v1       1.00      1.00      1.00         4\n",
      "2510.12758v1       1.00      1.00      1.00        10\n",
      "2510.12850v1       1.00      0.75      0.86         4\n",
      "2510.13050v1       1.00      0.75      0.86         8\n",
      "2510.13137v1       1.00      1.00      1.00         4\n",
      "2510.13937v1       1.00      1.00      1.00         5\n",
      "2510.14855v1       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.96       103\n",
      "   macro avg       0.99      0.96      0.97       103\n",
      "weighted avg       0.97      0.96      0.96       103\n",
      "\n",
      "\n",
      "Passage-level TEST accuracy: 0.9065\n",
      "Test set: 107 passages\n",
      "Test set classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "2509.20913v1       0.52      1.00      0.69        11\n",
      "2509.23158v1       1.00      1.00      1.00         5\n",
      "2510.05163v1       1.00      1.00      1.00         3\n",
      "2510.05736v1       1.00      1.00      1.00         2\n",
      "2510.07320v1       1.00      1.00      1.00         4\n",
      "2510.08116v1       1.00      1.00      1.00         6\n",
      "2510.08411v1       1.00      0.75      0.86         4\n",
      "2510.08662v1       1.00      1.00      1.00         5\n",
      "2510.08770v1       1.00      0.75      0.86         4\n",
      "2510.09187v1       1.00      0.75      0.86         4\n",
      "2510.10729v1       1.00      1.00      1.00         3\n",
      "2510.10822v1       1.00      0.80      0.89         5\n",
      "2510.11073v1       1.00      0.70      0.82        10\n",
      "2510.11301v1       1.00      1.00      1.00         4\n",
      "2510.12758v1       1.00      1.00      1.00        10\n",
      "2510.12850v1       1.00      0.75      0.86         4\n",
      "2510.13050v1       1.00      0.75      0.86         8\n",
      "2510.13137v1       1.00      1.00      1.00         4\n",
      "2510.13937v1       1.00      1.00      1.00         5\n",
      "2510.14855v1       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.91       107\n",
      "   macro avg       0.98      0.91      0.93       107\n",
      "weighted avg       0.95      0.91      0.91       107\n",
      "\n",
      "\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.9612\n",
      "Difference (overfitting indicator): 0.0388\n",
      "\n",
      "================================================================================\n",
      "TRAINING FINAL MODEL ON ALL DATA\n",
      "================================================================================\n",
      "Passage-level SVM model trained on 491 passages (all data)\n",
      "Final model features: 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING PASSAGE-LEVEL MODEL WITH GRID SEARCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__min_df': [1, 2],\n",
    "    'tfidfvectorizer__max_df': [0.8, 0.9, 1.0],\n",
    "    'tfidfvectorizer__max_features': [2000, 3000, 5000],\n",
    "    'svc__C': [0.1, 1, 5],\n",
    "    'svc__kernel': ['linear']\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "passage_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2)),\n",
    "    SVC(probability=True, random_state=42)\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "passage_grid_search = GridSearchCV(\n",
    "    passage_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "passage_grid_search.fit(X_train_pass, y_train_pass)\n",
    "\n",
    "print(f\"\\nBest parameters: {passage_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {passage_grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "passage_model = passage_grid_search.best_estimator_\n",
    "passage_vectorizer = passage_model.named_steps['tfidfvectorizer']\n",
    "\n",
    "# Validate on validation set\n",
    "val_pred_pass = passage_model.predict(X_val_pass)\n",
    "val_acc_pass = accuracy_score(y_val_pass, val_pred_pass)\n",
    "print(f\"\\nPassage-level validation accuracy: {val_acc_pass:.4f}\")\n",
    "print(f\"Training set: {len(X_train_pass)} passages, Validation set: {len(X_val_pass)} passages\")\n",
    "print(f\"Number of features: {len(passage_vectorizer.get_feature_names_out())}\")\n",
    "print(\"\\nValidation classification report:\")\n",
    "print(classification_report(y_val_pass, val_pred_pass, zero_division=0))\n",
    "\n",
    "# Test on held-out test set\n",
    "test_pred_pass = passage_model.predict(X_test_pass)\n",
    "test_acc_pass = accuracy_score(y_test_pass, test_pred_pass)\n",
    "print(f\"\\nPassage-level TEST accuracy: {test_acc_pass:.4f}\")\n",
    "print(f\"Test set: {len(X_test_pass)} passages\")\n",
    "print(\"Test set classification report:\")\n",
    "print(classification_report(y_test_pass, test_pred_pass, zero_division=0))\n",
    "\n",
    "# Check for overfitting: compare train vs validation accuracy\n",
    "train_pred_pass = passage_model.predict(X_train_pass)\n",
    "train_acc_pass = accuracy_score(y_train_pass, train_pred_pass)\n",
    "print(f\"\\nTraining accuracy: {train_acc_pass:.4f}\")\n",
    "print(f\"Validation accuracy: {val_acc_pass:.4f}\")\n",
    "print(f\"Difference (overfitting indicator): {train_acc_pass - val_acc_pass:.4f}\")\n",
    "if train_acc_pass - val_acc_pass > 0.1:\n",
    "    print(\"WARNING: Large gap between train and validation accuracy suggests overfitting!\")\n",
    "\n",
    "# Train final model on all data for production use (using best parameters)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON ALL DATA\")\n",
    "print(\"=\"*80)\n",
    "final_passage_model = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=passage_grid_search.best_params_['tfidfvectorizer__min_df'],\n",
    "        max_df=passage_grid_search.best_params_['tfidfvectorizer__max_df'],\n",
    "        max_features=passage_grid_search.best_params_['tfidfvectorizer__max_features']\n",
    "    ),\n",
    "    SVC(\n",
    "        C=passage_grid_search.best_params_['svc__C'],\n",
    "        kernel=passage_grid_search.best_params_['svc__kernel'],\n",
    "        probability=True,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "final_passage_model.fit(passage_texts, passage_labels)\n",
    "passage_model = final_passage_model\n",
    "passage_vectorizer = passage_model.named_steps['tfidfvectorizer']\n",
    "print(f\"Passage-level SVM model trained on {len(passage_texts)} passages (all data)\")\n",
    "print(f\"Final model features: {len(passage_vectorizer.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b827e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_svm_chunks(query: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages (chunks) using SVM classification + Cosine Re-ranking.\n",
    "    \"\"\"\n",
    "    if not query.strip():\n",
    "        return []\n",
    "\n",
    "    vectorizer = passage_model.named_steps['tfidfvectorizer']\n",
    "\n",
    "    proba = passage_model.predict_proba([query])[0]\n",
    "    classes = passage_model.classes_\n",
    "    \n",
    "    topk_idx = np.argsort(proba)[::-1][:k]\n",
    "    topk_doc_ids = [classes[idx] for idx in topk_idx]\n",
    "\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for rank, doc_id in enumerate(topk_doc_ids, start=1):\n",
    "\n",
    "        doc_passage_indices = [i for i, label in enumerate(passage_labels) if label == doc_id]\n",
    "        \n",
    "        if doc_passage_indices:\n",
    "            candidate_texts = [passage_texts[i] for i in doc_passage_indices]\n",
    "            \n",
    "            doc_passage_vectors = vectorizer.transform(candidate_texts)\n",
    "            \n",
    "            sims = cosine_similarity(query_vec, doc_passage_vectors)[0]\n",
    "            \n",
    "            best_local_idx = np.argmax(sims)\n",
    "            best_global_idx = doc_passage_indices[best_local_idx]\n",
    "            \n",
    "            meta = passage_meta[best_global_idx]\n",
    "            doc_prob = proba[classes.tolist().index(doc_id)]\n",
    "            \n",
    "            results.append({\n",
    "                \"rank\": rank,\n",
    "                \"score\": float(doc_prob), \n",
    "                \"sim_score\": float(sims[best_local_idx]), \n",
    "                \"text\": passage_texts[best_global_idx],\n",
    "                \"doc_id\": meta[\"doc_id\"],\n",
    "                \"title\": meta[\"title\"],\n",
    "                \"chunk_id\": meta[\"chunk_id\"],\n",
    "                \"start_word\": meta[\"start_word\"],\n",
    "                \"end_word\": meta[\"end_word\"],\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8af5a0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can we detect sarcasm using deep learning?\n",
      "--------------------------------------------------------------------------------\n",
      "[1] SVM probability=0.2010\n",
      "Paper: 2510.10729v1 — Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning\n",
      "Chunk: 2510.10729v1_chunk_2 (words 360–580)\n",
      "--------------------------------------------------------------------------------\n",
      "text using BERT for text embeddings and Dense Net for visual features. Sarcasm detection is vital for enhancing the interpretability of automated systems like sentiment analyzers chatbots and recommendation engines. While humans rely on context tone and expressions machines must infer sarcasm from textual patterns alone. This paper explores a conceptual solution using DCNNs combined with contextual embedding models to understand sarcasm s complex indicators such as irony sentiment contradiction  ...\n",
      "\n",
      "[2] SVM probability=0.1143\n",
      "Paper: 2509.20913v1 — Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales\n",
      "Chunk: 2509.20913v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales. To develop a deep learning framework to evaluate if and how incorporating micro-level mobility features alongside historical crime and sociodemo-graphic data enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. We advance the literature on computational methods and crime forecasting by focusing on four U.S. cities i.e. Baltimore Chicago Los Angeles an ...\n",
      "\n",
      "[3] SVM probability=0.0783\n",
      "Paper: 2510.11073v1 — ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer\n",
      "Chunk: 2510.11073v1_chunk_0 (words 0–220)\n",
      "--------------------------------------------------------------------------------\n",
      "ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer. Patient face images provide a convenient mean for evaluating eye diseases while also raising privacy concerns. Here we introduce ROFI a deep learning-based privacy protection framework for ophthal-mology. Using weakly supervised learning and neural identity translation ROFI anonymizes facial features while retaining disease features over 98% accuracy κ 0.90. It achieves 100% diagnostic sensitivity and  ...\n",
      "\n",
      "[4] SVM probability=0.0652\n",
      "Paper: 2510.08662v1 — DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops\n",
      "Chunk: 2510.08662v1_chunk_3 (words 540–760)\n",
      "--------------------------------------------------------------------------------\n",
      "unprecedented challenges to food conventional GS models often inadequately capture complex non-additive genetic effects which limits their prediction accuracy and robustness. Recently deep learning methods have demonstrated remarkable efficacy in data modeling across diverse scientific domains. Their capacity to automatically learn complex features enables the effective modeling of non-linear relationships between genotype and phenotype rendering them highly suitable for genomic prediction. Geno ...\n",
      "\n",
      "[5] SVM probability=0.0615\n",
      "Paper: 2510.12758v1 — PET Head Motion Estimation Using Supervised Deep Learning with Attention\n",
      "Chunk: 2510.12758v1_chunk_46 (words 8280–8500)\n",
      "--------------------------------------------------------------------------------\n",
      "the opposite manner using a model pre-trained on 18F and applying to 11C at test time suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites using the pre-trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans-formation metrics from different images generated from PET raw data.  ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = retrieve_svm_chunks(query, k=5)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"[{r['rank']}] SVM probability={r['score']:.4f}\")\n",
    "    print(f\"Paper: {r['doc_id']} — {r['title']}\")\n",
    "    print(f\"Chunk: {r['chunk_id']} (words {r['start_word']}–{r['end_word']})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:500], \"...\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
