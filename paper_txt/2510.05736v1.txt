Convolution and Graph-based Deep Learning Approaches for Gamma/Hadron Separation in Imaging Atmospheric Cherenkov Telescopes ar Xiv:2510.05736v1 [astro-ph.HE] 7 Oct 2025 Abhay Mehta,a,b,∗Dan Parsons,a,b Tim Lukas Holch,a David Bergea,band Matthias Weidlichb a Deutsches Elektronen-Synchrotron DESY, Platanenallee 6, 15738 Zeuthen, Germany b Humboldt-Universität zu Berlin, Unter den Linden 6, 10117 Berlin, Germany E-mail: abhay.mehta@desy.de The identification of γ-rays from the predominant hadronic-background is a key aspect in their ground-based detection using Imaging Atmospheric Cherenkov Telescopes (IACTs). While cur- rent methods are limited in their ability to exploit correlations in complex data, deep learning-based models offer a promising alternative by directly leveraging image-level information. However, several challenges involving the robustness and applicability of such models remain. Designing model architectures with inductive biases relevant for the task can help mitigate the problem. Three such deep learning-based models are proposed, trained, and evaluated on simulated data: (1) a hybrid convolutional and graph neural network model (CNN-GNN) using both image and graph data; (2) an enhanced CNN-GNN variant that incorporates additional reconstructed information within the graph construction; and (3) a graph neural network (GNN) model using image moments serving as a baseline. The new combined convolution and graph-based approach demonstrates improved performance over traditional methods, and the inclusion of reconstructed information offers further potential in generalization capabilities on real observational data. 39th International Cosmic Ray Conference (ICRC2025) 15–24 July 2025 Geneva, Switzerland ∗Speaker © Copyright owned by the author(s) under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives 4.0 International License (CC BY-NC-ND 4.0). https://pos.sissa.it/ Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta The field of very-high-energy (VHE) γ-ray astronomy has evolved significantly over the past three decades, driven largely due to observations from ground-based imaging telescopes. These Imaging Atmospheric Cherenkov Telescopes (IACTs) capture Cherenkov light produced by highly energetic particles interacting with the Earth’s atmosphere . Each camera image represents a two-dimensional projection of the Cherenkov light pool from the telescope’s position, and each event typically consists of multiple images taken simultaneously across an array of telescopes. This stereoscopic information is used to reconstruct the energy, direction, and type of the primary particle initiating the air shower. Event statistics, however, are dominated by hadron-induced air showers, which can outnumber γ-ray air showers by up to a factor of 104 . This makes the task of identifying γ-ray events from the hadronic background a central challenge for IACT-based observations. Hadronic air showers are fundamentally different from those initiated by γ-rays, which are electromagnetic in nature. This difference in shower development leads to small but significant differences in the camera images, which form the basis for separating the two event classes. Current- generation IACTs typically rely on Boosted Decision Trees (BDTs) trained on parameterized image features or goodness-of-fit parameters for this task [3–5]. Consequently, a natural motivation for exploring deep learning-based models stems from the possibility of improving event classification by directly using image-level information. Multiple studies have explored deep learning methods for identifying γ-rays and demonstrated exceptional performance on simulated data [6–10]. Most model architectures use convolutional neu- ral networks (CNNs) for extracting information from camera images and recurrent neural networks (RNNs) for its aggregation across an event. Graph neural networks (GNNs), applied on images represented as point clouds, have also been established as a viable approach for the same task . Despite their potential, a complete deployment of deep learning-based models on IACT data re- mains non-trivial due to a variety of issues, ranging from observational systematics to discrepancies between simulations and real-world data. The construction of models that can generalize to “unseen" situations is a long-standing problem in deep learning . The use of network architectures with inductive biases suitable for the given task can lead to improved generalizations by guiding the learning process towards more physically meaningful representations . For example, translational invariance in CNNs, temporal dependence in RNNs, and permutation equivariance in GNNs are all properties that align naturally with the structure of the data these models are typically applied to, thus contributing to their success in their domains. In the context of IACTs, as each image is a projection of the same event, there is no inherent ordering between them, making them permutation equivariant. This motivates a shift towards exploring GNNs for aggregating information across multiple telescopes. To that end, this work introduces a combined convolutional and graph neural network (CNN-GNN) based approach for γ/hadron separation in IACTs. 2 Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta Three models, with two distinct training strategies, are proposed and evaluated on simulated data. These include a CNN-GNN model trained on image and graph data, an enhanced CNN-GNN variant with additional reconstructed event information incorporated into the graph structure, and a baseline GNN model utilizing image moments, serving as a reference for existing methods. The models were trained on simulations of the High Energy Stereoscopic System (H.E.S.S.) located in Göllschau, Namibia . The H.E.S.S. array consists of four 12-meter telescopes (CT1-4) arranged in a square of side 120 meters, with an additional 28-meter telescope (CT5) at its center. Diffuse proton and γ-ray events, simulated at a zenith of 20◦and with a maximum view cone of 5◦, were chosen as the two event classes for this task. Similar models were also trained on the simulations of the upcoming Cherenkov Telescope Array Observatory , the results of which are not presented here. A custom data processing pipeline, based on ctapipe and Py Torch, was developed for the subsequent analysis of IACT data. The framework within the ctapipe v0.19.3 package was used to handle the low-level data processing tasks such as image calibration and cleaning, while the construction and training of the convolution and graph-based models were implemented using Py Torch v2.0.1 and Py Torch Geometric v2.4.0 , respectively. Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta γ-ray or hadron-induced based on the event images and information. To this end, the aim of the convolutional (CNN) half of the model remains to extract relevant image-level features from each telescope, while the graph-based (GNN) half is given additional contextual information (described in Section 3) to better learn the classification task. The output of the CNN also forms part of the input to the GNN, allowing the model to combine image features with telescope-level information during training. A key advantage of the graph representation in GNNs is its flexibility in handling varying numbers and types of telescopes present in IACT arrays . This is also relevant for when subsets of the array operate due to technical or observational constraints. Additionally, the graph approach also allows for inclusion of telescope-specific information, such as position, reconstructed parameters, temporal relationships, making the system more adaptable to the particularities of real-world data. The CNN component is a simple implementation inspired from the Inception module , employing convolution filters of multiple sizes in parallel to learn features at different spatial scales. Five convolutions with kernel sizes ranging from 1 to 15 are applied to each image and the output subsequently passed through two additional convolutional layers with max-pooling. The result is then flattened and processed through two fully connected layers, with dropout regularization applied before the final layer. The network operates on images from each telescope independently and returns a fixed-dimensional embedding for each telescope. The feature vectors extracted by the CNN are subsequently passed to the GNN component, which consists of a multi-layer Edge Conv architecture with residual concatenation across layers, following the original design . After message passing, the learned node representations are aggregated globally using a combination of mean, max, and sum pooling operations to form a graph-level event embedding, which is then used for classification. Alternative GNN layers such as GCNConv and GATConv were also explored, but no significant difference in performance was observed. A key aim of the work was to examine if the inclusion of reconstructed information in the model training contributed to improvements in performance. Consequently, three models with different GNN components were developed: a Fast CNN-GNN, a Strong GNN and a Hillas GNN. The specifics follow- • Fast CNN-GNN: Combines features extracted from camera images with telescope positions (in the ground frame), total image and maximum pixel intensity per image. • Strong CNN-GNN: An enhanced version which incorporates the reconstructed core position of the air shower. This is implemented by shifting the telescope positions in the tilted ground frame1 by centering the origin at the reconstructed impact point (Figure 2). • Hillas GNN: A simplified GNN trained solely on Hillas parameters extracted from the camera images serving as the baseline model. 1The tilted ground frame is the ground frame transformed according to the telescope’s pointing direction and is used for reconstructing the shower core position. 4 Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta corresponding to the area under curve (AUC) values in each case. All models are successfully able to learn to classify events between γ-rays and protons and the CNN-GNN models outperform the Hillas-based GNN model as well. Upon testing on a dataset with an additional local distance cut on images, as in , the CNN-GNN approach matches the classification performance of the previous models. Similar results were also seen when the same approach was applied and evaluated on simulations of the CTAO array. Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta where a higher Δ AUC fscore indicates a greater importance of that feature to the model’s decision process. The results of this computation for the Hillas GNN (n= 50) and the Fast CNN-GNN (n= 10) model trained via the split-training approach are shown in Convolution and graph-based deep learning approaches for γ/hadron separation in IACTs Abhay Mehta The authors thank the H.E.S.S. collaboration for the access to and use of the H.E.S.S. simula- tions. They also thank Iftach Sadeh for the valuable input and feedback during the course of this work.