— This study investigates the performance of 3D Convolutional Neural Networks (3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal feature extraction from video sequences, LSTMs are optimized for modeling temporal dependencies in sequential data. We evaluate both architectures on a dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy, computational efficiency, and latency under similar training conditions. Experimental results demonstrate that 3D CNNs achieve 92.4% recognition accuracy but require 3.2× more processing time per frame compared to LSTMs, which maintain 86.7% accuracy with significantly lower resource consumption. The hybrid 3D CNN- LSTM model shows decent performance, which suggests that context-dependent architecture selection is crucial for practical implementation. This project provides professional benchmarks for developing assistive technologies, highlighting trade-offs between recognition precision and real-time operational requirements in edge computing environments. II. LITERATURE REVIEW In their study, Necati Cihan Camgoz et al. demonstrate the effectiveness of 3D CNNs in capturing spatiotemporal features from raw sign language videos. Their work highlights the model’s ability to process both spatial (hand shapes) and temporal (gesture motion) information simultaneously, achieving high accuracy in gesture classification. A key insight from this paper is the trade-off between computational complexity and performance, as 3D CNNs require significant resources but excel in recognizing static and visually distinct signs. This informed our comparative analysis, where we evaluated 3D CNNs against LSTMs for real-time applicability. In another study, M. Al-Qurishi et al. provide a comprehensive survey of deep learning approaches, including 3D CNNs and LSTMs, for sign language recognition. The paper systematically compares model performance across public benchmarks, highlighting strengths (e.g., 3D CNN accuracy on spatial features) and limitations (e.g., LSTM dependency on sequential data). A critical takeaway is the discussion on open challenges, such as real-time deployment and dataset scarcity, which directly informed our methodology for balancing accuracy and computational efficiency in this work. In their comprehensive review, Yanqiong Zhang and Xianwei Jiang analyze cutting-edge techniques, including 3D CNNs, Transformers, and hybrid models, for sign language recognition. Published in CMES (2024), their work highlights breakthroughs in spatiotemporal modeling and addresses challenges like limited datasets and cross-signer Keywords —LSTM, 3D-CNN, Sign language recognition, ML, Assistive Technology I. INTRODUCTION S ign language is the primary way that people in the Deaf and Hard of Hearing (DHH) community communicate, yet there’s still a big gap because automated translation tech isn’t widely used. Recent advancements in deep learning have facilitated notable progress in the development of vision- based systems capable of recognizing and transcribing sign language into text. Two prominent methodologies include Long Short-Term Memory (LSTM) networks, which are proficient in modeling the temporal sequence of hand movements, and 3D Convolutional Neural Networks (3D CNNs), which process both the visual and time-based aspects of sign language videos all at once. LSTMs are good at tracking the flow of gestures over time, while 3D CNNs excel at capturing both spatial configurations and dynamic movements. This paper presents a comparative analysis of these two approaches for the translation of individual American Sign Language (ASL) gestures into text. We look at how accurate they are, how much computing power they need, what’s required to train them, and how well they work in real-time situations. Using Media Pipe for reliable hand tracking, we’ve F.Y.B. Tech Students’ Applied Science & Engineering Project1 (ASEP1) Paper, SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology, Pune, INDIA. generalization. A key insight is the growing use of attention mechanisms to improve long-sequence gesture recognition, which aligns with our exploration of LSTM-based temporal modeling. Their comparative evaluation of model architectures further validates our methodological choice to prioritize real-time efficiency without sacrificing accuracy. In their work, Ur Rehman et al. propose a hybrid deep learning framework combining 3D-CNNs and LSTMs to leverage spatial and temporal features for improved gesture recognition. Their experiments demonstrate that 3D-CNNs effectively capture hand shape and motion patterns, while LSTMs model long-term dependencies in gesture sequences. A key finding is the superior accuracy of their hybrid approach compared to standalone models, though at the cost of increased computational complexity. This study reinforced our decision to evaluate both architectures independently, while also highlighting potential future directions—such as hybrid models—to enhance real-time performance. In their study, X. Ouyang et al. propose a novel multi-task learning architecture integrating 3D-CNNs and LSTMs to jointly model spatial and temporal features for action recognition. The 3D-CNN extracts spatiotemporal hierarchies from video inputs, while the LSTM captures long-range dependencies across frames. A key innovation is their multi- task framework, which simultaneously optimizes for action classification and localization, improving generalization. The authors demonstrate state-of-the-art performance on benchmark datasets (e.g., UCF101), though they note the computational overhead of combining these architectures. This work informed our comparative analysis by highlighting the trade-offs between accuracy and real-time feasibility—a central theme in our evaluation of standalone 3D-CNN and LSTM models for sign language recognition. In his study, Dushyant Kumar Singh demonstrates the effectiveness of 3D-CNNs in recognizing dynamic gestures within Indian Sign Language (ISL). The paper highlights the model’s ability to extract spatiotemporal features directly from raw video inputs, achieving robust performance on ISL datasets. A critical insight is the model’s sensitivity to computational resources, which aligns with our findings on the trade-offs between 3D-CNN accuracy and real-time deployment constraints. This work further validates our comparative framework, particularly in evaluating spatial- temporal architectures for region-specific sign languages. In their innovative work, Ma et al. propose an enhanced 3D-CNN model incorporating attention mechanisms to improve focus on salient spatiotemporal features in sign language videos. Presented at *IEEE ICCE-Asia 2022*, their system achieves a 92.3% recognition rate by dynamically weighting critical frames and hand regions, reducing noise from irrelevant background motions. A key insight is the attention mechanism’s ability to boost interpretability while maintaining real-time performance (∼45ms latency on GPU). This work aligns with our exploration of 3D-CNNs’ strengths in spatial modeling, while their attention framework offers potential future direction to address our observed challenges in subtle gesture differentiation. P. Sinha et al. demonstrate the effectiveness of a CNN- LSTM hybrid for sign language recognition but note its computational overhead. This trade-off motivates our direct comparison of standalone 3D CNNs and LSTMs. While 3D CNNs excel at joint spatiotemporal feature extraction, their high complexity challenges real-time deployment. LSTMs, conversely, efficiently model temporal dynamics but lack innate spatial processing. Our evaluation extends Sinha et al.’s work by quantifying this accuracy-efficiency dichotomy, particularly for edge-device scenarios, and explores optimizations like depthwise-separable 3D convolutions for latency-sensitive applications. The author in this paper proposes an attention-enhanced CNN-LSTM architecture for isolated sign language recognition, addressing the limitations of traditional hybrid models in focusing on discriminative spatiotemporal features. Their framework employs a 3D CNN backbone to extract hierarchical spatial-temporal features, followed by an attention-LSTM to dynamically weight salient frames, achieving state-of-the-art accuracy on benchmark datasets (e.g., WLASL). However, the authors highlight the increased computational cost of attention mechanisms, necessitating a trade-off between precision and real-time performance. This work informs our evaluation of attention mechanisms in resource-constrained settings, where we explore pruning techniques to optimize their architecture for edge deployment. D. D. Meshram et al. provide a comprehensive review of deep learning-based approaches for Indian Sign Language (ISL) recognition, systematically comparing architectures like CNNs, LSTMs, and hybrid models. Their analysis reveals that 3D CNNs dominate spatial-temporal feature extraction for ISL videos, while attention-based LSTMs improve accuracy for continuous signs by modeling long-range dependencies. The authors critically highlight key challenges, including the scarcity of annotated ISL datasets and computational constraints for real-time mobile deployment. This review underscores the need for lightweight, region-specific models—a gap our work addresses through optimized spatial-temporal attention mechanisms and transfer learning on limited ISL data. III. METHODOLOGY The primary objective of this project is to develop an efficient and accurate system for translating sign language gestures into text. The methodology is structured into four main stages: data acquisition, preprocessing, model training, and performance evaluation. We have developed a LSTM (Long Short-Term Memory) Model and compared it with 3D – CNN Model architecture. Data Acquisition and Preprocessing: F.Y.B. Tech Students’ Applied Science & Engineering Project1 (ASEP1) Paper, SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology, Pune, INDIA. This work utilizes publicly available datasets, including the Indian Sign Language (ISL) dataset and an American Sign Language (ASL) dataset. The datasets comprise both static hand postures and dynamic gesture sequences corresponding to alphabets and numerals. Steps: (e.g., 36 for alphabets A–Z and digits 0–9). This produces a probability distribution over all gesture classes At the end of the architecture, we have the Output layer, which uses a softmax activation function. This layer is designed with exactly as many units as there are gesture classes to recognize—for example, 36 units to cover the alphabet (A-Z) and numbers (0-9). The softmax layer processes the Dense layer outputs and generates a probability distribution, giving a confidence score for each possible gesture. This means that for any given hand movement, the model not only identifies the most likely character but also provides a sense of how certain it is about each potential match, making it easier to trust and interpret its predictions. Advantages of LSTM Model:  Temporal Awareness: Unlike traditional CNNs which only analyze spatial features, LSTM models inherently understand the temporal evolution of gestures.  Handles Variable-Length Inputs: LSTM can process sequences of varying lengths, making it robust to different gesture speeds and durations.  Real-Time Capability: The model’s relatively small computational footprint allows it to run in real time on standard consumer hardware without requiring a GPU.  Noise Tolerance: Since the input is based on 3D hand landmarks rather than raw pixel data, the model is less sensitive to background noise and lighting variations, improving robustness in diverse environments  Scalability: The model can be easily extended to learn phrases or full sign language sentences by feeding longer sequences or stacking gesture outputs. 3D Convolutional Neural Network (3D CNN) Model: While LSTMs excel at learning temporal dependencies in sequential data, Convolutional Neural Networks (CNNs) particularly 3D CNNs offer a powerful alternative by learning spatiotemporal features directly from raw video input. A 3D CNN applies convolutional filters across both spatial dimensions (height, width) and the temporal dimension (time), making it especially suitable for video classification tasks where both motion and appearance are important.  Images and sequences of frames were extracted from real-time webcam feed or pre-recorded datasets.  Media Pipe was used to extract 3D hand landmarks (21 points per hand, each with x, y, z coordinates), resulting in 63 features per frame for single-hand tracking.  These features were normalized and reshaped to prepare them for time-series or spatial analysis, depending on the model. LSTM-Based Sign Language Recognition Model: The Long Short-Term Memory (LSTM) network, a type of Recurrent Neural Network (RNN), is particularly well-suited for sequence prediction problems, especially when there are long-term dependencies across time steps. In the context of sign language recognition, gestures are basically sequential- coming in sequences, a sign is not just a static posture but also use of hand movements over time. LSTM networks are capable of learning and remembering this information, making them highly effective for dynamic gesture recognition problems. The goal of the LSTM model in our system is to interpret a continuous stream of hand gestures captured in real-time from a webcam and convert them into corresponding alphabets or numbers. The model uses sequences of hand landmark coordinates, which are numerical representations of the spatial position of each key point on the hand across time. Model Architecture: The model is made of LSTM (Long Short-Term Memory) layers, capturing the entire trajectory of a hand gesture rather than just its position at one moment. These layers are designed to work as a team: the lower layers zero in on subtle details, like slight changes in finger positioning or wrist angles, while the higher layers build on this to understand the broader "movement signature" that defines a specific gesture, such as the fluid motion of signing a letter or number. To ensure the model doesn’t just memorize the training examples and can adapt to new, unseen gestures, we include a Dropout layer right after the LSTMs. During training, this layer randomly deactivates a portion of neurons, forcing the model to learn more flexible patterns. It’s like training the network to stay sharp even when some of its tools are temporarily unavailable, which helps it generalize better and perform reliably on fresh data. After the LSTM layers, fully connected (Dense) layers are employed to transform the temporal features into high- dimensional representations for classification. The final output layer applies a softmax activation function with C units, where C corresponds to the number of gesture classes In this research, we use a 3D CNN architecture as a comparative baseline to evaluate how well a spatial-temporal convolutional approach performs against the LSTM model for real-time sign language gesture recognition. Model Architecture: The 3D convolutional layers act as the core feature extractors in our model. They process short video clips using volumetric kernels that look at both the spatial layout (what’s happening in each frame) and the temporal flow (how things change over time). For instance, a 3×3×3 kernel analyzes a small 3×3 area across three consecutive frames, F.Y.B. Tech Students’ Applied Science & Engineering Project1 (ASEP1) Paper, SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology, Pune, INDIA.  3D CNN: Learns temporal features implicitly through 3D convolutions but is not as specialized in modeling long-term dependencies as LSTM. but also how they move—both of which are crucial for recognizing signs. Multiple 3D convolutional layers are stacked sequentially to progressively learn higher-level spatiotemporal patterns. Each convolutional block is followed by 3D max-pooling layers, which reduce spatial-temporal resolution while retaining salient features. Batch normalization is incorporated to stabilize training, and dropout layers are applied to mitigate overfitting by randomly deactivating neurons during training.  LSTM: Limited, relies only on coordinate data i.e.; no texture, color, or visual details  3D CNN: High spatial awareness due to access to pixel-level visual features in the input video.  LSTM: Excels in recognizing dynamic gestures involving motion over time.  3D CNN: Performs better for static or shape- dominant gestures due to its strong spatial feature extraction. At the end of the network, fully connected layers pull everything together to make a final prediction. The final softmax layer outputs a probability distribution over the gesture classes, indicating the model's prediction and its associated confidence level. Overall, this architecture is designed to fully capture both the visual details and the motion dynamics of sign language, all while staying efficient enough for practical use.  LSTM: Lightweight, requires less memory and computational power. Suitable for real-time and edge applications.  3D CNN: Computationally heavy; needs a GPU and high RAM for real-time performance. Advantages of 3D CNNs:  Spatiotemporal Feature Learning: Learns both motion and hand shape feature directly from raw frames without needing hand landmarks or key points.  No Feature Engineering Required: Unlike LSTM models which require pre-extracted landmarks (using Media Pipe, etc.), 3D CNNs learn directly from video data.  High Expressiveness: Can capture subtle differences in hand shapes and movements that might be lost in coordinate-only inputs. The LSTM-based approach provides a strong baseline for gesture recognition and serves as the backbone of our real- time sign-to-text translator application. In this research, we compare it with a 3D CNN architecture to evaluate its trade- offs in terms of accuracy, speed, and usability in real-world scenarios. LSTM vs 3D-CNN: A comparison  LSTM: Can generalize well on smaller datasets due to fewer trainable parameters.  3D CNN: Requires large amounts of labeled video data to avoid overfitting and learn robust features.  LSTM: Requires hand detection and landmark extraction (here, via Media Pipe), but reduces input dimensionality significantly.  3D CNN: Requires raw video clips, often with cropping, resizing, normalization, and augmentation.  LSTM: Easier to interpret as it works on landmarks; errors can be traced to motion or key point misalignment.  3D CNN: More complex to interpret; difficult to pinpoint which pixel regions influence predictions. Comparison:  LSTM: Uses pre-extracted hand landmarks (x, y, z coordinates of 21 key points per frame). These are fed as sequences (e.g., 30 frames × 63 features).  3D CNN: Takes raw video frames as input (e.g., 30 RGB frames of 128×128 pixels), preserving both shape and motion directly. 3D CNN Model Input Sequence of 3D hand landmarks Raw video frames (e.g., 30×128×128×3)  LSTM: Explicitly designed for sequential data, making it naturally suited for time-dependent gestures. Focus Temporal sequence modelling Spatiotemporal feature extraction F.Y.B. Tech Students’ Applied Science & Engineering Project1 (ASEP1) Paper, SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology, Pune, INDIA. Additionally, while the 3D CNN was slightly more accurate in offline evaluation, it showed signs of overfitting and struggled with fast-changing or subtle dynamic gestures in live scenarios. Best used for Static/Dynamic pictures Static as well as visually distinctive gestures Spatial Context Strong (learns from raw images) Temporal Modelling Limited (no texture or shape info) User testing and qualitative observations reinforced these results: the LSTM model demonstrated higher responsiveness and robustness in live video input, making it preferable for interactive applications such as assistive communication tools. Meanwhile, the 3D CNN, although precise in controlled environments, lacked the adaptability and responsiveness required for real-time translation. Overall, this comparative analysis underscores that while 3D CNNs offer higher classification accuracy, LSTM models strike a better balance between accuracy, speed, and computational efficiency, thus making them more appropriate for real-time sign language recognition systems deployed in practical settings. Moderate, using 3D convolution Preprocessing Hand tracking + landmark extraction Strong, using LSTM layers Cropping, resizing, normalization Computation Low (lightweight, real-time friendly) High (GPU required) Training Data Works with smaller datasets Requires larger labeled video datasets Real - Time Capability Excellent Limited, depends on hardware Model Size Small Large Generalization Good with regularization Needs augmentation and regularization Interpretability High (coordinate- based decisions) Low (complex visual features) V. Conclusion In conclusion, this research presents a robust and practical system for translating sign language gestures into text using deep learning techniques, with a particular focus on comparing the effectiveness of LSTM and 3D CNN architectures. Our project successfully implements a real-time sign language recognition system powered by an LSTM model, leveraging sequential hand landmark data extracted through Media Pipe. The LSTM model demonstrated strong performance in recognizing dynamic and temporally dependent hand gestures, making it highly suitable for real- time applications, especially on resource-limited devices due to its lightweight architecture and fast inference speed. In parallel, we evaluated a 3D CNN model that processes spatiotemporal features across consecutive frames, offering slightly higher classification accuracy in offline scenarios. However, the 3D CNN comes with a significantly higher computational cost and latency, which may hinder its use in live environments. The comparative analysis reveals that while 3D CNNs excel in capturing complex motion patterns across space and time, LSTMs offer a better balance between performance, efficiency, and practicality for deployment in real-world assistive technologies. The system’s GUI further enhances user interaction by displaying detected signs, maintaining a dynamic sentence output, and providing a reference module for individual ASL letters. Overall, this research not only delivers a functional and accessible prototype but also provides critical insights into model selection and optimization for gesture recognition tasks. It opens new avenues for enhancing communication accessibility for the deaf and hard-of-hearing community through AI-powered solutions and sets a foundation for future enhancements such as hybrid models, attention mechanisms, and multilingual sign language support. Deployment Suitability Mobile, Web Apps Desktop or Cloud interface IV. RESULTS AND DISCUSSIONS The comparative evaluation between the LSTM and 3D CNN models for sign language to text translation demonstrates key differences in performance, architecture suitability, and real- time applicability. The LSTM model, which exploits the temporal dependencies within sequential gesture data, achieved an accuracy of 86.7% on our test dataset. It proved particularly effective in recognizing dynamic gestures that require understanding the order and flow of hand movements, a common trait in sign languages. The LSTM model is lightweight, efficient, and capable of delivering smooth real- time predictions even on low-resource devices. Its use of sequential 3D landmark data (e.g., 30 frames × 63 features) allowed for effective modeling of motion, but it occasionally showed reduced precision in differentiating spatially similar static gestures. In contrast, the 3D CNN model, which processes spatiotemporal video data using volumetric convolution kernels (e.g., 3×3×3), achieved a higher overall accuracy of 92.4%. It excelled at capturing rich spatial features across frames, resulting in superior performance in classifying static or visually distinct signs. However, the model's complexity came at the cost of increased inference time (around 65 milliseconds) and a larger memory requirement of 87.6 MB, making it less suitable for real-time applications unless run on high-performance hardware with GPU acceleration. F.Y.B. Tech Students’ Applied Science & Engineering Project1 (ASEP1) Paper, SEM 2 A.Y. 2024-25 Vishwakarma Institute of Technology, Pune, INDIA. VI. ACKNOWLEDGMENT We would like to express our sincere gratitude to our faculty mentor, Prof. Madhumati Pol, for her invaluable guidance, insights, and encouragement throughout the development of this project. Her expertise and constructive feedback played a crucial role in shaping our research and ensuring the successful implementation of the website. We are also grateful to Vishwakarma Institute of Technology for providing the necessary resources, infrastructure, and a conducive learning environment.