Head movement poses a significant challenge in brain positron emission tomography (PET) imaging, re- sulting in image artifacts and tracer uptake quantification inaccuracies. Effective head motion estimation and correc- tion are crucial for precise quantitative image analysis and accurate diagnosis of neurological disorders. Hardware- based motion tracking (HMT) has limited applicability in real-world clinical practice. To overcome this limitation, we propose a deep-learning head motion correction ap- proach with cross-attention (DL-HMC++) to predict rigid head motion from one-second 3D PET raw data. DL-HMC++ is trained in a supervised manner by leveraging exist- ing dynamic PET scans with gold-standard motion mea- surements from external HMT. We evaluate DL-HMC++ on two PET scanners (HRRT and m CT) and four radiotracers (18F-FDG, 18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and generalization of the ap- proach in large cohort PET studies. Quantitative and qual- itative results demonstrate that DL-HMC++ consistently outperforms state-of-the-art data-driven motion estimation methods, producing motion-free images with clear delin- eation of brain structures and reduced motion artifacts that are indistinguishable from gold-standard HMT. Brain region of interest standard uptake value analysis exhibits average difference ratios between DL-HMC++ and gold- standard HMT to be 1.2±0.5% for HRRT and 0.5±0.2% for m CT. DL-HMC++ demonstrates the potential for data-driven PET head motion correction to remove the burden of HMT, making motion correction accessible to clinical popula- tions beyond research settings. The code is available at https://github.com/maxxxxxxcai/DL-HMC-TMI. ar Xiv:2510.12758v1 [cs.CV] 14 Oct 2025 Index Terms— Deep Learning, Supervised Learning, PET, Head Motion Correction, Image Registration, Cross- attention I. INTRODUCTION Positron emission tomography (PET) imaging has gained prominence in human brain studies due to the availability of a diverse range of radiotracers. These radiotracers enable inves- tigation of various neurotransmitters and receptor dynamics in different brain targets , as well as studies of physiological or pathological processes . PET is commonly employed for diagnosis and monitoring of neurodegenerative diseases, including Alzheimer’s disease, Parkinson’s disease, epilepsy, and certain brain tumors , . However, the presence of patient movement during PET brain scanning poses a signif- icant obstacle to high-quality PET image reconstruction and subsequent quantitative analysis . Even minor instances of head motion can substantially impact brain PET quantification, resulting in diminished image clarity, reduced concentrations in regions with high tracer uptake, and mis-estimation in tracer kinetic modeling . This problem is further exacerbated by the long duration of PET studies, where patients can involuntarily move . Even with physical head restraints, typical translations in the range of 5 to 20 mm and rotations of 1 to 4°are observed . Therefore, accurate monitoring and correction of head motion are critical for brain PET studies. PET head motion estimation involves tracking patient move- ment during image acquisition, while motion correction (MC) refers to the process of compensating for the effects of head movement . Generally, patient movements in brain imaging are assumed to be of a rigid nature, composed of translation and rotation in three dimensions. The initial process to correct head motion involves motion estimation. Once the motion information has been estimated, the motion-corrected PET image can be reconstructed using standard techniques such as frame-based or event-by-event (EBE) MC . Therefore, accurate motion estimation is crucial for realizing high-quality PET imaging. © 2025 IEEE. This accepted manuscript has been accepted for publication in IEEE Transactions on Medical Imaging. DOI: 10.1109/TMI.2025.3620714. Personal use is permitted, but republica- tion/redistribution requires IEEE permission. This work was supported by the National Institutes of Health (NIH) R21 EB028954. (Zhuotong Cai and Tianyi Zeng contributed equally to this work.) (Corresponding author: Tianyi Zeng; John Onofrey.) Zhuotong Cai is with the National Key Laboratory of Human- Machine Hybrid Augmented Intelligence, Xi’an Jiaotong University, Xi’an, Shannxi, China and Department of Radiology and Biomedi- cal Imaging, Yale University, New Haven, CT 06519 USA. (email: cai199624@stu.xjtu.edu.cn; zhuotong.cai@yale.edu). Tianyi Zeng and Kathryn Fontaine are with the Department of Radi- ology and Biomedical Imaging, Yale University, New Haven, CT 06519 USA. (e-mail: tianyi.zeng@yale.edu; kathryn.fontaine@yale.edu). Jiazhen Zhang and El ́eonore V. Lieffrig are with the Department of Biomedical Engineering, Yale University, New Haven, CT 06519 USA. (e-mail: jiazhen.zhang@yale.edu; eleonore.lieffrig@yale.edu). Chenyu You is with the Department of Electrical and Computer Engineering, Yale University, New Haven, CT 06511 USA. (e-mail: chenyu.you@yale.edu). James S. Duncan is with the Department of Radiology and Biomedical Imaging, Department of Biomedical Engineering and Department of Electrical and Computer Engineering, Yale University, New Haven, CT 06519 USA. (e-mail: james.duncan@yale.edu). Jingmin Xin is with the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi’an Jiaotong University, Xi’an, Shan- nxi, 710049 China. (e-mail: jxin@mail.xjtu.edu.cn). Enette Mae Revilla and Yihuan Lu are with United Imaging Health- care, Shanghai, China. (e-mail: enette.revilla@united-imaging.com; yihuan.lu@united-imaging.com). John A. Onofrey is with the Department of Radiology and Biomed- ical Imaging, Department of Biomedical Engineering and Department of Urology, Yale University, New Haven, CT 06519 USA. (e-mail: john.onofrey@yale.edu). 2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 Physical restraint during PET scanning can substantially reduce head motion effects. However, such methods cannot eliminate movement entirely, and this restrictive approach may be uncomfortable, especially over long scan durations, which reduces their acceptability for real-world use . Currently, head motion estimation methods are primarily cat- egorized into the following types: (i) hardware-based mo- tion tracking (HMT), and (ii) data-driven approaches. For HMT, high-frequency head motion information is provided by external devices. Marker-based HMT, such as Polaris Vicra (NDI, Canada), tracks light-reflecting markers on the patient’s head . Despite its potential benefits, Vicra is not commonly employed in clinical practice because it necessitates the attach- ment of the marker to the patient. Any inadvertent slippage or wobbling of the Vicra tool can introduce inaccuracies into the motion tracking process, thereby compromising the integrity of the data collected . Markerless HMT has also been developed for PET head motion estimation. Iwao et al. applied a time-of-flight (TOF) range sensor to achieve markerless head motion track- ing in a helmet PET system. Slipsager et al. and Zeng et al. applied camera systems in brain PET scans to achieve accurate high-frequency motion estimation. However, these systems can be challenged by facial expressions and other non- rigid motions . In general, HMT methods mainly rely on extra hardware support and setup, which limits their practical application in real-world clinical scenarios. On the other hand, data-driven methods estimate head mo- tion from reconstructions or PET raw data. Spangler-Bickell et al. utilized ultra-fast reconstruction methods to achieve motion estimation from short reconstruction frames in high- sensitivity and temporal resolution PET systems. Revilla et al. developed a data-driven head motion detection method based on the centroid of distribution (COD) of 3D PET cloud images (PCIs). These methods utilized intensity-based image registration methods to align different frames, but these methods are sensitive to tracer kinetics and require manual parameter tuning. In contrast, deep learning (DL) methods, leveraging neural networks to construct a hierarchical repre- sentation of data through multiple layers of hidden units , enable registration approaches to extract pertinent features directly from the data . Salehi et al. proposed a DL model for medical image rigid registration and achieved real- time pose estimation of MRI. Unsupervised DL methods were also developed for non-rigid medical image registration . Inspired by DL-based registration methods, Zeng et al. proposed a supervised DL head motion correction (DL-HMC) framework to predict rigid head motion information from PCIs using Vicra HMT as gold-standard motion information. However, due to the noisy PCIs and limited generalization across data distributions, the effectiveness of these methods diminishes when applied to testing subjects that differ from the training dataset, especially when addressing subjects with significant movements. Subsequent DL methods have explored various strategies for PET head motion estimation. Sundar et al. utilized conditional generative adversarial networks to synthesize pseudo high-count images from low-count PET brain images and applied frame-based registration for MC , which ameliorated motion blurring to determine accurate motion information in an 18F-FDG study. However, intra- frame motion can not be solved by frame-based MC, and the MRI navigators used in this study are challenging to implement with brain-dedicated PET scanners. Lieffrig et al. developed a multi-task architecture for head MC, in which the rigid motion and motion-free PCI were predicted by the network. The multi-task network enabled the model to learn the embedding of PCI representation, however, this network was sensitive to noise that introduced bias in testing subjects. Reimers et al. utilized a DL method to transform low- count images to high-count images, thereby predicting motion from high-quality subframes. However, training the network requires motion-free PET data, which is not available in this case. To address the limitations of the original DL-HMC ap- proach, this study introduces an enhanced model, DL-HMC++, that incorporates a cross-attention mechanism, aiming to en- hance motion estimation and generalization performance . Notably, attention mechanisms have demonstrated effective MC performance in cardiac image analysis applications , . Our cross-attention mechanism takes a pair of features as input and computes their correlations to establish spatial correspondence between reference and moving PCIs. This explicitly enables the model to concentrate on the head region, which is the most relevant anatomy for motion estimation in brain PET studies. This manuscript extends our previous work by (i) including a rigorous validation of DL-HMC++ using a large cohort of human PET studies, encompassing over 280 brain scans with 4 different tracers, (ii) providing extensive model analysis to assess generalization using two different PET scanners with distinct TOF characteristics and different tracers, including cross-tracer generalization experi- ments, (iii) ablation studies to justify model design choices, (iv) quantitative evaluation of MC accuracy, and (v) com- prehensive validation studies against several state-of-the-art (SOTA) benchmark motion estimation methods. Quantitative and qualitative evaluations demonstrate the robustness of DL- HMC++ across extensive experiments and highlight its ability to correct head motion in PET studies using only raw image data without the need for either reconstruction techniques or HMT. II. METHODS A. Data-Driven Brain PET Motion Estimation Framework Our deep learning approach to brain PET head motion correction estimates rigid motion at one-second time resolu- tion. This data-driven motion estimation model utilizes one- second 3D PET cloud image (PCI) representations as input. The reference Iref PCI and moving Imov PCI are created by back-projecting the PET listmode data from one-second time windows at times tref and tmov, respectively, along the line-of- response (LOR) with normalization for scanner sensitivity. For model training and evaluation, each one-second PCI has cor- responding Vicra HMT information (rigid transformation ma- trix) as the gold-standard motion. We train the model to esti- mate the rigid motion transformation θ = [tx, ty, tz, rx, ry, rz] CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 3 between Iref and Imov where θ includes three translation (td) and three rotation (rd) parameters for each axis d = {x, y, z}. attention map Amr, the attention features are updated for both the reference and moving features as follows: Aref = Amr · Vref, Amov = AT mr · Vmov. B. DL-HMC++ Overview DL-HMC++ utilizes a supervised deep learning framework to predict rigid head motion for PET imaging. This network consists of three main components (Fig. 1): (i) the feature extractor; (ii) the cross-attention module; and, (iii) the regres- sion layers. The feature extractor employs a shared-weight convolutional encoder to capture regional features from both the reference Iref and moving Imov PCIs. In contrast to our previous DL-HMC approach that used a Res Net encoder, here, we adopt a U-Net encoder with fewer parameters to extract features. Specifically, this encoder utilizes three convolutional layers with a 53 kernel size followed by a convolution with a 13 kernel, with the number of feature channels set to 32, 64, and 128, respectively. We introduce a novel cross-attention mechanism to capture local features and global correspondence between the reference and moving PCIs, which will be elaborated in the following section. To enhance the representation of aggregated information follow- ing the cross-attention phase, we integrate a Deep Normal- ization and Fusion (DNF) module both prior to and after the concatenation process . The DNF module includes a series of convolutional layers, batch normalization and Re LU activation to refine the feature integration process. Finally, a fully connected multi-layer perceptron (MLP) block takes the output of the final DNF block to infer the six rigid transformation motion parameters ˆθ. To enhance the model’s ability to identify and prioritize the most critical feature representation for the motion analysis between the moving and reference PCIs, we incorporate a self- gating mechanism. This approach assigns variable weights to the input data, enabling the model to discern and selectively integrate relevant information from both the moving and refer- ence PCIs. The gating mechanism is operated by dynamically adjusting the contribution of each input, ensuring that the most informative parts have a greater influence on the outcome of the motion estimation, which is formulated as follows: Gref, Gmov = σ(G(Aref)), σ(G(Amov)) ∈HW D, where σ is the logistic sigmoid activation function and G is the 1×1×1 convolution layer. The gate module effectively determines the extent to which information from the PCI could be retained and automatically learned during the training. By applying this gate across the features of both the moving and reference PCIs, the model generates a weighted combination that emphasizes the most relevant features for motion analysis. This results in an enriched feature representation that captures the essential details from both images, facilitating a more precise and informed estimation of motion. The final attention feature representations for both the moving and reference features are derived as follows: Fref = Gref Aref + Vref, Fmov = Gmov Amov + Vmov. C. DL-HMC++ Cross-Attention III. RESULTS Because of the ultra-short time duration (one-second), low system sensitivity, and lack of essential physical correction, low-frequency bias within the PCI significantly affects MC performance, making it challenging for the model to track head motion. To mitigate the impact of noise and to enhance motion estimation performance, we introduce the attention mechanism in our model to emphasize the head region. This module estab- lishes spatial correspondences between features derived from the reference image and those from the moving image. It takes two inputs fref ∈RC×H×W ×D and fmov ∈RC×H×W ×D, which represent the feature maps of the reference and moving images, respectively, where H, W and D denote the feature map dimensions and C denotes the number of feature channel. Initially, we partition fref into reference key Kref and value Vref and, likewise, fmov is divided into moving query Kmov and value Vmov: We validate DL-HMC++’s effectiveness for head motion estimation using a diverse set of brain PET studies from two different scanners. We compare performance with multiple motion estimation baselines and provide ablation studies to justify model design choices. Finally, we demonstrate accurate motion estimation and correction through rigorous quantitative and qualitative evaluations. A. Experimental Setup 1) Data: We retrospectively identified a cohort of existing brain PET studies from the Yale PET Center. The cohort con- tains a diverse set of PET data from four different radiotracers acquired on two different scanners: (i) 120 18F-FDG and 120 11C-UCB-J scans acquired on a brain-dedicated High Resolution Research Tomograph (HRRT) scanner (Siemens Healthineers, Germany) without time-of-flight (TOF); and (ii) 24 18F-FPEB and 20 11C-LSN3172176 scans acquired on a conventional m CT scanner (Siemens Health- ineers, Germany) with TOF. The datasets contain a diverse mix of subjects and clinical conditions that include healthy controls, neurological disorders such as Alzheimer’s Disease (AD), mild cognitive impairment (MCI), epilepsy, and other diagnoses. We divide each dataset into Training, Validation and Testing sets using an 8:1:1 ratio (Tab. I). All scans include Kref = Wafref, Vref = Wbfref, Kmov = Wafmov, Vmov = Wbfmov, where Wa, Wb are the 1×1×1 convolution layers. We reshape Kmov and Kref to the dimension of C ×(HWD) and calculate the attention matrix using the following equation: Amr = Softmax(KT mov Kref) ∈R(HW D)×(HW D), where Amr represents the similarity matrix, correlating each row of KT mov with each column of Kref. Upon computing the 4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 DNF Predicted I$%& Conv motion Encoder Cross-attention DNF BN tx ty tz rx Re LU Regression Conv Reference PET Cloud Image Re LU Flatten Share Weight Concatenation Linear Conv Linear Re LU Linear DNF Conv I!"# ry Conv BN Encoder rz BN MSE Vicra Rigid Motion Re LU Moving PET Cloud Image Cross-attention Wb: 1×1×1 Wa: 1×1×1 Wb: 1×1×1 V$%& Reference Branch f$%& G: 1×1×1 G: 1×1×1 F$%& A!"# Sigmoid Sigmoid G!"# K$%& attention reference Embedded reference PCI feature softmax S Self-gate Wa: 1×1×1 A!$ Moving Branch F!"# K!"# f!"# A$%& G$%& attention moving feature V!"# Embedded moving PCI Fig. 1. DL-HMC++ network architecture. (Top) A shared encoder extracts imaging features from a pair of moving and reference PET cloud images. Then, the extracted features are fed into the cross-attention module to learn the correlation of anatomical features. Deep Normalization and Fusion (DNF) blocks refine the attention features both before and after concatenation. Finally, concatenated attention features are fed into a multi-layer perceptron Regression block to predict motion. (Bottom) Details of the cross-attention module. TABLE I PET STUDY COHORT. THE HRRT AND MCT SCANNER COHORTS ARE DESCRIBED IN TERMS OF SEX, HEALTH STATUS, INJECTED ACTIVITY, AND MOTION INFORMATION. REPORTED VALUES ARE MEAN±SD ACROSS SUBJECTS. IN COHORTS WITH A NUMBER OF SUBJECTS GREATER THAN TWENTY, MOTION WAS COMPUTED ON 20 RANDOMLY SELECTED SUBJECTS TO REPRESENT MOTION ACROSS THE WHOLE DATASET. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Train Test Train Test Train Test Train Test N Subj. (M/F) 100 (56/44) 20 (13/7) 100 (53/45) 20 (16/4) 20 (8/12) 4 (1/3) 16 (7/9) 4 (4/0) Healthy Control 42 7 37 8 20 4 8 2 Alzheimer’s Disease 24 3 20 2 0 0 3 19 1 9 0 0 0 5 8 2 3 2 0 0 0 7 7 31 8 0 0 0 0 Injected activity (m Ci) 4.83±0.28 4.93±0.15 14.99±5.15 14.91±4.84 3.75±1.19 4.47±0.16 14.27±4.43 15.77±6.32 Motion (mm) 7.69±6.80 11.20±3.53 8.56±6.87 10.79±8.29 11.01±11.64 3.90±1.48 8.96±7.54 9.46±3.71 Vicra HMT information used as gold-standard motion estima- tion, T1-weighted magnetic resonance imaging (MRI), PET- space to MRI-space transformation matrices, and Free Surfer anatomical MRI segmentations . All PET imaging data is 30 minutes acquired from 60-minutes post-injection. Summary estimates of head motion magnitude were quantified over the entire scan duration using the method described by Jin et al. in . All subjects were enrolled in studies approved by the Yale Institutional Review Board and Radiation Safety Committee with written informed consent. 2) Evaluation Metrics: We evaluate head motion estimation performance using quantitative and qualitative assessment. a) Quantitative Assessment of Motion Estimation: To quan- titatively evaluate the performance of motion estimation, we calculate the Root Mean Squared Error (RMSE) between the estimated motion parameters (ˆθ) and the Vicra gold-standard (θ). The RMSE was computed for each individual motion component (translation and rotation) separately across the full scan duration. To robustly summarize motion estimation per- formance, we calculate the mean value and standard deviation CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 5 (SD) of the RMSE error across all testing subjects. We assess the statistical significance of DL-HMC++ compared to other MC methods on the HRRT dataset using a two-tailed Wilcoxon signed-rank test to evaluate if the DL-HMC++ RMSE result is smaller than that of the other methods. The Wilcoxon signed- rank test was selectively applied to the HRRT’s 18F-FDG and 11C-UCB-J datasets, but did not apply to the m CT datasets due to the test set sample size (n=4 subjects) being below the minimum requirement (n≥6). b) Qualitative and Quantitative Assessment of Reconstructed PET Images: For HRRT 18F-FDG and m CT 18F-FPEB stud- ies, we qualitatively compare MOLAR reconstructed images by visual inspection and quantitatively assess differences by computing normalized error maps Epred. Here, Epred = (Rpred −RVicra)/ max (|Rpred −RVicra|) (scale to the range [−1, 1]), where Rpred and RVicra are the reconstructed im- ages from motion-correction and Vicra HMT, respectively. To evaluate the final motion-corrected PET reconstruction images quantitatively, we perform brain ROI analyses using the Free Surfer segmented ROI masks to quantify mean standard uptake value (SUV) within each ROI. We aggre- gate the original 109 Free Surfer ROIs into 14 grey matter (GM) ROIs: Amygdala; Caudate; Cerebellum Cortex; Frontal; Hippocampus; Insula; Occipital; Pallidum; Parietal; Putamen; Temporal; Thalamus; and two white matter (WM) ROIs, the Cerebellum and Cerebral WM. We perform a bias-variance analysis between the mean SUV within each ROI and the SUV derived using the Vicra gold-standard by computing the absolute difference ratio. To evaluate performance at anatomically meaningful lo- cations, we calculate the mean distance error (MDE) of anatomical brain ROIs . Using the Free Surfer segmented ROI masks, we calculate the center-of-mass (COM) for each ROI on the Vicra MC result (COMVicra). Then, the same ROI masking is applied to the MOLAR reconstruction images with different MC methods, and the estimated COM (COMest) of each method is calculated. The MDE is defined as the mean of the Euclidean distance between COMVicra and COMest across all ROIs. A larger MDE indicates worse motion estimation. dure that minimizes the sum-of-squared differences; (ii) Sim- ple Elastix (SIM), a widely utilized medical image reg- istration tool that employs mutual information as a similarity metric to rigidly register the PCIs; (iii) Imregtform (IMR), a medical image registration method that uses intensity-based rigid registration algorithm with MSE loss, which was used in prior data-driven PET head MC studies ; (iv) DL- HMC , our prior supervised deep learning approach for head MC that includes a time-conditioning module and ex- cludes attention; (v) DL-HMC without time-conditioning (DL- HMC w/o TC), which removes the time conditional module from the original DL-HMC; and (vi) Dual-Channel Squeeze- Fusion-Excitation (Du SFE) , a deep learning registration approach designed to extract and fuse the input information for cross-modality rigid registration. To further enhance the registration quality of the intensity-based methods, following the same workflow in , high-resolution one-second fast reconstruction images (FRIs) were generated using CPU- parallel reconstruction platforms for the m CT dataset . We evaluated BIS and IMR using FRIs as inputs during the m CT experiments. No motion correction (NMC) results were also compared for reference. 5) Implementation Details: a) Data Processing: To create the DL-HMC++ input, we pre-process the HRRT PCI data volumes by downsam- pling from 256×256×207 voxels (1.22×1.22×1.23 mm3) to 32×32×32 voxels (9.76×9.76×7.96 mm3) using area interpo- lation. Similar pre-processing is applied to m CT PCI data from 150×150×109 voxels (2.04×2.04×2.03 mm3 voxel spacing) to 32×32×32 voxels (9.56×9.56×6.91 mm3 voxel spacing). b) Network Training: To efficiently train the network, we randomly sub-sample 360 out of 1,800 time points for each study in the training set. During each training epoch, we randomly pair two PCIs as reference Iref and moving Imov image inputs, such that tmov > tref, and calculate their relative Vicra motion on the fly. We train the network using a mini- batch size of 12 and minimize the mean squared error (MSE) between the predicted motion estimate ˆθ and Vicra θ using Adam optimization with initial learning rate 5e-4, γ=0.98, and exponential decay with step size 200 for training. c) Network Inference: For inference on testing subjects independent of the training data, we utilize a single reference PCI Iref at the first time point and register all following PCIs at the remaining time points to estimate the rigid transformation to the reference space Iref. d) Event-by-Event (EBE) Motion Compensated Reconstruction: Once the rigid motion transformation parameters have been estimated by DL-HMC++, we reconstruct the PET image using the EBE motion compensation OSEM list-mode algo- rithm for resolution-recovery reconstruction (MOLAR) . MOLAR reassigns the endpoints of each LOR according to the motion estimation result to reconstruct the motion-corrected PET image. For HRRT studies, OSEM reconstruction (2 iterations × 30 subsets) with spatially invariant point-spread- function (PSF) of 2.5-mm full-width-half-maximum (FWHM) is applied with reconstruction voxel size 1.22×1.22×1.23 mm3. For m CT studies, OSEM reconstruction (3 iterations × 21 subsets) with spatially invariant PSF of 4.0-mm FWHM is 3) Cross-tracer Generalization Evaluation: To validate the model’s cross-tracer generalization capability, we conduct a comprehensive evaluation by directly applying the model weights trained on 11C datasets to perform inference on 18F datasets without any fine-tuning or parameter adjustment. Specifically, the model weights obtained from HRRT 11C- UCB-J training are applied to 18F-FDG data, while the weights from m CT 11C-LSN3172176 training are evaluated on 18F- FPEB data. Quantitative assessment of motion estimation is conducted by comparing the model’s performance on these unseen tracers with the gold-standard Vicra, evaluating RMSE for both translation and rotation parameters (Sec. III-A.2.a). This evaluation provides critical insights into the model’s ro- bustness and generalizability across diverse tracer applications. 4) Baseline Motion Estimation Methods: We comprehen- sively compared our approach for head motion estima- tion against SOTA benchmark methods, including intensity- based registration and deep learning methods: (i) Bio Image Suite (BIS), an intensity-based rigid registration proce- 6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 applied with reconstruction voxel size 2.04×2.04×2.00 mm3. C. m CT Results 1) 18F-FPEB: DL-HMC++ remains competitive on the m CT 18F-FPEB data, reaching RMSE of 0.54 mm in transla- tion and 0.40° in rotation (Table II) on the testing dataset. We observe a consistent trend between intensity-based registration methods and DL methods from the HRRT to m CT, where DL methods outperform SOTA image-intensity registration methods (BIS, IMR) that even utilize FRIs as input , . Similar to the HRRT results, DL-HMC++’s attention mechanism helps capture the motion with better estimation performance. It is also noticeable that DL-HMC++ ranked the best in both translation and rotation error, outperforming the original DL-HMC by 42% in translation. Figure 4 shows the motion prediction results for the 18F- FPEB dataset, comparing DL-HMC++ with the baseline DL- HMC and the Vicra gold standard. While the overall perfor- mance on m CT data is less accurate than on HRRT data likely due to relatively fewer training data samples, DL-HMC++ demonstrates notable improvements over DL-HMC. A key example is in 18F-FPEB Subject #1 (translation Z), where DL-HMC fails to track the motion (red bounding box), while DL-HMC++ successfully detects the substantial movements. In 18F-FPEB Subject #2, both DL-HMC and DL-HMC++ underestimate rotations on the x-axis and z-axis, however, this error is limited to ∼1.5°. B. HRRT Results 1) 18F-FDG: DL-HMC++ demonstrates the best quantita- tive motion estimation performance compared to all other benchmark methods, with translation and rotation RMSE of 1.27 mm and 1.16°, respectively (Table II). The Wilcoxon signed-rank test reveals that DL-HMC++ achieves statistically significant improvements (p<0.05) in both translation and rotation errors compared to all benchmark methods. Overall, DL methods outperform the intensity-based registration ap- proaches with more accurate and effective motion estimation results. DL-HMC++ significantly outperformed original DL- HMC, demonstrating a 49% and 27% improvement in trans- lation and rotation, respectively. Figure 2 visualizes DL-HMC++ motion estimation results with respect to the original DL-HMC and the Vicra gold- standard, which demonstrates that the proposed method can effectively track head motion. In FDG Subject #1, both mod- els demonstrate excellent alignment with actual Vicra head motion patterns. For Subject #2, a poor performance occurs in translation X (red bounding box), where DL-HMC++ shows a misalignment with Vicra, however, DL-HMC exhibits larger errors. This mismatch may be attributed to the substantial distance between the moving frame and the reference frame. Moreover, our model performs well during other periods, demonstrating its capability to estimate movements with rela- tively large translations over 15 mm and 9-degree rotations. In addition, DL-HMC++’s proposed cross-attention module enhances the model’s ability to correct motion by concen- trating on the head region during the motion tracking, which we confirm using Grad-CAM to visualize saliency maps and compare to DL-HMC (Fig. 3). DL-HMC’s saliency maps highlight areas outside the head, suggesting this model failed to focus on the relevant anatomical information in the PCI. 2) 11C-LSN3172176: Building upon the promising results demonstrated with 18F in m CT, our proposed DL-HMC++ framework maintains superior performance in both transla- tion and rotation estimation for the more challenging 11C- LSN3172176. The quantitative results in Table II reveal that DL-HMC++ outperforms all benchmark methods, demonstrat- ing an 18% improvement in translation and 16% improvement in rotation compared to Du SFE. The 11C subject #1 visualization in Figure 4 further presents a noteworthy observation. While DL-HMC fails to capture motion information, as demonstrated by its flattened prediction curve, our proposed DL-HMC++ algorithm maintains robust performance. Although the red bounding box indicates an intensity mismatch with Vicra due to continuous movements with relatively large and rapid amplitudes, DL-HMC++ suc- cessfully detects the overall movement trends up to 10 mm in translation X and 4° in rotation Z. In summary, the significant improvements in motion esti- mation achieved by DL-HMC++ over other methods across diverse scenarios and challenging conditions underscore the enhanced robustness of our proposed method. 2) 11C-UCB-J: The performance evaluation on 11C data from HRRT demonstrates consistent superiority of DL- HMC++, similar to its performance on 18F data (Tab. II). Quantitative results indicate that DL-HMC++ achieves the best performance across all evaluation metrics, with translation and rotation RMSE values of 1.26 mm and 1.22°, respectively. Statistical evaluation confirms that DL-HMC++ achieves sig- nificantly superior performance over nearly all benchmark methods (p<0.05). Compared to the original DL-HMC, DL- HMC++ demonstrates a 39% improvement in translation and a 10% improvement in rotation. Visualizing the motion prediction results for one 11C subject in HRRT (Fig. 2, third column), DL-HMC++ demonstrates promising capability in capturing large motion patterns, even under challenging conditions (e.g., 14 mm in z-axis translation and 7° in x-axis rotation). Compared to the original DL-HMC, DL-HMC++ achieves superior motion detection sensitivity. For example, as highlighted by the red bounding box, DL- HMC++ benefits from the enhanced attention module to precisely predict both the motion trend and magnitude, even for a 10 mm movement. D. DL-HMC++ Ablation Studies We conducted a series of ablation studies on the HRRT 18F-FDG dataset to evaluate individual components and select parameters that lead to the best motion estimation performance (Table III). 1) Network Architecture: To demonstrate the effectiveness of the DL-HMC++ architecture, we compare (i) the proposed model architecture with self-gating and DNF; (ii) the model without self-gating; (iii) the model without DNF; and (iv) the model without both self-gating and DNF. DL-HMC++ without CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 7 11C-UCB-J Subject #2 Subject #1 18F-FDG Subject #1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation (mm) Rotation (deg) Times (Second) Times (Second) Times (Second) DL-HMC++ DL-HMC Vicra Fig. 2. HRRT motion prediction results with 18F-FDG and 11C-UCB-J tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters (from top to bottom: translation in x, y, z directions and rotation about the x, y, z axes) from gold- standard Vicra (orange), DL-HMC (yellow) and our proposed method (blue) on HRRT. Red boxes indicate time intervals of interest for DL-HMC++ performance. TABLE II QUANTITATIVE MOTION ESTIMATION RESULTS. MOTION PREDICTION RMSE ERROR OF TRANSLATION (TRANS.) (MM) AND ROTATION (ROT.) (DEGREES) COMPONENTS COMPARED TO VICRA GOLD-STANDARD ON TWO PET SCANNERS (HRRT AND MCT) USING FOUR RADIOTRACERS (18F-FDG, 18F-FPEB, 11C-UCB-J, AND 11C-LSN3172176). REPORTED VALUES ARE MEAN±SD. HRRT m CT 18F-FDG 11C-UCB-J 18F-FPEB 11C-LSN3172176 Method Trans. (mm) Rot. (deg) Trans. (mm) Rot. (deg) Trans. (mm) Rot. (deg) Trans. (mm) Rot. (deg) NMC 6.29±5.79∗ 3.12±1.42∗ 6.86±19.58∗ 3.27±6.14∗ 2.42±1.43 1.36±0.48 4.63±7.76 2.10±1.36 BIS 4.26±5.31∗ 2.06±3.01∗ 3.18±3.56∗ 1.63±1.54∗ 1.32±0.06 0.53±0.05 1.40±0.20 0.66±0.06 SIM 3.15±4.87∗ 1.94±2.70∗ 3.04±2.53∗ 1.58±1.32∗ 1.57±0.10 1.24±0.02 3.06±2.05 2.60±3.03 IMR 2.84±3.83∗ 2.25±2.85∗ 3.52±3.97∗ 1.77±1.50∗ 1.38±0.28 0.55±0.05 2.32±2.26 0.88±0.07 DL-HMC 2.49±2.43∗ 1.59±2.32∗ 2.07±1.87∗ 1.35±1.09∗ 0.93±0.20 0.40±0.03 1.46±0.35 0.71±0.09 -w/o TC 1.76±1.19∗ 1.33±1.63∗ 1.54±0.62∗ 1.34±1.13∗ 0.80±0.01 0.57±0.01 1.19±0.11 0.61±0.02 Du SFE 1.56±0.66∗ 1.37±1.73∗ 1.36±0.46 1.36±0.85∗ 0.60±0.03 0.41±0.02 1.21±0.12 0.69±0.10 DL-HMC++ 1.27±0.46 1.16±1.20 1.26±0.44 1.22±0.98 0.54±0.00 0.40±0.00 0.99±0.02 0.58±0.03 Note: ∗indicates p < 0.05. gating and DNF demonstrate the worse performance. Re- moving the self-gating mechanism from the attention module degrades MC performance 0.25 mm in translation and 0.21° in rotation, which demonstrates that our self-gating mechanism selectively distills the most relevant feature representation for motion tracking. Moreover, our results show that removing the DNF results in a performance drop of 22% in translation and 13% in rotation, which indicates that DNF plays a significant role in effectively aggregating information between the moving and reference branches to enhance the model’s performance. 2) Attention Type: We experiment with different atten- tion types: (i) cross-attention, and (ii) self-attention. Com- pared with the self-attention mechanism, which computes feature similarities within each input image individually, cross- attention concentrates feature learning on the head areas by 8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 Reconstruction TABLE IV ENCODER ABLATION STUDY. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE ON THE HRRT 18F-FDG DATASETS. THE ENCODER PARAMETERS, FLOPS, AND INFERENCE TIME ARE ALSO LISTED FOR COMPARISON. REPORTED VALUES ARE MEAN±SD WHERE APPROPRIATE. DL-HMC PCI DL-HMC++ Encoder Trans. (mm) Rot. (deg) Parameters (M) FLOPs (×109) Inference Time (ms) Res Net 1.62±0.83 1.37±1.88 14.61 4.6 5.8 U-Net 1.27±0.46 1.16±1.20 0.86 4.0 3.3 tively, compared to the results when trained using 20 subjects. These results highlight the need for large training cohorts of PET studies when developing DL-based brain motion correction methods. (a) 360s (b) 720s (c) 1080s (d) 1440s (e) 1800s Fig. 3. Grad-CAM saliency map visualization. Sagittal view from five different time frames of the HRRT testing set during 30 min (1,800 s) PET acquisition. Our proposed DL-HMC++ method more accurately localizes the head anatomy compared to DL-HMC without attention. 4) PET Cloud Image (PCI) Size: We evaluate the perfor- mance of our model under various 3D PCI sizes: 323, 643, and 963. As PCI size increases, there is a slight degradation in performance. Despite having lower spatial resolution, small PCI dimensions benefit from smooth images due to increased downsampling compared to larger PCIs (see Fig. 5). In con- trast, the larger but noisier PCIs impair network training and fail to optimize motion correction performance. TABLE III ABLATION STUDIES. MOTION PREDICTION OF TRANSLATION AND ROTATION TRANSFORMATION RMSE COMPARED TO VICRA GOLD-STANDARD MOTION TRACKING ON THE HRRT 18F-FDG DATASETS FOR NETWORK ARCHITECTURE, ATTENTION TYPE, CLOUD SIZE AND SUBJECT NUMBER. REPORTED VALUES ARE MEAN±SD. 5) Network Encoder: We further evaluate the choice of image encoder by comparing DL-HMC++’s U-Net encoder to DL-HMC’s Res Net encoder, removing the fully connected layer for a fair comparison. As shown in Table IV, we adopt the lightweight U-Net encoder instead of the Res Net encoder used in DL-HMC. This change significantly reduces the number of encoder parameters from 14.61M to 0.86M, which enhances DL-HMC++ in terms of both training and inference efficiency. Ablation Part Trans. (mm) Rot. (deg) Proposed 1.27±0.46 1.16±1.20 w/o gate 1.52±0.52 1.37±1.98 w/o DNF 1.62±1.03 1.33±1.77 backbone 2.31±1.85 1.44±1.78 Network Arch. Attention Type self attention 1.61±0.64 1.33±1.75 Proposed 1.27±0.46 1.16±1.20 20 2.10±2.27 1.88±2.71 40 1.69±0.79 1.44±1.56 60 1.56±0.90 1.38±1.73 80 1.38±0.50 1.24±1.20 100 1.27±0.46 1.16±1.20 Subject Number E. Motion-Corrected PET Image Reconstruction 1) Image Reconstruction Result: Figures 6 and 7 show MOLAR reconstruction images and normalized error maps with respect to Vicra gold-standard. We randomly select one subject from the HRRT 18F-FDG testing set and one subject from the m CT 18F-FPEB testing set for visualization. We com- pare reconstruction using DL-HMC++ to NMC, SIM, Du SFE, and DL-HMC with the Vicra gold-standard. Qualitatively, reconstruction using DL-HMC++ demonstrates the sharpest anatomical structure delineation and least deviation (normal- ized error map) from the Vicra gold-standard. Additionally, we compute the Structural Similarity Index (SSIM) and Nor- malized Mean Squared Error (NMSE) for each individual view to quantitatively assess image quality and reconstruction accuracy. In the HRRT 18F-FDG study, DL-HMC++-based recon- struction results clearly show the gyrus and sulcus on the entire cortex compared to NMC. DL-HMC++ shows improved ROI anatomical structures such as the caudate and thalamus in the transverse view, as well as the parietal and frontal lobes in the coronal and sagittal views, respectively. In addition, DL- HMC++ exhibits the highest SSIM, the lowest NMSE, and 323 1.27±0.46 1.16±1.20 643 1.45±0.78 1.37±1.75 963 1.59±0.60 1.49±1.85 PET Cloud Size computing the similarity between both the moving and ref- erence clouds. Quantitative evaluations demonstrate that our approach using cross-attention consistently outperforms self- attention in both translation and rotation. These results demon- strate that our approach boosts the model’s MC performance by creating spatial correspondences between the moving and reference clouds. 3) Training Set Size: We evaluate the impact of varying the number of subjects used for model training by evaluating performance using 20, 40, 60, 80, and 100 subjects. As the number of subjects increases, we observe a corresponding enhancement in the performance of MC with a decrease in transformation error. DL-HMC++ achieves the best evaluation results on both translation and rotation using 100 subjects, demonstrating improvements of 39.5% and 38.3%, respec- CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 9 18F-FPEB 11C-LSN3172176 Subject #2 Subject #1 Subject #1 Translation Y Translation Z Rotation X Rotation Y Rotation Z Translation X Translation (mm) Rotation (deg) Times (Second) Times (Second) Times (Second) DL-HMC++ DL-HMC Vicra Fig. 4. m CT motion prediction results with 18F-FPEB and 11C-LSN3172176 tracers. Each column represents a unique subject from the test set. Rows show rigid transformation parameters (from top to bottom: translation in x, y, z directions and rotation about the x, y, z axes) from gold- standard Vicra (orange), DL-HMC (yellow) and our proposed method (blue) on m CT. Red boxes indicate time intervals of interest for DL-HMC++ performance. m CT 18F-FPEB studies. When evaluating anatomical brain ROI motion error, our results reveal a distinct advantage of DL methods over intensity-based methods with PCI input in terms of the MDE metric. In both studies, DL-HMC++ consistently demonstrates the smallest average MDE, underscoring the ro- bustness and effectiveness of our proposed method. Compared with Du SFE, DL-HMC++ not only achieves superior average MDE but also maintains lower standard deviation, indicating reduced variability of the proposed model. This reaffirms the superiority of DL-HMC++ in mitigating motion-related artifacts, rendering it a promising advancement in data-driven head motion estimation methods. the smallest deviations from Vicra results compared to other methods, as indicated by the error maps. In the m CT 18F-FPEB study, NMC and SIM produce higher visual errors than the DL methods. Notably, DLHMC++ achieves best quantification quality from SSIM and NMSE. The transverse view (Fig. 7) indicates that DL-HMC++ eliminates motion blurring for the caudate area, and the GM- WM interface can be delineated. 2) Brain ROI SUV Evaluation: We average ROI SUV evalu- ation results across all 20 testing subjects in the HRRT 18F- FDG study and 4 testing subjects in the m CT 18F-FPEB study and compared percentage differences to the Vicra gold- standard (Tab. V). Overall, DL-HMC++ outperforms all other methods, achieving the smallest mean SUV difference and the lowest standard deviation across both studies. Compared to DL-HMC, DL-HMC++ demonstrates superior performance, with a 1.5% improvement in mean SUV difference for 18F- FDG dataset and a 0.5% improvement in 18F-FPEB dataset. For 18F-FDG, the Wilcoxon signed-rank test indicates that the ROI SUV error of DL-HMC++ is significantly smaller than all other methods (p<0.05). For 18F-FPEB, DL-HMC++ and Vicra are nearly identical, with a 0.5% average difference. Notably, SIM performs worse than NMC, indicating that the intensity-based registration method with PCI input introduces false extra motion due to poor optimization. F. Cross-tracer Generalization Performance Table VII summarizes the motion estimation RMSE results for two cross-tracer tasks using DL-HMC++. When compared to direct training on 18F-FDG, the cross-tracer experiment yields comparable results, with 0.23 mm higher for translation and 0.22° higher for rotation. For 18F-FPEB, the cross-tracer results show 0.20 mm higher translation error and 0.15° higher rotation error than directly training results, but still outperform all intensity-based registration methods and the DL-HMC method despite training with limited training data and different tracer characteristics. 3) MDE Evaluation Result: Table VI presents the MDE metric result of all testing subjects in HRRT 18F-FDG and 10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 TABLE V ROI EVALUATION RESULT OF DIFFERENT METHODS ON HRRT AND MCT. THE ABSOLUTE DIFFERENCE RATIO (ADR) SERVES AS THE METRIC TO QUANTIFY THE DISCREPANCY BETWEEN DIFFERENT METHODS AND VICRA GOLD-STANDARD. Dataset HRRT 18F-FDG m CT 18F-FPEB ROI\ADR% NMC SIM DL-HMC Du SFE DL-HMC++ NMC SIM DL-HMC Du SFE DL-HMC++ Amygdala 6.8 6.8 2.1 1.9 1.7 1.2 3.5 1.0 0.9 0.9 Caudate 13.8 11.8 5.6 2.4 2.0 4.6 10.2 2.2 0.6 0.6 Cerebellum Cortex 13.8 11.8 5.6 2.4 2.0 0.4 0.7 0.3 0.2 0.2 Cerebellum WM 5.6 5.5 1.3 0.7 0.6 0.9 0.5 0.6 0.4 0.4 Cerebral WM 4.3 3.5 2.0 1.1 1.1 1.6 3.0 1.1 0.6 0.4 Frontal 10.5 8.0 5.0 2.3 1.9 2.9 5.2 1.5 0.6 0.7 Hippocampus 7.9 6.6 2.0 0.9 0.9 2.6 3.3 1.9 1.2 0.7 Insula 4.8 3.7 1.5 0.7 0.7 1.8 4.1 0.5 0.5 0.3 Occipital 8.6 8.6 3.2 1.7 1.5 0.9 2.0 0.4 0.6 0.6 Pallidum 4.5 3.4 1.4 1.0 1.0 0.9 3.0 0.8 0.7 0.4 Parietal 10.7 9.3 4.1 2.1 1.7 1.9 3.4 0.9 0.6 0.5 Putamen 8.7 6.9 3.3 1.0 1.1 1.7 2.7 1.1 0.4 0.5 Temporal 8.0 7.1 3.0 1.2 1.1 1.3 3.1 0.9 0.4 0.4 Thalamus 9.7 7.7 2.6 1.0 0.9 1.9 2.3 0.8 0.4 0.4 Mean±SD 7.9±2.7 6.8±2.3 2.7±1.3 1.4±0.6 1.2±0.5 1.7±1.0 3.3±2.2 1.0±0.5 0.6±0.2 0.5±0.2 TABLE VI MDE METRIC FOR HRRT 18F-FDG AND MCT 18F-FPEB STUDIES. ANATOMICAL CENTER OF MASS DISTANCE ERROR METRIC COMPARED 643 Voxels TO THE GOLD-STANDARD VICRA. REPORTED VALUES IN MM AND ARE REPORTED AS MEAN±SD. Method HRRT 18F-FDG m CT 18F-FPEB NMC 1.92±1.86 1.96±1.59 SIM 1.86±0.54 1.59±0.53 DL-HMC 0.65±0.41 0.80±0.61 Du SFE 0.44±0.23 0.76±0.72 DL-HMC++ 0.39±0.11 0.65±0.66 TABLE VII CROSS-TRACER GENERALIZATION RMSE RESULTS. Tasks Trans. (mm) Rot. (deg) Transverse Coronal Sagittal 18F-FDG NMC 6.29±5.79 3.12±1.42 11C-UCB-J to 18F-FDG 1.50±0.37 1.38±1.52 DL-HMC++ on 18F-FDG 1.27±0.46 1.16±1.20 Fig. 5. 3D PET Cloud Image (PCI) Dimensions. Example one-second HRRT PET cloud images of different dimensions and resolutions: (top) 323 voxels; (middle) 643 voxels; and (bottom) 963 voxels. 18F-FPEB NMC 2.42±1.43 1.36±0.48 11C-LSN3172176 to 18F-FPEB 0.74±0.02 0.55±0.00 DL-HMC++ on 18F-FPEB 0.54±0.00 0.40±0.00 IV. DISCUSSION DL-HMC++, a novel supervised deep learning model for PET head motion estimation with a cross-attention module, demonstrates effective motion estimation capabilities with- out the need for external hardware-based motion tracking (HMT) on testing subjects from two different scanners and four different tracers in a large cohort study. Our evalua- tion on two different PET scanners, HRRT and m CT, using four different tracers, 18F-FDG, 18F-FPEB, 11C-UCB-J, and 11C-LSN3172176, shows that DL-HMC++ outperforms other benchmark SOTA methods, yielding motion tracking results similar to gold-standard Vicra HMT. Qualitative and quantita- tive results demonstrate that the proposed method effectively eliminates motion blurring for head PET scans. In addition, we validate each contribution of our model design choices through comprehensive ablation studies. By integrating the cross-attention mechanism, our model establishes spatial cor- respondences between the reference and moving PCIs, which enhances the ability of the model to track motion. Compared to the original DL-HMC implementation, the cross-attention mechanism guides the network to focus on motion-relevant information, diminishing the influence of irrelevant features. This process not only enhances the precision of the motion es- timation but also improves robustness across the scan duration. Remarkably, despite extremely blurry images (Fig. 5), DL- HMC++ demonstrates anatomical motion errors of magnitude ∼1 mm (Tab. VI) that are far below the input PCI voxel size CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION DLHMC++ Vicra Coronal Transverse Sagittal SSIM: 0.977 NMSE: 0.009 SSIM: 0.960 NMSE: 0.024 SSIM: 0.942 NMSE: 0.034 SSIM: 0.844 NMSE: 0.131 SSIM: 0.956 NMSE: 0.023 1.00 0 40 Intensity (k Bq/cm3) Caudate 0.00 Thalamus -1.00 SSIM: 0.965 NMSE: 0.013 SSIM: 0.944 NMSE: 0.028 SSIM: 0.878 NMSE: 0.065 SSIM: 0.889 NMSE: 0.071 SSIM: 0.938 NMSE: 0.027 1.00 0 40 Intensity (k Bq/cm3) 0.00 Parietal -1.00 SSIM: 0.966 NMSE: 0.013 SSIM: 0.923 NMSE: 0.040 SSIM: 0.884 NMSE: 0.060 SSIM: 0.801 NMSE: 0.138 SSIM: 0.942 NMSE: 0.026 1.00 0 40 Intensity (k Bq/cm3) Frontal 0.00 -1.00 Fig. 6. MOLAR Reconstruction comparison and error map between different MC methods for an HRRT 18F-FDG testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. SOTA data-driven motion tracking method, we implemented the IMR method following Spangler-Bickell’s work on the m CT dataset. However, the motion estimation result reveals that all DL methods, especially DL-HMC++, outperform the IMR result. In addition, we performed an ablation study for the IMR using 8 randomly selected subjects from the m CT 18F- FPEB dataset. Following optimization strategies in , mo- tion estimation performance without 6-mm Gaussian filtering, FRI input and dynamic reference frame were evaluated and the results are summarized in Table VIII. The IMR ablation result demonstrates that FRI is the primary contributor to the performance improvement of IMR, where filtering and dynamic reference frame did not affect the performance. Notably, compared with DL-HMC++, a significant limitation of applying IMR is the need to develop a fast reconstruction platform to support fast reconstruction frames, alongside the requirement for fine-tuning for different tracers. In our studies, due to the patient’s posture for the PET scan, movements in the rotation along the Y-axis (vertical direction) TABLE VIII COMPREHENSIVE ABLATION STUDY FOR IMR METHOD ON THE MCT 18F-FPEB DATASET Method Trans. (mm) Rot. (deg) IMR 1.64±0.49 0.78±0.34 w/o filter 1.55±0.54 0.77±0.35 w/o FRI 4.30±6.31 1.43±0.46 w/o dynamic reference 1.53±0.40 0.76±0.34 of ∼10 mm3 for both the HRRT and m CT studies. The observed failures and performance degradation for intensity-based registration methods on 11C dataset, e.g., the IMR result on 11C-LSN3172176 dataset (mean translation error 2.32 mm) compared to the 18F-FPEB dataset (mean translation error 1.38), are expected. This is due to the intensity variations and noise in the dynamic input data, especially when comparing the appearance differences between the first reference time frame and the later frames. To compare with 12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2025 NMC SIM DLHMC Du SFE DLHMC++ Vicra Coronal Transverse Sagittal SSIM: 0.989 NMSE: 0.019 SSIM: 0.986 NMSE: 0.023 SSIM: 0.861 NMSE: 0.343 SSIM: 0.852 NMSE: 0.369 SSIM: 0.988 NMSE: 0.021 1.00 0 20 Intensity (k Bq/cm3) Caudate 0.00 -1.00 SSIM: 0.970 NMSE: 0.027 SSIM: 0.965 NMSE: 0.034 SSIM: 0.746 NMSE: 0.351 SSIM: 0.739 NMSE: 0.353 SSIM: 0.969 NMSE: 0.028 1.00 Intensity (k Bq/cm3) 0 20 0.00 -1.00 SSIM: 0.960 NMSE: 0.030 SSIM: 0.956 NMSE: 0.037 SSIM: 0.758 NMSE: 0.296 SSIM: 0.755 NMSE: 0.288 SSIM: 0.956 NMSE: 0.034 1.00 Intensity (k Bq/cm3) 0 0.00 -1.00 Fig. 7. MOLAR Reconstruction comparison and error map between different MC methods for an m CT 18F-FPEB testing subject. Arrows on reconstruction images indicate the specific brain ROI for visual comparison. from all subjects were extremely small, making it challenging for the model to capture. One reason is that Y rotation is less frequent than X (horizontal direction) rotation and Z (patient bed movement direction) rotation, resulting in less variability in Y rotation for the model to learn. Additionally, Y rotation tends to have a small magnitude. Both reasons make it more difficult for the model to capture Y rotation changes compared with translation changes. Two possible solutions can be used to alleviate this. One is to assign a higher weight to Y rotations in the loss function. The other is to perform data augmentation to increase the variability of Y rotations. We further compared the performance and computational efficiency of two deep learning methods with attention mech- anism, Du SFE and DL-HMC++. As shown in Table IX, the quantitative analysis demonstrates that DL-HMC++ achieves a 13.6% improvement in translation and a 12.5% improvement in rotation compared to Du SFE. Additionally, the lightweight architecture of our proposed framework substantially enhances our advantages. Specifically, DL-HMC++ shows a 37% reduc- tion in the number of parameters (2.2M vs. 3.5M), an 81% de- crease in computational cost (4.0G FLOPs vs. 21.3G FLOPs), and a 57% faster inference time (3.30ms vs. 7.67ms). These enhancements highlight the efficiency of DL-HMC++ in terms of resource utilization and computational speed, making it a more viable option for potential real-world applications where computational resources and time are critical constraints. The consistent outperformance of DL-HMC++ across all datasets further underscores its robustness and reliability in motion estimation tasks. Through comprehensive experimentation across diverse tracer types and scanner cohorts (Tab. II), we identified performance improvements resulting from the removal of the time-conditioning module from the original DL-HMC architecture. Although this module was initially designed to CAI et al.: PET HEAD MOTION ESTIMATION USING SUPERVISED DEEP LEARNING WITH ATTENTION 13 model for various tracers and scanners, including an ultra-high performance human brain PET/CT scanner , which has a spatial resolution of less than 2.0 mm and is more sensitive to motion effects. We will also investigate the feasibility of applying semi-supervised learning and unsupervised learning for PET head motion estimation . TABLE IX COMPUTATIONAL EFFICIENCY AND PERFORMANCE COMPARISON BETWEEN DL-HMC++ AND DUSFE. REPORTED VALUE OF AVERAGE TRANSLATION AND ROTATION ERRORS ARE THE MEAN VALUE OF ALL DATASETS. Method Parameters (×106) FLOPs (×109) Inference Time (ms) Memory (GB) Avg. Trans. Avg. Rot. V. CONCLUSION In this paper, we proposed DL-HMC++ to predict head motion directly from PET raw data to achieve robust data- driven head motion estimation. DL-HMC++ incorporates a cross-attention mechanism to compute the correlation between two one-second PET cloud images. Cross-attention boosts the model’s ability to track the motion by establishing spatial correspondence between the two images to be registered and focuses network learning on the most important regions of the image for head motion. We validated DL-HMC++ in a large cohort PET study with 4 different tracers on more than 280 subjects, and the results demonstrated significant motion estimation performance improvements both qualitatively and quantitatively compared to SOTA data-driven head motion estimation methods. Extensive evaluation and ablation studies demonstrate the superior performance and feasibility of our proposed DL-HMC++ to address head motion estimation for PET without the need for hardware-based motion tracking. Furthermore, the cross-tracer generalization experiment high- lights the potential of the proposed network to effectively generalize across various tracers. Du SFE 3.5 21.3 7.67 30.3 1.18 0.96 DL-HMC++ 2.2 4.0 3.30 6.9 1.02 0.84 enhance temporal information encoding, our findings indicate that it introduces redundancy: the sampling strategy and image data already provide sufficient temporal information. This redundancy leads the model to neglect spatial information, resulting in overfitting on the training data. In the ablation study, we explored using different PCI sizes ranging from 323 to 963. The results indicate that increasing the voxel size of the cloud image led to a degradation in performance. A possible reason for this decline is the increase in noise levels and the corresponding decrease in the signal- to-noise ratio with larger dimensions. Our findings suggest that larger voxel sizes provide a more stable and robust signal representation, which is crucial for accurately detecting motion even under noisy conditions. In the cross-tracer generalization experiment, we explored the possibility of using a pre-trained network on different tracer datasets. Due to the intrinsic characteristics of 11C, the PCIs are noisier and thus more challenging to train. By applying a network trained on such a difficult dataset to a dataset with more stable tracer dynamics at late time points (e.g., 18F), we demonstrated that DL-HMC++ exhibits gener- alizability across different tracers. Less intuitively, performing the cross-tracer experiment in the opposite manner, using a model pre-trained on 18F and applying to 11C at test time, suffered from model failure. Future studies are needed to study this cross-tracer phenomenon in detail. Future work will also consider applying DL-HMC++ to other sites, using the pre- trained network with few-shot fine-tuning to ensure that the network adapts to site-specific variations. The motion estimation methods in our study estimate trans- formation metrics from different images generated from PET raw data. Theoretically, motion parameters can also be directly estimated from sinograms, and it is feasible to employ deep learning algorithms for this purpose. However, part of our dataset includes TOF information, which causes the sinogram size to be much larger than the image size. In the future, we will explore the possibility of applying DL-HMC++ to other domains, such as sinograms and COD traces. The proposed DL-HMC++ method exhibits certain limi- tations. Although DL-HMC++ achieves comparable motion tracking results with short half-life 11C tracers, it exhibits a notable constraint in its inability to effectively detect motion during periods of rapid tracer dynamic changes, such as the first 10 minutes post-injection. Moreover, Vicra failure and inaccuracy may have a negative effect on the proposed super- vised model. In the future, we aim to develop a generalized