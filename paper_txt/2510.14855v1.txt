Early detection of melanoma has grown to be essential because it significantly improves survival rates, but automated analysis of skin lesions still remains challenging. ABCDE, which stands for Asymmetry, Border irregularity, Color variation, Diameter, and Evolving, is a well-known classification method for skin lesions, but most deep learning mechanisms treat it as a black box, as most of the human interpretable features are not explained. In this work, we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect, opening more windows for future exploration. The A, B, C, and D values are quantified particularly within this work. Moreover, this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary, the classification worked with an accuracy of around 89 percent, with melanoma AUC being 0.96, while the feature evaluation performed well in predicting asymmetry, color variation, and diameter, though border irregularity remains more difficult to model. Overall, this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria, thus improving our understanding of skin cancer progression. Keywords: melanoma, skin lesion, ABCDE criteria, deep learning, HAM10000 dataset, Generative adversarial networks (GANs), benign nevi, malignant melanoma Melanoma, an aggressive form of skin cancer, is one of the leading causes of death due to skin cancer . Early diagnosis is important because the 5-year survival rate exceeds 90% for early-stage melanoma, but drops below 20% for advanced stages . In order to differentiate between harmful and harmless lesions, dermatologists utilize the ABCDE method. “A” stands for “asymmetry,” as malignant skin lesions often appear to be uneven; “B” stands for “border irregularity,” as scientists search for jagged or notched edges; “C” Preprint submitted to ar Xiv October 17, 2025 stands for “color variation”; “D” stands for diameter, as larger lesions are more likely to be malignant; and “E” stands for “evolving,” as skin lesions evolve over time . If a lesion displays two or more of the attributes described above, the lesion is most likely harmful melanoma. The ABCDE criteria are effective because they are easy to understand and to screen for suspicious lesions . Many deep learning techniques are proficient at classifying skin lesion images as either benign, malignant, or one of several other clinically recognized categories found in datasets such as HAM10000 . Convolutional neural networks are utilized most commonly, as they are trained to classify images. However, these CNN models lack clear interpretabil- ity because they only provide point predictions of lesion type without explanation. In the context of melanoma detection, this lack of explanation can hinder clear, interpretable di- agnoses. The “E” feature of ABCDE has also proven challenging to evaluate via deep learning methods. Single static images fail to capture such a change. Some mobile applications ask users to input their moles periodically to observe any changes, but this is difficult to enforce. However, using AI to predict and visualize how a lesion changes over time is a promising direction. Recent advances in medical imaging have made it possible to create realistic transformations of medical images. For example, J ̈utte et al. (2024) utilized a Cycle GAN to create a sequence of dermoscopic images that show the potential of a benign nevus transforming into a malignant melanoma . As discussed above, the quantification of ABCDE features changing over time as well as the actual images changing can improve our understanding about melanoma growth patterns . In this work, we propose a deep-learning framework that combines classification, ABCDE feature quantification, and feature evolution simulation. First, we design a CNN that learns to predict both the lesion’s class and quantify continuous values representing each ABCDE criterion. Second, we develop a strategy for obtaining quantitative ABCDE labels from dermoscopic images by image processing and expert knowledge. This enables supervised training of the feature regression branch. Subsequently, we introduce a module to simulate the temporal evolution of lesions. Generative adversarial networks and sequential interpola- tion are used so that this system can produce a plausible future state of a given lesion and track how the ABCDE scores change. Automated ABCDE analysis was already prevalent before the era of deep learning. Re- searchers wanted to mimic the ABCD rule of dermoscopy with computer algorithms. Early systems computed hand-crafted image features corresponding to asymmetry, border irregu- larity, color variegation, and lesion diameter. For example, in 2001, Ganster et al. extracted 122 features related to the ABCD criteria and applied conventional classifiers for automated melanoma recognition . Asymmetry was quantified via shape moments, border irregu- larity via fractal dimension or edge abruptness, color variegation via histogram analysis, and diameter via lesion area in pixels . These pipelines showed that it is feasible to use computer vision for melanoma screening, but they often need expert parameter tun- ing and struggle with variations in image quality. A recent survey by Celebi et al. (2022) summarizes many such efforts to automate parts of the ABCD rule . In general, while these approaches brought interpretability (each feature could be reported), their accuracy 2 was typically lower than that of data-driven deep learning, which can learn more complex representations. In addition, the success of CNNs in image recognition has led to numerous applications in dermatology. CNN models have become very accurate while classifying lesions into cate- gories such as melanoma, basal cell carcinoma, nevus, etc. For example, on the HAM10000 dataset, approaches like an Efficient Net or an ensemble of CNNs reach high overall accuracy (often 85–90%+) in 7-class classification . Some research focuses on binary classification (melanoma vs. benign), as they report very high specificity and good sensitivity . Still, as described above, these models lack interpretability. Other researchers have used saliency maps and class activation maps in order to improve interpretability. However, these pixel- level explanations do not directly communicate which clinical features are most explanatory of the diagnosis. This motivates the usage of the ABCDE criteria explicitly. Choi et al. (2024) proposed an “ABC ensemble model” that preprocesses images to emphasize asym- metry, border, and color regions as they feed them into specialized network branches; these are then combined for classification . Still, their model did not output quantitative values for each criterion, even though they were able to use ABCD rule knowledge to improve classification performance. Notably, they omitted the diameter feature because the scaling in the data was inconsistent . In summary, this work aims to build upon prior research by combining the interpretability of rule-based features with the accuracy of deep learning. This work also extends on previous research by adding the ability to simulate temporal changes. 2.1. Overview of the Framework The framework contains two main components: a CNN to perform lesion classification and ABCDE feature regression from a dermoscopic image, and also an evolution simulation module that shows how ABCDE features might progress over time. Given a dermoscopic image of a skin lesion, we first optionally preprocess it (including lesion segmentation and color normalization). The multi-task CNN then processes the image to output both a class prediction and a set of numeric scores corresponding to A, B, C, and D features. The CNN is optimized by using a combined loss that includes classification error and regression error on the ABCDE scores. After this model is trained, it can provide an interpretation for its diagnosis by showing the ABCDE scores. For the evolution simulation, we take a lesion image and generate a sequence of future images showing increasing malignancy. This CNN model is applied to each generated frame to track how the ABCDE scores change; it basically gives a trajectory of the features. Additionally, the model’s internal representation is used to predict feature values without image generation. 2.2. Multi-Task CNN Architecture This multi-task deep learning model is built based on a convolutional neural network that first extracts a shared representation of the input image, followed by two “heads” (out- put branches). These branches consist of one for lesion classification and one for ABCDE 3 feature regression. This design allows the model to learn common visual features that are useful for both tasks . Still, the separate heads specialize in their respective outputs. We experimented with several backbone architectures like Res Net50 and Efficient Net, but we ultimately chose Res Net50 due to its balance of depth and efficiency . The classifica- tion head is a dense layer that produces a probability distribution over the lesion classes. The regression head is a fully-connected dense layer to produce 4 values corresponding to [A, B, C, D] (E is not supervised). Overall, we use linear outputs with appropriate acti- vation/normalization; we do this to make sure that the feature values fall in a reasonable range (0 to 1 for A, B, C and a scaled range for D as discussed later). The input for this network is a dermoscopic image. The images are rescaled to 224 by 224 pixels, and we also normalize all the color channels. The Res Net50 backbone processes the image through a series of convolutional layers. This gives a final feature map which is global-average-pooled to a 2048- dimensional feature vector. This vector represents high- level information about the lesion. Also, the network is trained to predict the ABCDE features, so the vector encodes information relevant to asymmetry, border, color, and others, in addition to other features useful to classify lesions. After that, we add a fully connected layer called the classification head. This takes in the 2048 feature vector and produces logits for each of the seven classes for HAM10000 . These include nv, mel, bcc, akiec, bkl, df, and vasc and correspond to melanocytic nevus, melanoma, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma, and vascular lesion . During training, we use a cross-entropy loss for this head. The regression head maps the same feature vector to five numeric outputs representing [A, B, C, D, E]. No activation (linear output) is applied for regression. However, these values are constrained through the training data scaling and loss function; this is so that the outputs remain in plausible ranges. Mean squared error loss is used here as it sums over the 5 features for this head. 4 round or evenly colored lesions yield a score near 0, but irregularly shaped or unevenly colored lesions yield a score closer to 1. The final value is stored in the variable “A”. • Border Irregularity (B): An irregular border is one that is ragged, notched, or blurred. We capture two aspects which are the shape irregularity and the sharpness of the border . For shape irregularity, we compute the lesion’s convex hull and compare it to the actual border . After that, we define the ratio between the lesion and the convex hull. A perfectly smooth-edged lesion has a ratio of 1.0, but lesions with indented edges or notches have a ratio less than 1. Higher values mean more irregularity. In order to analyze the border clarity, we take an approach similar to that of J ̈utte et al.; we compute the gradient magnitude along the lesion perimeter. We take the dermoscopic image, calculate the gradient around the border, and average it over all border pixels. Sharp borders have a higher gradient while fuzzy borders have a low gradient. We normalize this gradient value to . For the overall border irregularity score, we combine shape and gradient. In this equation, the gradient B value is inverted because a low gradient (fuzzy border) should increase the irregularity score. Thus, a lesion with a very jagged shape or a very blurred edge will have B near 1. • Color Variation (C): We measure how many different colors and shades are in the lesion. Common criteria for skin lesion images include colors like light brown, dark brown, black, blue-gray, white, and red . A melanoma often has different colors. To quantify this value, we compute the dispersion of colors in the lesion. Specifically, we apply a clustering in color space to the lesion pixels . The number of color clusters is determined, and then the dispersion index is calculated; this is the standard deviation of color distances of pixels from their cluster centroids as they are weighted by cluster size. This gives a value between 0 and 1 as 0 signifies a very unified color while 1 gives off a heterogeneity of colors. Additionally, we count the number of distinct color clusters. If there are more clusters, there is more color variety. We map the number of clusters to a 0–1 range as well. Our final color variation score C is a combination (we used the dispersion index primarily, and added a small increment for each distinct color beyond one). Benign single-color nevi typically score a value of C less than 0.2 while many melanomas with multiple shades of brown, black, and red would score more than 0.7. • Diameter (D): For the most part, lesions with a diameter greater than 6 millimeters are deemed as suspicious. However, these images lack a consistent physical scale as the zoom level varies . HAM10000 images come from different devices and magnifica- tions . Unfortunately, no data on mm-per-pixel is given. That is why we will have to approximate the diameter relatively. After segmentation, we compute the maximum distance across the lesion (in pixels); this is the largest axis of the bounding box. We 6 also compute the area-equivalent diameter, which is the diameter of a circle with the same area as the lesion. This max span is more robust for irregularly shaped lesions. To map this to a score, we make an assumption that an average nevus in the dataset (say 30 pixels across in the image) corresponds to about 5 mm; this is based on typical dermoscope resolution. D = clamp max diameter (pixels) p6 mm , 0, 1  This is where p6 mm is the approximate pixel length of 6 millimeters. For better cali- bration, one could include a reference scale or use the dataset’s metadata; some ISIC images have ruler markers, but this is not the case in HAM10000. In our training labels, we set p6mm such that the top 5% largest lesions get D close to 1.0 and small lesions get D near 0.2. The diameter values are thus relative. We treat D as a contin- uous variable because it influences risk in a graded way even though it is non-linear. • Evolving (E): Because the dataset does not contain time-series images, we could not directly measure or predict lesion evolution. That is why in this work we focus only on the static ABCD features for regression. The E criterion was not included in the training or loss function. Future work will investigate modeling lesion evolution. It will utilize time-series data to better capture this aspect. It is still good to note that the automatically generated labels for A, B, C, and D are approximate. This is because errors in lesion segmentation or variations in image conditions can introduce noise. For example, if a lesion image is very close-up, the diameter estimate may falsely appear large; if lighting causes part of the border to fade into the skin, the border gradient metric might be low even if the border is fairly smooth. These issues could be mitigated with preprocessing. Applying a light hair removal filter (using inpainting for dark hairs) and a color normalization so that background skin has a consistent reference color across images. This can improve the accuracy of the feature computations. These computations output for each training image an A, B, C, and D value. We use these as the target outputs for the regression head. In this work, E is not directly mentioned but only represents the evolution or progression of the other A, B, C, and D features. 2.4. Model Training Strategy The model is trained on the HAM10000 dataset ; this dataset contains 10,015 der- moscopic images of pigmented lesions across 7 diagnostic categories. However, images are not evenly distributed across diagnostic categories (benign and melanoma). To handle this, we perform data balancing. Specifically, we use a combination of oversampling and class- balanced loss. During each training epoch, we sample images such that each class is roughly equally represented; this is done with the help of random oversampling of minority classes. We also weight the classification loss inversely proportional to class frequency. The data is split in this manner: 70% of the images for training, 10% for validation, and 20% for testing. We also improve the training images with random horizontal or vertical flips, small 7 rotations, zoom-in and out, and lighting adjustments. This helps to expose the network to varied appearances and also slightly simulates changes. In order to preprocess each image, we resize it to 224x224, apply the hair removal filter, normalize the color, and segment the lesions. For the loss functions, we have to combine the losses for both the heads of the model (classification and regression). The classification loss is the cross-entropy loss for the class prediction while the regression loss is the mean squared error between predicted and target scores for features A through D. We control the balance between these two losses using a weight parameter. In the main experiments, we set this value to be 1. However, it is also good to run other tests by setting that value to either 0 to train only classification or setting the value to a larger number than 1 to focus only on regression. This is very important because it shows why combining both tasks in this deep learning framework is beneficial. For the optimization functions, the Adam optimizer is used with an initial learning rate of 0.0001. If the validation loss stops improving, we reduce the learning rate by a factor of 10. The training itself runs for around 100 epochs. To help prevent overfitting, we apply L2 weight decay with a value of 0.00001. All of these experiments use Py Torch and are trained on an NVIDIA A100 GPU. For lesion classification, we evaluate standard metrics including the overall accuracy, per- class accuracy, precision, sensitivity, and F1-score for each class, and especially the melanoma detection specificity. Missing a melanoma would be the worst error. We also compute the balanced multi-class accuracy and the area under the ROC curve for the binary melanoma vs others task. For the ABCD feature regression, we measure the Pearson correlation between the predicted and ground-truth feature values on the test set; we also take a look at the mean absolute error. High correlation would mean that the model has learned to predict the features in relative terms even if there is some bias. Because the ground truth features (ABCD) are approximate, we focus more on correlation and rank ordering than exact error magnitude.We do not have ground truth for “E” on static images, so we cannot directly evaluate E predictions in the standard test set. To take care of the E, we will evaluate E in the context of simulated sequences. 2.5. Lesion Evolution Simulation Additionally, we propose a method to simulate how a lesion’s image and thereby, its ABCD features might change over time. This module operates in two possible modes. The first possible method for this is to use a generative adversarial network to generate a future version of a lesion image. Similar to the study presented by Jutte et al., a Cycle GAN architecture is optimal here in order to translate an image from the domain of benign- appearing lesions to the domain of malignant-appearing lesions . The Cycle GAN learns to produce an image that does indeed retain the structure of the input, like the particular lesion’s shape and significant features, but it changes its appearance to resemble a melanoma. Evidently, according to what was mentioned above with the ABCD rule, higher asymmetry, more colors, and more irregular borders with the same background and context would show exactly that of a benign lesion evolving into a malignant melanoma. We also train the reverse mapping from a melanoma to a nevus using Cycle GAN’s cycle-consistency; this is so 8 that the network doesn’t simply always output a generic melanoma. To simulate a gradual evolution, we use frame interpolation ; this produces intermediate images that represent smooth transitions between benign and malignant appearances. Each frame is then analyzed by the multi-task CNN as it outputs ABCDE scores. By plotting the scores over frames, this yields a curve for each feature in order to exhibit the trend. These intermediate frames are not strictly physically accurate predictions because melanoma growth is not necessarily linear, but they are a good visual to provide clarity. The sequence can be thought of as a what-if scenario for how the lesion could evolve if it were to become malignant . An alternate approach to this would be directly observing the feature values. The idea is to use the internal representation of the lesion, say the 2048-d feature from the CNN, and model how it would drift as the lesion evolves. The multi-task CNN’s feature extractor would be a state encoder in this scenario. Then, the “time steps” would be simulated by a small network that adjusts this state in the direction of malignancy. We train a simple feed-forward network to model how lesion features change over time. At each time step, it updates the latent feature vector zt by predicting how it will change. This is trained with the help of feature sequences extracted from images generated by the Cycle GAN model. For each simulated sequence, we collect CNN features from each frame, and this gives us starting and ending feature pairs. The network learns to predict the difference between the initial and final features, and this helps it learn how features evolve as a lesion becomes more malignant. After training, we can simulate progression by applying small steps in the predicted direction. For example, starting with a benign lesion’s features, we can apply several steps and watch the ABCD scores increase; this parallels malignant transformation. While this method does not generate images, it is faster and does not need to store or run the image generator during inference. The model itself learns how features move towards malignant characteristics over time. In the final system, the image generation approach above would be used primarily for visualization while the second approach would be used to verify that the direction of change in feature space corresponds to increasing ABCDE scores. Something that is important to note is a smooth change in segmentation masks for reliable ABCDE measurement; this is pointed out by J ̈utte et al. . These sequences are not used to update the CNN. There should be no further training as that would require known ground truth of future states. They are purely for evaluation and demonstration. For this study, the latter approach (directly observing feature values) is performed to visualize plausible ABCD trajectories, and no image synthesis is used in the reported experiments. On the HAM10000 dataset, the multitask CNN model did end up showing a strong classification performance. The overall accuracy was 89%; it correctly classified 89% of all test samples. Also, it had a balanced accuracy of 76%. Some lesion types are much more common than others in the dataset, so the balanced accuracy gives equal weight to each class. This is regardless of how frequent or rare it is. It did end up performing well across all lesion types, even the less common ones. 9 In the specific task of finding the difference between melanoma and all other lesion types, the model was effective. The ROC curve in Figure 2 performed very well. The area under the curve (AUC) is 0.96. That means that the model is very efficient in classifying and at identifying melanomas correctly while minimizing false positives.