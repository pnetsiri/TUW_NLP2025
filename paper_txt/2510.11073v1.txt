Patient face images provide a convenient mean for evaluating eye diseases, while also raising privacy concerns. Here, we introduce ROFI, a deep learning-based privacy protection framework for ophthal- mology. Using weakly supervised learning and neural identity translation, ROFI anonymizes facial features while retaining disease features (over 98% accuracy, κ > 0.90). It achieves 100% diagnostic sensitivity and high agreement (κ > 0.90) across eleven eye diseases in three cohorts, anonymiz- ing over 95% of images. ROFI works with AI systems, maintaining original diagnoses (κ > 0.80), and supports secure image reversal (over 98% similarity), enabling audits and long-term care. These results show ROFI’s effectiveness of protecting patient privacy in the digital medicine era. 1 Image data is crucial for clinical diagnosis [1–3], medical research , as well as artificial intelligence (AI)-aided disease diagnosis [6–15]. Medical images account for over 90% of all medical data and grow by more than 30% annually . However, the collection of personal medical images also increases the risk of privacy breaches [18–26]. Thus, the development of anonymization methods [27– 33] is increasingly critical, aiming to remove the personal identifiable information from the images. Among all medical image types, the facial images draw particular attentions [34–36], as it includes rich biometric identifying information, and is widely adopted in access control . In ophthalmology, facial images play a more prominent role than many other medical special- ties [39–45], particularly for diagnosing external eye conditions like strabismus, ptosis, and thyroid eye disease (TED). These eye diseases manifest through visible signs within the periocular areas and the related facial tissues [46–49]. Traditional anonymization techniques, such as cropping out the eye , blurring or applying mosaics to faces, are insufficient, since advanced face recognition systems [53–55] can still identify individuals from these altered images [56–58]. Artificial Intelligence Generated Content (AIGC)-based methods [59–64] could further distort the identity, but at the cost of obscuring local details critical for diagnosis. Face swapping techniques [65–70] replace original facial features with those of another individual, such as a celebrity’s . Similarly, face de- identification methods [72–78] transform the face into a totally virtual one. While protecting privacy, the above approaches also heavily alter the clinical signs necessary for eye disease diagnosis. Therefore, it is urgent to develop ophthalmic sign-preserving facial anonymization methods. Indeed, there are few preliminary efforts on this, which adopt hand-crafted clinical fea- tures such as facial morphology [81–83] parameters and facial keypoints to partially represent the disease signs. These hand-crafted features are enough for diagnosing a limited spectrum of dis- eases such as ptosis , but are incapable of handling many other complex conditions. For instance, conjunctival melanoma manifests with diverse conjunctival pigmentation and lesion patterns involving multiple regions. Squamous Cell Carcinoma presents with highly-variable red patches or nodules on the eyelid or ocular surface. Additionally, the above methods are not reversible, limiting their ability to retrieve personal medical records for decision-making and medical audits . In this article, we introduce ROFI (Figure 1a,b), the first ophthalmic sign-preserving and reversible facial anonymization approach. ROFI is achieved through two key designs: (1) Data-Driven Ophthalmic Sign Detection: Using weakly-supervised learning techniques, given a large-scale set of patient faces annotated with binary labels (whether or not they have eye disease), a neural net- work automatically separates disease-related signs from facial images. This achieves autonomous sign detection with affordable data annotation cost. (2) Reversible Neural Identity Translation: Lever- aging the flexible feature translation capability of the Transformers , we build a pair of Neural Identity Protector and Neural Identity Restorer models to achieve reversible transformation of facial identities. The procedures are governed by a customized privacy key. This enables accurate recon- struction of original images when being authorized. Compared to previous approaches , our approach is uniquely usable throughout the entire medical process. During early screening of eye diseases using facial images, ROFI largely alleviates the patient privacy issue. In subsequent rigorous examinations, it reconstructs the original images to refer to the history medical records. To verify the effectiveness of ROFI, we conducted a comprehensive study across three clinical centers (Figure 1c) enrolling 17,181 patients from Shanghai Ninth People’s Hospital (SNPH), 493 patients from Eye Center of Xiangya Hospital of Central South University (ECXHCSU), and 79 patients from Renmin Hospital of Wuhan University (RHWU). We evaluated the clinical usability of ROFI in two key aspects: 1) the completeness of ophthalmic signs, and 2) the effectiveness for screening eleven ophthalmic diseases with external manifestations. Then, we explored the usability of ROFI-anonymized images for medical AI models. Further, we assessed the facial anonymization capability with modern face recognition systems. Finally, we evaluated the similarity between the reversibly reconstructed images and the originals, and utilized the reconstructed images to retrieve historical medical records, assessing the efficacy of hormone therapy for thyroid eye disease (TED). 2 2.1 Cohort Building For the development and evaluation of ROFI, we included 17181 patients who had undergone ophthal- mologic examinations and had facial images captured at three hospitals in China between January 1, 2018, and December 25, 2023. All patients involved in this study have agreed to participate in the project either by themselves or via their legal guidance. The derivation cohort comprised patients from Shanghai Ninth People’s Hospital (SNPH), which served as the primary site for training and internal validation. Patients were included in the study if they met the following criteria: (1) Had at least one facial image available that was taken within the study period. (2) Were diagnosed with ophthalmic diseases characterized by external manifestations. (3) Provided images that contained sufficient quality and clarity for ophthalmic sign detection. Patients were excluded if: (1) Their facial images were of insufficient quality, such as being too blurry or having significant occlusions that could interfere with the identification of ophthalmic signs. This resulted in the final SNPH cohort comprising 12,289 patients. We randomly divided the SNPH cohort into three subsets: 11,836 for developing, 222 for model-selection, and 231 for internal validation. For external validation, two additional cohorts were established, ECXHCSU and RHWU, with 246 and 48 patients, respectively. These patients were enrolled using the same criteria as the SNPH cohort. The external test sets aim to assess the ROFI model’s generalization across different clinical settings. Given the extensive annotation effort required for disease-related region mask labeling, developing set images were annotated only with binary labels indicating patient health status. Weakly-supervised learning techniques were then applied to autonomously localize disease signs, thereby reducing the physicians’ workload. For the validation sets, images were annotated with the corresponding oph- thalmic disease name. The study was approved by the Institutional Review Boards of Shanghai Ninth People’s Hospital (Approval No.SH9H-2022-T380-1), Eye Center of Xiangya Hospital of Central South University (Approval No.202407131), and Renmin Hospital of Wuhan University (Approval No.WDRY2024-K238). All procedures involving human participants in this study were conducted in accordance with the ethical standards of the institutional research committee and the latest version of the Declaration of Helsinki . Informed consent was obtained from all individual participants included in the study. For participants who were minors or otherwise unable to provide consent independently, written informed consent was obtained from their parents or legally authorized rep- resentatives. Participation in the prospective study at the ROFI Program was voluntary, and all individuals (or their legal representatives) were fully informed about the nature, purpose, poten- tial risks, and benefits of the study prior to enrollment. Participants were assured of their right to withdraw from the study at any time without any consequences to their care or treatment. 2.2 Model Development We have developed the ROFI framework, a reversible patient face privacy protection approach that particularly preserves ophthalmic disease signs (Figure 1a). Unlike previous methods that either lack ophthalmic sign modeling or rely solely on hand-crafted sign features , we introduced a learnable ophthalmic sign detector, which autonomously mines eye disease-related sign features from vast patient facial images in a weakly-supervised manner. The detected sign features are then used to substitute the ophthalmic features that are altered during the privacy protection procedure. To amend any artifacts caused by the feature substitution process, we employ a Transformer -based feature enhancement network, called DA-Former to refine these features, finally producing the high-quality anonymous patient faces. The sign detector is trained on the patient facial images of developing set, which is labeled with binary health state classifications (healthy or not), with a weakly-supervised region-score-max learn- ing strategy. Subsequently, the neural identity translator, and DA Transformer are jointly optimized, ensuring the generated images are well-protected, highly-realistic, and highly clinically usable. The model details and implementation are described in the Method section. 2.3 ROFI Well Preserved Eye Signs Eye sign evaluation is the fundamental of diagnosing ophthalmic diseases. For instance, the typical symptom of ptosis is the drooping of the upper eyelid . Ocular melanoma is typically associated with the conjunctiva , and some malignant tumors can lead to pupil abnormalities. In this 3 section, we have chosen the following typical eye signs that are relevant to most eye diseases: eyelid shape, iris shape, conjunctival disorder (Conj-Dis) , lacrimal apparatus disorder (LA-Dis) , eyelid disorder (Eyelid-Dis) , and iris disorder (Iris-Dis) . The former two shapes were quantified using eyelid/iris keypoints, which were automatically detected by ocular keypoint detection models . The latter four disorders were assessed manually by ophthalmologists, with each image being reviewed by three specialists. The final diagnosis was determined by majority voting. More explanation of the above signs are provided in the Supplementary Information. To better demonstrate the superiority of ROFI, we compared it with five representative facial privacy protection methods: the conventional approach that applies mosaic to the image (Mosaic) , using state-of-the-art AI generation model to regenerate facial content (AIGC), famous face swapping software Sim Swap (Face Swap), the method specifically designed for ophthalmic use (Digital Mask), and the state-of-the-art face de-identification method (G2Face). The comparison of these methods is given in the supplementary and WUPH validation sets, respectively (Figure 3b and Supplementary Table 4). Given that sensi- tivity measures the ratio of detected positive samples to all positive samples, our approach detected all patients with eye disease and reminded them to go to the hospital, while all other approaches failed. For example, on WUPH, the sensitivity achieved by our approach (100%, 95% CI: 100%- 100%), significantly outperformed Mosaic (85.00%, 95% CI: 74.36%-95.00%), AIGC (77.50%, 95% CI: 64.28%-89.74%), Face Swap (97.50%, 95% CI: 91.89%-100.00%), Digital Mask (85.00%, 95% CI: 72.97%-95.12%), and G2Face (85.00%, 95% CI: 74.27%-95.12%). All other methods missed a consid- erable number of positive cases, which could potentially delay diagnosis and treatment. In contrast, ROFI timely facilitates medical intervention for individuals suffering from eye diseases. For disease diagnosis task, we evaluated the medical outcomes of images protected by different methods using Cohen’s Kappa (κ) values. Our method achieved κ ≥0.81 across all three validation sets for all eye diseases (Figure 3c and Supplementary Table 5), indicating excellent clinical agree- ment between the results obtained from the ROFI-protected and the original images. For instance, on SNPH, ROFI obtained κ values of 0.9594 (95% CI: 0.9033-1.0154) for BCC, 0.9046 (95% CI: 0.7735-1.0357) for CM, 0.8732 (95% CI: 0.7318-1.0147) for CL, 1.0 (95% CI: 1.0-1.0) for SCC, and similar high values for strabismus, ptosis, and TED. Given that a κ value exceeding 0.81 signifies remarkable agreement, ROFI can be effectively deployed for remote clinical diagnosis application, without obviously compromising clinical outcomes. In contrast, Mosaic and AIGC performed poorly across all diseases due to the removal of critical disease signs during processing. Digital Mask showed good results for conditions like TED and ptosis, which can be assessed with hand-crafted features such as eyelid shape, but it failed with other diseases such as BCC and CM, achieving poor κ values of 0.0712 (95% CI: -0.0986-0.2409) and 0.0993 (95% CI: -0.1400-0.3386) for these two diseases on ECXHCSU, respectively. This was far below our approach, which achieved κ = 0.9692 (95% CI: 0.9091-1.0294) and κ = 1.0 (95% CI: 1.0-1.0) for the above two conditions on ECXHCSU. Face Swap and G2Face showed better performance than Mosaic, AIGC, and Digital Mask, but still did not reach κ > 0.81 for any eye disease, suggesting their unsuitability for clinical use. These findings highlighted ROFI as the first clinically-friendly facial image privacy protection method that can be deployed to both immunologic (TED), congenital (e.g., ptosis and microblepharon), corneal (CL), and tumor (e.g., BCC, SCC, and CM) eye diseases. Besides, ROFI achieved a Matthews Correlation Coefficient (MCC) value of 0.9450 with 95% CI: 0.8684-1.0000 on WUPH, which is also much better than G2Face 0.5157 with 95% CI: 0.3705-0.6895, Digital Mask 0.4960 with 95% CI: 0.3390-0.6422, Face Swap 0.6501 with 95% CI: 0.5142-0.8318, AIGC 0.1513 with 95% CI: 0.0016-0.3394, and Mosaic 0.1604 with 95% CI: 0.0449-0.3442 (Supplementary Table 6 and Supplementary Figure 4), further indicating the superior clinical outcome of ROFI. The confusion matrix highlighted that previous methods are effective for only a subset of eye diseases (Figure 3d, Supplementary Figure 5 and Supplementary Figure 6). For example, on WUPH, Face Swap and G2Face excelled with BCC and ptosis but faltered on TED, while Digital Mask performed well on TED and ptosis but failed on BCC. In contrast, our approach maintained well performance across all disease categories. Additionally, we compare ROFI with other approaches on two other tasks, namely, facial landmark detection and tumor segmentation. For the facial landmark detection, we evaluate with the large- scale Celeb A-HQ dataset . We adopt the MTCNN to automatically detect the landmarks of the original and the protected images, and quantify the performance with mean squared error. For the tumor segmentation task, we engaged physicians to annotate the tumor regions for Basal Cell Carcinoma (BCC) instances in SNPH set. The Dice coefficients between the tumor regions from the original and the protected images by various approaches are calculated and compared. As shown in Supplementary Table 7, our method achieves superior accuracy in landmark detection compared to existing techniques. Concretely, the error of landmarks detected from the ROFI- protected images is 2.9871, which is much lower than Mosaic (5.6799), AIGC (7.8945), Face Swap (4.8921), Digital Mask (6.3891), and G2Face (3.7130). The accurate landmark preservation capabil- ity of our approach also implies that it could be potentially deployed to other medical conditions. For instance, precise facial landmark detection is crucial for analyzing facial symmetry, a key clinical indicator in diagnosing conditions such as facial palsy or Bell’s palsy . To evaluate the capability of our method in fine-grained disease detection, we collaborated with board-certified physicians to annotate the tumor regions in BCC (Basal Cell Carcinoma) instances within the SNPH validation set. As shown in Supplementary Table 8, our approach achieves a Dice coefficient of 0.7389, 5 largely outperforming previous methods such as G2Face (0.5782) and Face Swap (0.2783). This sub- stantial improvement demonstrates that our method is not only effective for coarse-level disease classification, but also capable of performing fine-grained disease severity assessment. 2.5 ROFI was Friendly to Medical AI Models An increasing number of artificial intelligence (AI) [6–10] models are being developed to predict patient health conditions from medical images, with some now deployed in real-world applications, e.g., video-based disease screening [109–118]. It is crucial that protected images by ROFI remain recognizable by AI models. To verify this, we divided the SNPH and ECXHCSU datasets into training and test subsets; WUPH was not used due to its too small data size to train an AI model. The models were trained on the training subset and evaluated on test sets processed by different privacy protection methods. We developed facial image-based eye disease diagnostic models based on two neural network architectures: the classical convolutional neural network (CNN) model Res Net50 and a recently-popular Transformer-based model Vi T . Then, we use the trained models to assess the impact of different privacy protection methods on the AI diagnosis performance. The details are provided in the Method section. Our approach, ROFI, achieved excellent consistency with results obtained from original images, as indicated by the highest Cohen’s Kappa values (κ) among all privacy protection methods. For instance, on SNPH using the Res Net50 diagnosis model, ROFI achieved a κ value of 0.9077 (95% CI: 0.8569–0.9586), significantly outperforming the state-of-the-art facial privacy protection method G2Face (κ = 0.7257, 95% CI: 0.6454–0.8060) (Figure 4a). Similarly, with the Vi T diagnosis model, ROFI demonstrated significantly higher κ values compared to other approaches (Figure 4b). Our approach achieved an Area Under the Receiver Operating Characteristic curve (AUROC) of 0.9113 (95% CI: 0.8952-0.9113) on SNPH, when being evaluated using the classic Res Net50, remarkably outperforming Mosaic (AUROC: 0.6102; 95% CI: 0.5941-0.6262), AIGC (AUROC: 0.7438; 95% CI: 0.7262-0.7614), Face Swap (AUROC: 0.7983; 95% CI: 0.7781-0.8186), Digital Mask (AUROC: 0.7853; 95% CI: 0.7816-0.7891), and G2Face (AUROC: 0.8776; 95% CI: 0.8604-0.8949) (Figure 4c). Using the Vi T, our method attained AUROCs of 0.9124 (95% CI: 0.9059-0.9190) on SNPH and 0.7894 (95% CI: 0.7715-0.8072) on ECXHCSU, significantly outperforming other approaches (Figure 4c). Furthermore, we compared the ROC curves and per-disease AUROCs of our approach with other approaches (Figure 4d, Supplementary Figure 7, Supplementary Figure 8 and Supplementary Figure 9). Our approach exhibited ROC curves more similar to those derived from original image data. For per-disease AUROCs, our method either matched or exceeded the performance of the other two approaches. For instance, for the Vi T diagnostic model on SNPH, for BCC, our method achieved an AUROC of 0.950, compared to 0.908 for Face Swap and 0.904 for G2Face. These findings highlighted the high compatibility of our method to medical AI models. We notice that, in some settings, our method slightly outperforms the results obtained with the original images. We conjecture that this improvement arises, because our protected images enhance the representation of eye disease-related features, while reducing individual-specific details. This dual effect likely contributes to more robust diagnostic accuracy. We also trained a stronger Vi T model using input images of size 512×512. As shown in Supple- mentary Table 9, increasing the image resolution leads to a noticeable improvement in AI diagnostic performance across all privacy-preserving approaches. Nevertheless, our ROFI still demonstrates a significant advantage, achieving an AUROC of 94.59%, substantially outperforming the second-best method, G2Face, which achieved 86.75%. 2.6 ROFI Protected Patient Privacy and Enabled Reversibility To assess the efficacy of the proposed ROFI technique in protecting patient privacy, we conducted an investigation into its performance on evading face recognition systems. In a typical scenario, a face recognition network extracts facial features from protected images and compares them against ID card photos of patients. The patient ID corresponding to the image with the closest feature distance is adopted as the recognition result. We conducted the experiments on the SNPH set, as the ID card information is extremely sensitive and difficult to acquire from other external sets. We evaluated the privacy protection performance using two face recognition algorithms: Ada- Cos and Arc Face . When using the Ada Cos , ROFI protected 96.54% of images, surpassing Mosaic (90.91%), AIGC (93.08%), Face Swap (74.90%), Digital Mask (94.81%), and G2Face (93.51%) (Figure 5b). With Arc Face , ROFI maintained strong protection rates (Figure 5b). 6 Furthermore, cropping the eye region, a traditional method for protecting privacy in ophthalmol- ogy , provides insufficient protection. With the Arc Face face recognition method, this approach only safeguarded 68.40% and 74.03% of patients for left and right eyes, respectively, compared to 94.81% with our method (Figure 5c). This finding underscores the urgent need for a novel patient privacy protection method, as the current “unsafe” eye-cropping strategy is widely employed in ophthalmic departments for privacy purposes. Our protection model ROFI fills this blank. To better contextualize ROFI’s performance, we compare it against standard pixel-wise and feature-wise Differential Privacy (DP) methods. We calibrated the noise level in the DP methods to achieve an identity recognition rate ( 3.0%) comparable to that of ROFI. The results (Supplementary Table 13) show that while providing a similar level of empirical privacy, ROFI maintains a significantly higher diagnostic utility (91.25% AUROC) compared to both pixel-wise DP (62.32% AUROC) and feature-wise DP (76.69% AUROC) on SNPH validation set with Vi T- based diagnosis model. The above results show the advantage of our architecture in utility-privacy trade-off. For a more rigorous evaluation, we adopted a state-of-the-art membership inference attack framework based on shadow modeling . We used this framework to calibrate the feature- wise DP method to a comparable level of empirical privacy as ROFI, specifically a True Positive Rate (TPR) of approximately 1.4% at a 1% False Positive Rate (FPR). The full details of our attack methodology are provided in the Supplementary Material. Under this calibrated privacy risk, the diagnostic performance of the DP method dropped to 61.41% AUROC, while ROFI maintained its performance of 91.25% AUROC. This result highlights the significant utility cost incurred by standard DP approaches for medical diagnostic tasks. We further conducted a human face recognition study to assess privacy protection from a human perspective. The study was designed as a forced-choice matching task involving 5 participants, with detailed methodology provided in the Supplementary Material. In an initial phase with 200 trials (supplementary Table 10), a Kruskal-Wallis H-test confirmed a statistically significant difference in protection effectiveness among the various methods (H=21.69, P=0.0006). However, we noted that these initial trials were insufficient to robustly discriminate the performance of ROFI from the two other strong anonymization methods, Digital Mask and G2Face. Consequently, to validate our findings with greater statistical power, we conducted an addi- tional 1,000 trials focusing specifically on these three top-performing methods. The expanded results (supplementary Table 10) showed that ROFI achieved a recognition rate of only 1.1% (14/1200), demonstrating a substantially stronger performance than both Digital Mask at 3.5% (42/1200) and G2Face at 3.1% (38/1200). A one-tailed Fisher’s Exact Test confirmed that ROFI’s superiority is highly statistically significant when compared to both Digital Mask (P = 0.000099) and G2Face (P = 0.000529). These results provide strong statistical evidence that ROFI offers a more effective defense against human recognition. ROFI supports reversibility. With the authorized neural identity restorer software and a private key, the original image can be accurately reconstructed from the ROFI image. Compared to G2Face, the only reversible method examined, our approach achieved superior reversed ID similarity (97.19%) and image similarity (98.47%) over G2Face’s 74.72% and 93.48%, respectively (Figure 5d). The reversed image enabled reliable traceability for medical audits, as well as facilitating the precise retrieval of personalized medical records, thereby enhancing longitudinal examinations. Finally, we assessed the efficacy of TED hormone treatment by comparing the current image with a retrieved historical image (Figure 5e). In the “Single” scenario, where no historical information was utilized, the κ value obtained was 0.2758 (95% CI: -0.0860-0.6378). Using the general reversible privacy protection technique G2Face, κ was only marginally improved to 0.3913 (95% CI: 0.0956- 0.6869), due to the suboptimal reconstruction quality of G2Face. Discrepancies in the reconstructed images, including alterations in skin color and increased blurriness (Figure 5f), impeded accurate retrieval of historical data, resulting in a less reliable comparison with the current image. In contrast, our method, ROFI, yielded the highest κ value of 0.8888 (95% CI: 0.7393-1.0384), attributable to its superior reconstruction performance. In this article, we introduced, developed, and validated a facial privacy protection technique ROFI for the ophthalmic department. ROFI learned from real patient data to prioritize the preservation of ophthalmic sign features that are crucial for the clinical diagnosis, while simultaneously obscur- ing identifying biometrics of patients. By harnessing the potent capabilities of deep learning in 7 autonomous disease sign detection and identification protection, fueled with a large-scale real oph- thalmic patient dataset, ROFI demonstrated superior performance in both clinical diagnosis and privacy protection. ROFI proved to be highly reliable for clinical diagnosis. A comprehensive clinical diagnosis study showed the remarkable consistency (Cohen’s kappa κ > 0.81) achieved by ophthalmologists when utilizing ROFI-protected images, compared to diagnostic results obtained from the original images. In comparison to previous privacy protection methods that does not consider any ophthalmic features or consider only hand-crafted features, our approach significantly outperformed them, particularly for diseases that rely on discriminating local subtle cues, such as Basal Cell Carcinoma, Conjunctival Melanoma, and Corneal Leukoma. ROFI was shown to effectively obfuscate biometric features, thereby providing a high level of empirical privacy protection. The results revealed that the identities of over 95% of patients were successfully obfuscated, preventing recognition by facial recognition systems. The privacy protection rates were comparable to previous facial anonymous techniques, albeit with the latter achieving this by sacrificing some ophthalmic features that are necessary for diagnosing eye diseases. Since our method automatically identified and preserved only the disease-related ocular regions, it avoided introducing additional privacy risks, even with the retention of more complete ophthalmic features. With different private keys, ROFI generates diverse outputs. we randomly sample 100 different keys to encrypt the same input image into distinct results. We then measure the standard deviation of the encrypted images in both the pixel space and the identity feature space. As shown in the supplementary Table 14, varying key leads to a substantial pixel-wise deviation (σpixel = 36.52), indicating that the generated images are visually distinct. Moreover, it results in a large mean pairwise cosine distance of 0.8845 between their identity features. We further visualize the patient face images encrypted by different private key. As shown in Supplementary Figure 10, the different noises leads to different encryption results. This probabilistic generation results ensure that ROFI does not merely anonymize faces, but actively obfuscates them within a broad distribution of possible outputs, significantly increasing the difficulty and cost of any potential privacy attack. ROFI offers several advantages over the traditional eye cropping approach. First, regarding pri- vacy protection, conventional eye cropping may expose more identifiable cues, such as periorbital skin texture and iris details—leading to only a 70% protection rate, which is significantly lower than the 95% efficacy achieved by ROFI, as demonstrated in Figure 5 (C). In terms of iris recognition attacks, ROFI remains effective as it preserves disease-relevant infor- mation, such as overall iris shape and color, while obfuscating identity-sensitive iris details like pigment distribution. To evaluate iris privacy, we used the open-source iris recognition algorithm Open Iris . The original images (or those processed by eye cropping) had a 74.45% recognition success rate, highlighting the vulnerability of standard facial images to iris-based identification. In contrast, ROFI-protected images achieved only a 0.87% success rate, indicating that identity-related iris features are effectively obscured by our method. Second, regarding disease diagnosis, eye cropping removes surrounding facial features entirely, while ROFI preserves key facial structures. This enables the observation of clinically relevant facial manifestations, thereby supporting more comprehensive medical assessments. For instance, in patients with Bell’s palsy , our ROFI framework enables accurate detection of facial land- marks, which can be used to assess facial asymmetry—a key clinical indicator of the condition. Such diagnostically valuable features would be completely lost when using eye-only cropping strategies. Even in cases of ocular diseases, certain conditions such as squamous cell carcinoma (SCC) can present with symptoms extending beyond the eye region, potentially involving the forehead or cheeks . Certain specific full-face features, such as “hypothyroid facies” , are also helpful for diagnosing thyroid eye disease (TED). Our ROFI framework retains these extraocular signs, whereas eye cropping discards them entirely, potentially delaying timely medical follow-up. Finally, we mention that ROFI is not contradictory to eye cropping. Rather, ROFI is fully com- patible with subsequent eye cropping, offering ophthalmologists the flexibility to focus on the ocular region or adopt this combination as a safer and more informative strategy. ROFI had been demonstrated to be effective when integrated into medical AI models. These mod- els are adopted in the preliminary screening of diseases, significantly conserving physician resources while broadening the scope of early disease detection. Moreover, given that most AI models are deployed on remote cloud platforms, owing to their high computational demands and reliance on GPU clusters, there is an imperative need to protect the privacy of patient information contained 8 within transmitted images. Our findings indicated that AI diagnostic outcomes derived from ROFI- protected images exhibit substantial consistency (κ > 0.81) with those from the original images. These findings indicate that ROFI effectively anonymizing patient identity with minimal impact on the medical AI’s decision-making process, as evidenced by the high agreement rates. ROFI was reversible in terms of both the identification and appearance of patient facial images. For long-term patient care, such as monitoring chronic conditions like Thyroid Eye Disease, clini- cians must compare current images to a historical baseline. ROFI’s key-based reversal restores this crucial longitudinal link, which is lost in irreversible methods. Additionally, reversibility is essential for medical and legal auditing. It provides a secure “digital fingerprint” that ensures traceability and accountability, allowing auditors to verify that a diagnosis corresponds to the correct patient record, thus maintaining the integrity of clinical workflows. Traceability was critical for maintain- ing accurate medical records and auditing the clinical treatment procedure, aligning with the GCP standard [129–131]. Despite its reversibility, our method remained safe due to the confidentiality of both the protection and restoration software, which are distributed to partners only after signing a confidentiality agreement. Furthermore, even if the protection and decoder soft-wares are exposed to attackers, without the private key, the attacks can still not obtain the original image. The privacy key-based encryption scheme of the ROFI model is safe enough. First, the key is the 512-dimensional float vector with 32-bit precision, providing 2512×32 = 216384 possible combinations, which is computationally infeasible to brute-force. To empirically validate this, we conducted exper- iments where each encrypted sample from SNPH validation set was subjected to 1,000 brute-force attempts, showing a 0% success rate, confirming the robustness against collision attack. Further, our approach can be combined with timestamp-based password authentication protocols, such as kerberos protocol , to defense replay attacks. We also attempted to obtain a universal key capable of decrypting any encrypted image through gradient-based optimization, in an adversarial manner. Specifically, given the encrypted images gen- erated by ROFI, we input them into the decryption module of ROFI. We froze the parameters of the ROFI model and optimized only the key vector, with the objective of minimizing the difference between the decrypted image and the corresponding original image. With the optimized key, the protection rate of the decrypted data still remained quite high at 96.96%. ROFI is robust to adversial attack. We used the Basic Iterative Method (BIM) algorithm to generate adversarial noise, which was then added to the ROFI-protected images to perform the identity spoofing attack. The optimizing goal of the adversarial attack was to make the decrypted images resemble those of a spoofing identity. The iterative step is set to 15. Notably, even with a perturbation noise norm ε of 0.05, the attack success rate remains below 5%. To further enhance robustness, we implemented a defense strategy by adding random noise to the input during infer- ence . After applying this defense mechanism, the attack success rate dropped to zero across all tested ε values (Supplementary Table 11), demonstrating the reliability of our approach. These results confirm the robustness of our method, whether or not the defense strategy is applied. The implications of ROFI extend broadly across digital healthcare. It can accelerate the adoption of telemedicine by allowing patients to securely transmit clinically useful images without fear of identification, democratizing access to specialist care. For research, ROFI can help break down data silos by enabling the creation of large, multi-center, privacy-compliant datasets, which is critical for studying rare diseases and training robust AI models. Furthermore, by demonstrating that diagnostic models can be effectively trained on privacy-protected images, ROFI paves the way for developing trustworthy and ethical AI systems that can be safely deployed in cloud environments. Despite the aforementioned advantages and contributions of ROFI to the privacy-protected clin- ical diagnosis field, it inevitably has some limitations. First, although ROFI was evaluated in three cohorts, the data was from Asian cohorts. In the future, we will validate the clinical outcomes on indi- viduals from other racial cohorts. Second, ROFI is evaluated on facial images, which could be used to initially assess the external eye diseases such as ptosis. Future work will explore privacy exposure and protection issues associated with fundus photographs, which could be adopted in the diagnosis of other eye diseases such as the diabetic retinopathy. Third, real-world deployment of ROFI presents ethical and technological challenges that must be addressed. Ethically, the most significant hurdle is key management and governance. Robust policies are needed to define who can authorize image reversal and under what circumstances, supported by a secure Key Management System with strict audit trails. The informed consent process must also be transparent, clearly explaining to patients how their data is protected and the reversible nature of the process. Technologically, challenges include seamless integration with existing hospital IT systems like PACS and EMR, and the need 9 for sufficient computational resources (e.g., GPU clusters) to process image data at scale. Fourth, in future, we will also evaluate the effectiveness of the proposed ROFI technique on recently emerging large multi-modal medical models [135–141]. In summary, we have substantiated the effectiveness of the proposed ROFI technique across various applications, including clinical diagnosis, privacy protection, compatibility to medical AI models, and longitudinal diagnosis support. ROFI exemplifies a critical step towards harmonizing the dual goals of advancing medical diagnostics with remote medicine and safeguarding patient privacy. As we move into an era where digital medicine plays an increasingly pivotal role, technologies like ROFI will be indispensable in building a future where healthcare is both effective and ethically sound. People’s Hospital (Approval No.SH9H-2022-T380-1), Eye Center of Xiangya Hospital of Central South University (Approval No.202407131), and Renmin Hospital of Wuhan University (Approval No.WDRY2024-K238). All patients agreed to participate in the prospective study at the ROFI Pro- gram either by themselves or via their legal guidance. For the publication of identifiable images, the written informed consent was obtained from the parents or legal guardians. This study complies with the latest version of the Declaration of Helsinki . 4.1 Model Architecture of ROFI The ROFI framework is designed to achieve the dual goals of privacy and accuracy through a multi- stage process. First, an Ophthalmic Sign Detector, trained via weakly supervised learning, identifies and extracts clinically relevant sign features from the ocular regions. In parallel, a Transformer- based Neural Identity Translator alters the global facial features to obscure the patient’s identity, guided by a private key. The framework then intelligently integrates these two outputs by replacing the eye-region features in the anonymized feature map with the original, sign-preserving features. A refinement network (DA-Former) ensures a seamless transition between these regions before a final decoder, which reconstructs a high-quality, privacy-protected image.. The whole architecture of the ROFI model is outlined in Figure 6a, where all main components are implemented with neural networks and fully learnable. In the privacy protection procedure, ROFI employs the Ophthalmic Sign Detector and the Neural Identity Translator (Figure 6b), to simultaneously preserve the ophthalmic features and protect the patient privacy. The Ophthalmic Sign Detector identifies disease-related features within eyes in a weakly-supervised manner, which will be detailed in Section 4.2. The Transformer-based Neural Identity Translator network alters the identifying facial appearances such as face shape. Subsequently, the output features from the above two components are integrated, i.e., the ophthalmic features are used to substitute the transformed facial features within the detected ophthalmic sign regions. The integrated feature map is then enhanced through a quality enhancement network DA-Former (Figure 6c), amending the boundary defects caused by the feature substitution. The enhanced feature is then input into a decoding network Dec-Net (Figure 6d), producing the privacy-protected facial images. During the privacy recovery procedure, we employ a Transformer-based Privacy Recovery network to revert the anonymized face to its original form, using the same private key as in the protection procedure. The Neural Identity Translator network (Figure 6b) aims to alter the identity information within the facial image. The process begins with a feature extractor that transforms input image into the deep feature map, which consists of a series of convolution layers. The feature map is then flattened into a one-dimensional feature. The private key is a 512-dimensional vector, drawn from the Gaussian distribution with unit variance, and is pre-pended before the flattened feature. Then, the combined features are passed through six Transformer blocks , which leverage self-attention mechanism to transform the bio-identifying information within feature, effectively protecting the privacy. Finally, the output is unflattened back into the feature map. We conduct ablation studies on the number of transformer blocks within the Neural Identity Transformer. As shown in Supplementary Table 12, the ID protection rate increases with the number of Transformer blocks, from 93.27% using two blocks to 96.54% with six blocks. Further increasing the depth to eight or ten blocks brings only marginal improvement (96.61%), while significantly increasing computational cost and model size. 10 Therefore, we adopt the six-block configuration as it achieves a good trade-off between performance and efficiency. We further visualize the facial features before/after the neural identity translation procedure. As shown in Figure 7, the original feature map highlights discriminative identity features such as the mouth and nose contours. After protection, the facial features are largely reduced or eliminated, while the eye features remain well-preserved and prominently highlighted. The architecture of the Neural Identity Restorer is similar to that of the Neural Identity Translator (Figure 6b), with one difference: the input to the Neural Identity Restorer includes a learnable token that signals the network to operate in “Restoring mode”. The learnable token is fixed across all patients. When the same private key used in the privacy protection procedure is applied, the original facial information could be restored. Conversely, if an incorrect private key is used, such as a random key by an attacker, the network generates the facial information of a virtual face, thereby maintaining patient privacy. This is achieved by introducing the learning objectives in Section 4.3. The wrong virtual face is determined for the same original face. This prevents attackers from exploiting output variability to infer patterns of the original image. The DA-Former module (Figure 6c) aims to correct any artifacts caused by the stitching of the eye and facial features. The process begins with flattening the stitched feature map into a one- dimensional feature. The flattened feature is then passed through two Transformer blocks. After the transformations, the output is unflattened back into the feature map. The refinement network is guided by the GAN loss, which incorporates both local-wise and global-wise terms to guide the refinement process. The local-wise GAN loss guides the refined features to achieve a smooth transition between the eye and facial regions, while the global-wise loss enables that the refined information results in a realistic and coherent face image. The Dec-Net module (Figure 6d) aims to reconstruct high-quality facial images from the feature map enhanced by DA-Former. It consists of a series of residue blocks and upsample layers to gradually decode the high-resolution image from the low-resolution feature. Concretely, the Dec-Net architecture sequentially comprises four residual blocks, an up-sampling layer, two residual blocks, another up-sampling layer, one residual block, and a final up-sampling layer. Each residual block consists of two 3×3 convolutions with an intermediate Re LU activation function and skip connection. Batch normalization is not adopted to accurately preserve low-level features. 4.2 Ophthalmic Sign Detector The Ophthalmic Sign Detector is designed to detect ophthalmic sign features that are necessary for ophthalmic diagnosis (Figure 8). For images from the SNPH cohort’s developing set, physicians annotate each image’s health state. A neural network φ, which consists of an ophthalmic feature extractor followed by an ophthalmic sign scorer, scores each region within each image. The score of the region with the highest confidence score serves as the image-level disease confidence, supervised by an image-level binary label y. Specifically, y = 0 indicates a healthy state, while y = 1 denotes disease presence. This approach is termed the “region-score-max” learning strategy. Formally, given an input eye region image X ∈RH×W , φ assigns a sign score to each region of Xeye. The maximum abnormal score is used as the overall result for the entire image: S = φ(X) ∈R H 8 × W 8 , (1) p = max(S), Lclassify = −y · log(p) + (1 −y) · log(1 −p), where S represents the region-wise sign scores, and p is the highest-confidence region score. As for the network architecture of φ, ophthalmic feature extractor consists of the first three stages (Conv1, Res1, and Res2) of the Res Net50 architecture, while ophthalmic sign scorer is implemented as a linear layer followed by a Sigmoid function . Despite training on image-level annotations, the detector effectively identifies classification-relevant regions in a weakly supervised manner, aligning with previous studies . Regions with a sign score exceeding 0.5 are designated as ophthalmic sign regions. Then, the features of these regions are selected as the ophthalmic feature. The sign map threshold value 0.5 is determined by the empirical investigation. As shown in Table 1, higher cutoff values (e.g., 0.7 or 0.9) result in more aggressive suppression of facial fea- tures, leading to better de-identification performance (ID Protection Rate = 96.96%). However, such thresholds also risk removing diagnostically relevant regions, as evidenced by the noticeable drop 11 in classification AUROC ( dropped to only 86.19%). Conversely, lower cutoff values preserve more image content, which helps maintain high diagnostic accuracy (high AUROC), but leave identifiable features partially intact, reducing de-identification effectiveness. For example, the ID protection rate is only 92.04%, when the cutoff value is set to 0.1. Moreover, we visualize the sign maps to provide a more intuitive understanding of the oph- thalmic features learned by the model. As shown in Figure 9, the disease-related ophthalmic signs are successfully captured. Although some false-positive regions are detected, they lack specific identity information and therefore do not compromise the overall de-identification performance. Threshold Value 0.1 0.3 0.5 0.7 0.9 ID Protection Rate (%) 92.04 93.93 96.54 96.96 96.96 Classification AUROC (%) 91.34 91.34 91.25 88.26 86.19 Zhang, S. & Metaxas, D. On the challenges and perspectives of foundation models for medical image analysis. Medical image analysis 91, 102996 (2024). Singh, L. K., Garg, H. & Pooja. Automated glaucoma type iden- tification using machine learning or deep learning techniques. In Advancement of machine intelligence in interactive medical image analysis, 241–263 . Singh, L. K., Garg, H., Khanna, M., Bhadoria, R. S. et al. An analytical study on machine learning techniques. In Multidisciplinary functions of Blockchain technology in AI and Io T applications, 137–157 (IGI Global Scientific Publishing, 2021). Singh, L. K. & Khanna, M. Introduction to artificial intelligence and current trends. In Innovations in Artificial Intelligence and Human-Computer Interaction in the Digital Era, 31–66 . Singh, L. K., Pooja, Garg, H., Khanna, M. & Bhadoria, R. S. An enhanced deep image model for glaucoma diagnosis using feature-based detection in retinal fundus. Medical & Biological Engineering & Computing 59, 333–353 (2021). Singh, L. K., Pooja, Garg, H. & Khanna, M. Deep learning system applicability for rapid glaucoma prediction from fundus images across various data sets. Evolving Systems 13, 807– 836 (2022). Zhao, Y., Wang, X., Che, T., Bao, G. & Li, S. Multi-task deep learning for medical image computing and analysis: A review. Computers in Biology and Medicine 153, 106496 (2023). Kiryati, N. & Landau, Y. Dataset growth in medical image analysis research. Journal of imaging 7, 155 (2021). Wang, M., Qin, Y., Liu, J. & Li, W. Identifying personal physiological data risks to the internet of everything: the case of facial data breach risks. Humanities and Social Sciences Communications 10, 1–15 (2023). Ziller, A. et al. Reconciling privacy and accuracy in ai for medical imaging. Nature Machine Intelligence 6, 764–774 (2024). Holub, P. et al. Privacy risks of whole-slide image sharing in digital pathology. Nature Communications 14, 2577 (2023). Mittal, S. et al. On responsible machine learning datasets emphasizing fairness, privacy and regulatory norms with examples in biometrics and healthcare. Nature Machine Intelligence 6, 936–949 (2024). Heidt, A. Intellectual property and data privacy: The hidden risks of ai. Nature (2024). Ballhausen, H. et al. Privacy-friendly evaluation of patient data with secure multiparty computation in a european pilot study. npj Digital Medicine 7, 280 (2024). Wan, Z. et al. Sociotechnical safeguards for genomic data privacy. Nature Reviews Genetics 23, 429–445 (2022). Steeg, K. et al. Re-identification of anonymised mri head images with publicly available software: investigation of the current risk to patient privacy. e Clinical Medicine 78 (2024). Tian, Y. et al. Towards all-in-one medical image re-identification. In Proceedings of the Computer Vision and Pattern Recognition Conference, 30774–30786 (2025). 15 Muschelli, J. Recommendations for processing head ct data. Frontiers in neuroinformatics 13, 61 (2019). Moore, S. M. et al. De-identification of medical images with retention of scientific research value. Radiographics 35, 727–735 (2015). Newhauser, W. et al. Anonymization of dicom electronic medical records for radiation therapy. Computers in biology and medicine 53, 134–140 (2014). Robinson, J. D. Beyond the dicom header: additional issues in deidentification. American Journal of Roentgenology 203, W658–W664 (2014). Rodr ́ıguez Gonz ́alez, D., Carpenter, T., van Hemert, J. I. & Wardlaw, J. An open source toolkit for medical imaging de-identification. European radiology 20, 1896–1904 (2010). Tian, Y., Wang, S. & Zhai, G. Medical manifestation-aware de-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, 26363–26372 (2025). Tian, Y. et al. Semantics versus identity: A divide-and-conquer approach towards adjustable medical image de-identification. ar Xiv preprint ar Xiv:2507.21703 (2025). Newton, E. M., Sweeney, L. & Malin, B. Preserving privacy by de-identifying face images. IEEE transactions on Knowledge and Data Engineering 17, 232–243 (2005). Padilla-L ́opez, J. R., Chaaraoui, A. A. & Fl ́orez-Revuelta, F. Visual privacy protection methods: A survey. Expert Systems with Applications 42, 4177–4195 (2015). Singh, L. K., Khanna, M. & Garg, H. Multimodal biometric based on fusion of ridge features with minutiae features and face features. International Journal of Information System Modeling and Design (IJISMD) 11, 37–57 (2020). Yu, T., Teoh, A. P., Wang, C. & Bian, Q. Convenient or risky? investigat- ing the behavioral intention to use facial recognition payment in smart hospitals. Humanities and Social Sciences Communications 11, 1–20 (2024). Lee, J.-C., Bi, L. & Liu, H. User stickiness to facial recognition payment technol- ogy: insights from sako’s trust typology, privacy concerns, and a cross-cultural context. Humanities and Social Sciences Communications 11, 1–15 (2024). Khumdat, N., Phukpattaranont, P. & Tengtrisorn, S. Development of a computer system for strabismus screening. In The 6th 2013 Biomedical Engineering International Conference, 1–5 . Jung, S.-M., Umirzakova, S. & Whangbo, T.-K. Strabismus classification using face features. In 2019 International Symposium on Multimedia and Communication Technology (ISMAC), 1–4 . de Figueiredo, L. A., Dias, J. V. P., Polati, M., Carricondo, P. C. & Debert, I. Strabismus and artificial intelligence app: optimizing diagnostic and accuracy. Translational Vision Science & Technology 10, 22–22 (2021). Zheng, C. et al. Detection of referable horizontal strabismus in children’s primary gaze photographs using deep learning. Translational vision science & technology 10, 33–33 (2021). Reid, J. E. & Eaton, E. Artificial intelligence for pediatric ophthalmology. Current opinion in ophthalmology 30, 337–346 (2019). Leong, Y.-Y., Vasseneix, C., Finkelstein, M. T., Milea, D. & Najjar, R. P. Artificial intelligence meets neuro-ophthalmology. The Asia-Pacific Journal of Ophthalmology 11, 111–125 (2022). 16 Shu, Q. et al. Artificial intelligence for early detection of pediatric eye diseases using mobile photos. JAMA Network Open 7, e2425124–e2425124 (2024). Graham, P. Epidemiology of strabismus. The British journal of ophthalmology 58, 224 (1974). Wilson 2nd, F. Adverse external ocular effects of topical ophthalmic therapy: an epidemiologic, laboratory, and clinical study. Transactions of the American Ophthalmological Society 81, 854 (1983). Patel, S. J. & Lundy, D. C. Ocular manifestations of autoimmune disease. American family physician 66, 991–998 (2002). Zlatanovic, G., Veselinovic, D., Cekic, S., Zivkovic, M. & Zlatanovic, M. Ocular manifestation of rheumatoid arthritis-different forms and frequency. Bosnian journal of basic medical sciences 10, 323–327 (2010). Yang, Y., Chen, X. & Lin, H. Privacy preserving technology in ophthalmology. Current Opinion in Ophthalmology 35, 431–437 (2024). Gedraite, E. S. & Hadad, M. Investigation on the effect of a gaussian blur in image filtering and segmentation. In Proceedings ELMAR-2011, 393–396 . Battiato, S., Di Blasi, G., Farinella, G. M., Gallo, G. et al. A survey of digital mosaic techniques. In Eurographics Italian Chapter Conference, 129–135 . Deng, J., Guo, J., Xue, N. & Zafeiriou, S. Arcface: Addi- tive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 4690–4699 (2019). Zhang, X., Zhao, R., Qiao, Y., Wang, X. & Li, H. Adacos: Adaptively scaling cosine logits for effectively learning deep face representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10823–10832 (2019). Wang, H. et al. Cosface: Large margin cosine loss for deep face recogni- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, 5265–5274 (2018). Ding, C. & Tao, D. Trunk-branch ensemble convolutional neural networks for video-based face recognition. IEEE transactions on pattern analysis and machine intelligence 40, 1002–1014 (2017). Li, P., Prieto, L., Mery, D. & Flynn, P. J. On low-resolution face recognition in the wild: Comparisons and new techniques. IEEE Transactions on Information Forensics and Security 14, 2000–2012 (2019). Ahmed, A., Guo, J., Ali, F., Deeba, F. & Ahmed, A. Lbph based improved face recognition at low resolution. In 2018 international conference on Artificial Intelligence and big data (ICAIBD), 144–147 . Tripathy, S., Kannala, J. & Rahtu, E. Facegan: Facial attribute controllable reenactment gan. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 1329–1338 (2021). Shoshan, A., Bhonker, N., Kviatkovsky, I. & Medioni, G. Gan-control: Explicitly controllable gans. In Proceedings of the IEEE/CVF international conference on computer vision, 14083– 14093 (2021). 17 He, Z., Zuo, W., Kan, M., Shan, S. & Chen, X. Attgan: Facial attribute editing by only changing what you want. IEEE transactions on image processing 28, 5464–5478 (2019). Rombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684–10695 (2022). Podell, D. et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis. ar Xiv preprint ar Xiv:2307.01952 (2023). Kim, G., Kwon, T. & Ye, J. C. Diffusionclip: Text- guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2426–2435 (2022). Korshunova, I., Shi, W., Dambre, J. & Theis, L. Fast face-swap using convolutional neural networks. In Proceedings of the IEEE international conference on computer vision, 3677–3685 (2017). Chen, R., Chen, X., Ni, B. & Ge, Y. Simswap: An efficient framework for high fidelity face swap- ping. In Proceedings of the 28th ACM international conference on multimedia, 2003–2011 (2020). Zhu, Y., Li, Q., Wang, J., Xu, C.-Z. & Sun, Z. One shot face swapping on megapix- els. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 4834–4844 (2021). Xu, C. et al. Region-aware face swapping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7632–7641 (2022). Naruniec, J., Helminger, L., Schroers, C. & Weber, R. M. High-resolution neural face swapping for visual effects. In Computer Graphics Forum, vol. 39, 173–184 (Wiley Online Library, 2020). Chadha, A., Kumar, V., Kashyap, S. & Gupta, M. Deepfake: an overview. In Proceedings of second international conference on computing, communications, and cyber-security: IC4S 2020, 557–566 . Karras, T., Aila, T., Laine, S. & Lehtinen, J. Progressive growing of gans for improved quality, stability, and variation. ar Xiv preprint ar Xiv:1710.10196 (2017). Li, T. & Lin, L. Anonymousnet: Natural face de-identification with measurable privacy. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 0–0 (2019). Gu, X., Luo, W., Ryoo, M. S. & Lee, Y. J. Password-conditioned anonymization and deanonymization with face identity transformers. In European conference on computer vision, 727–743 . Li, D., Wang, W., Zhao, K., Dong, J. & Tan, T. Riddle: Reversible and diversified de-identification with latent encryptor. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8093–8102 (2023). Yang, H. et al. G2face: High-fidelity reversible face anonymization via generative and geometric priors. IEEE Transactions on Information Forensics and Security (2024). Cai, Z. et al. Disguise without disruption: Utility-preserving face de-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, 918–926 (2024). 18 Wen, Y., Liu, B., Cao, J., Xie, R. & Song, L. Divide and conquer: a two-step method for high quality face de-identification with model explainability. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5148–5157 (2023). Cao, J., Liu, B., Wen, Y., Xie, R. & Song, L. Personalized and invert- ible face de-identification by disentangled identity information manipulation. In Proceedings of the IEEE/CVF international conference on computer vision, 3334–3342 (2021). Yang, Y. et al. A digital mask to safeguard patient privacy. Nature Medicine 28, 1883–1892 (2022). Zhu, B., Zhang, C., Sui, Y. & Li, L. Facemotionpreserve: a generative approach for facial de-identification and medical information preservation. Scientific Reports 14, 17275 (2024). Paysan, P., Knothe, R., Amberg, B., Romdhani, S. & Vetter, T. A 3d face model for pose and illumination invariant face recognition. In 2009 sixth IEEE international conference on advanced video and signal based surveillance, 296–301 . Li, T., Bolkart, T., Black, M. J., Li, H. & Romero, J. Learning a model of facial shape and expression from 4d scans. ACM Trans. Graph. 36, 194–1 (2017). Feng, Y., Feng, H., Black, M. J. & Bolkart, T. Learning an animatable detailed 3d face model from in-the-wild images. ACM Transactions on Graphics (To G) 40, 1–13 (2021). Sun, Y., Wang, X. & Tang, X. Deep convolutional network cascade for facial point detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3476–3483 (2013). Rashid, M., Gu, X. & Jae Lee, Y. Interspecies knowledge transfer for facial keypoint detec- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 6894–6903 (2017). Marenco, M. et al. Clinical presentation and management of congenital ptosis. Clinical Ophthalmology 453–463 (2017). Seregard, S. Conjunctival melanoma. Survey of ophthalmology 42, 321–350 (1998). Marks, R. Squamous cell carcinoma. The Lancet 347, 735–738 (1996). Rothman, B., Leonard, J. C. & Vigoda, M. M. Future of electronic health records: implications for decision support. Mount Sinai Journal of Medicine: A Journal of Translational and Personalized Medicine 79, 757–768 (2012). Smith, T. Medical audit. BMJ: British Medical Journal 300, 65 (1990). Act, A. Health insurance portability and accountability act of 1996. Public law 104, 191 (1996). Hsu, K.-J., Lin, Y.-Y. & Chuang, Y.-Y. Weakly supervised salient object detection by learning a classifier-driven map generator. IEEE Transactions on Image Processing 28, 5435–5449 (2019). Lu, M. Y. et al. Data-efficient and weakly supervised computational pathology on whole-slide images. Nature biomedical engineering 5, 555–570 (2021). Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017). Bierer, B. E. Declaration of helsinki—revisions for the 21st century. JAMA (2024). 19 D ́ıaz-Manera, J., Luna, S. & Roig, C. Ocular ptosis: differential diagnosis and treatment. Current opinion in neurology 31, 618–627 (2018). Wong, J. R., Nanji, A. A., Galor, A. & Karp, C. L. Management of conjunctival malignant melanoma: a review and update. Expert review of ophthalmology 9, 185–204 (2014). Stannard, C., Sauerwein, W., Maree, G. & Lecuona, K. Radiotherapy for ocular tumours. Eye 27, 119–127 (2013). Azari, A. A. & Barney, N. P. Conjunctivitis: a systematic review of diagnosis and treatment. Jama 310, 1721–1730 (2013). Jones, L. T. An anatomical approach to problems of the eyelids and lacrimal apparatus. Archives of Ophthalmology 66, 111–124 (1961). Yin, V. T., Merritt, H. A., Sniegowski, M. & Esmaeli, B. Eyelid and ocular surface carcinoma: diagnosis and management. Clinics in dermatology 33, 159–169 (2015). Aslam, T. M., Tan, S. Z. & Dhillon, B. Iris recognition in the presence of ocular disease. Journal of The Royal Society Interface 6, 489–493 (2009). King, D. E. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research 10, 1755–1758 (2009). AI, G. Mediapipe face mesh. https://github.com/google-ai-edge/mediapipe/blob/master/ docs/solutions/face mesh.md. Accessed: 2024-07-18. Bernardes, R., Serranho, P. & Lobo, C. Digital ocular fundus imaging: a review. Ophthalmologica 226, 161–181 (2011). Zhang, K., Zhang, Z., Li, Z. & Qiao, Y. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE signal processing letters 23, 1499–1503 (2016). Bertels, J. et al. Optimizing the dice score and jaccard index for medical image segmentation: Theory and practice. In Medical Image Computing and Computer Assisted Intervention, 92– 100 . Roob, G., Fazekas, F. & Hartung, H.-P. Peripheral facial palsy: etiology, diagnosis and treatment. European neurology 41, 3–9 (1999). Li, B. et al. The performance of a deep learning system in assisting junior ophthal- mologists in diagnosing 13 major fundus diseases: a prospective multi-center clinical trial. NPJ digital medicine 7, 8 (2024). Tian, Y., Min, X., Zhai, G. & Gao, Z. Video-based early asd detection via temporal pyramid networks. In 2019 IEEE International Conference on Multimedia and Expo (ICME), 272–277 . Tian, Y. et al. A coding framework and benchmark towards low-bitrate video understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence 46, 5852–5872 (2024). Tian, Y., Yan, Y., Zhai, G., Guo, G. & Gao, Z. Ean: event adaptive network for enhanced action recognition. International Journal of Computer Vision 130, 2453–2471 (2022). Tian, Y., Yan, Y., Zhai, G., Chen, L. & Gao, Z. Clsa: A contrastive learning framework with selective aggregation for video rescaling. IEEE Transactions on Image Processing 32, 1300–1314 (2023). Tian, Y., Lu, G., Zhai, G. & Gao, Z. Non-semantics suppressed mask learning for unsupervised video semantic compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 13610–13622 20 (2023). Tian, Y., Lu, G. & Zhai, G. Free-vsc: Free semantics from visual foundation models for unsupervised video semantic compression. In European Conference on Computer Vision, 163– 183 . Tian, Y. et al. Self-conditioned probabilistic learning of video rescaling. In Proceedings of the IEEE/CVF international conference on computer vision, 4490–4499 (2021). Chen, Z. et al. Gaia: Rethinking action quality assessment for ai-generated videos. Advances in Neural Information Processing Systems 37, 40111–40144 (2024). Tian, Y. et al. Smc++: Masked learning of unsupervised video semantic compression. IEEE Transactions on Pattern Analysis and Machine Intelligence (2025). He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778 (2016). Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. ar Xiv preprint ar Xiv:2010.11929 (2020). Fan, L. Image pixelization with differential privacy. In IFIP Annual Conference on Data and Applications Security and Privacy, 148–162 . Xue, H. et al. Dp-image: Differential privacy for image data in feature space. ar Xiv preprint ar Xiv:2103.07073 (2021). Carlini, N. et al. Membership inference attacks from first principles. In 2022 IEEE symposium on security and privacy (SP), 1897–1914 . Nasr, M., Songi, S., Thakurta, A., Papernot, N. & Carlin, N. Adversary instantiation: Lower bounds for differentially private machine learning. In 2021 IEEE Symposium on security and privacy (SP), 866–882 . Worldcoin. Open-iris. https://github.com/worldcoin/open-iris (2024). Git Hub repository. Baugh, R. F. et al. Clinical practice guideline: Bell’s palsy. Otolaryngology–Head and Neck Surgery 149, S1–S27 (2013). Bernstein, S. C., Lim, K. K., Brodland, D. G. & Heidelberg, K. A. The many faces of squamous cell carcinoma. Dermatologic surgery 22, 243–254 (1996). Uday, S., Scott, B. & Alvi, S. Hashimoto’s hypothyroidism presenting with sufe (slipped upper femoral epiphysis). Case Reports 2014, bcr2013203095 (2014). Organization, W. H. et al. Handbook for good clinical research practice (gcp): guidance for implementation (2005). Kern, P. Medical treatment of echinococcosis under the guidance of good clinical practice (gcp/ich). Parasitology International 55, S273–S282 (2006). Grimes, D. A. et al. The good clinical practice guideline: a bronze standard for clinical research. The Lancet 366, 172–174 (2005). Pirzada, A. A. & Mc Donald, C. Kerberos assisted authentication in mobile ad-hoc networks. In ACM International Conference Proceeding Series, vol. 56, 41–46 (2004). 21 Kurakin, A., Goodfellow, I. J. & Bengio, S. Adversarial examples in the physical world. In Artificial intelligence safety and security, 99–112 (Chapman and Hall/CRC, 2018). Cohen, J., Rosenfeld, E. & Kolter, Z. Certified adversarial robustness via randomized smoothing. In international conference on machine learning, 1310–1320 . Team, G. et al. Gemma: Open models based on gemini research and technology. ar Xiv preprint ar Xiv:2403.08295 (2024). Zhang, Z. et al. Aibench: Towards trustworthy evaluation under the 45° law. https://aiben.ch/ (2025). Zhang, Z. et al. Large multimodal models evaluation: A survey. https://github.com/aiben-ch/ LMM-Evaluation-Survey (2025). Project Page: AIBench, available online. Chen, Z. et al. Can large models fool the eye? a new turing test for biological animation. ar Xiv preprint ar Xiv:2508.06072 (2025). Ji, K. et al. Medomni-45 {\deg}: A safety-performance benchmark for reasoning-oriented llms in medicine. ar Xiv preprint ar Xiv:2508.16213 (2025). Li, C. et al. Information density principle for mllm benchmarks. ar Xiv preprint ar Xiv:2503.10079 (2025). Li, C. et al. Image quality assessment: From human to machine preference. In Proceedings of the Computer Vision and Pattern Recognition Conference, 7570–7581 (2025). Narayan, S. The generalized sigmoid activation function: Competitive supervised learning. Information sciences 99, 69–82 (1997). Liu, W. et al. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 212–220 (2017). Zhang, R., Isola, P., Efros, A. A., Shechtman, E. & Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586–595 (2018). Goodfellow, I. et al. Generative adversarial networks. Communications of the ACM 63, 139– 144 (2020). Isola, P., Zhu, J.-Y., Zhou, T. & Efros, A. A. Image-to- image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1125–1134 (2017). Miyato, T., Kataoka, T., Koyama, M. & Yoshida, Y. Spectral normalization for generative adversarial networks. ar Xiv preprint ar Xiv:1802.05957 (2018). Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B. & Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017). Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. ar Xiv preprint ar Xiv:1412.6980 (2014). Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). 22 Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. ar Xiv preprint ar Xiv:1711.05101 (2017). Takahashi, R., Matsubara, T. & Uehara, K. Data augmentation using random image cropping and patching for deep cnns. IEEE Transactions on Circuits and Systems for Video Technology 30, 2917–2931 (2019). Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248–255 . Dettori, J. R. & Norvell, D. C. Kappa and beyond: is there agreement? Global spine journal 10, 499–501 (2020). Pembury Smith, M. Q. & Ruxton, G. D. Effective use of the mcnemar test. Behavioral Ecology and Sociobiology 74, 1–9 (2020). Chicco, D. & Jurman, G. The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation. BMC genomics 21, 1–13 (2020). 23 a) Protected Face Physician Patient Face Success (k>0.81) Ophthalmic Sign Eye Signs Detector Medical AI DA-Former Success (k>0.81) ? Virtual Neural Identity Protector Face Face Identifier Failed (Acc:3.46%) ROFI Private Key b) TED Historic Image Medical Records Reversed Protected Success (k>0.81) Face Face Diagnosis: TED Treatment Patient ID: 3512 ? Efficacy Evaluation Treat: Hormone Medical Audits Search Neural Identity Restorer c) Development Cohort External Validation Cohort SNPH (N=17181) ECXHCSU (N=493) WURH (N=79) Inclusion Criteria 1. Images of good visual quality. 2. Images showing clear disease-related manifestations. SNPH Cohort(N=12289 ) External Cohorts (N=294) Developing Set (N=11836) Model Selection Set (N=246) SNPH Test Set (N=231 ) ECXHCSU Test WURH Test Set (N=222 ) Set (N=48) c) a) Conjunctival Disorder (Conj-Dis) Keypoints Keypoints b) Eyelid Error, SNPH Iris Error, SNPH 50 40 Eyelid Disorder (Eyelid-Dis) Normalized Error (%) Normalized Error (%) 30 Detection Detection 20 Image (LA-Dis) 10 ? 0 G2Face Ours Mosaic AIGC Face Swap Digital Mask G2Face Ours Iris Disorder (Iris-Dis) Eye Sign Evaluation Consistency Eye Sign Evaluation Analysis d) Eye Sign Consistency, SNPH Eye Sign Consistency, ECXHCSU Eye Sign Consistency, WUPH Eyelid-Dis Conj-Dis Eyelid-Dis Conj-Dis Eyelid-Dis Conj-Dis Mosaic AIGC Face Swap 0.6 0.81 1.0 0.6 0.81 1.0 0.6 0.81 1.0 Digital Mask G2Face Ours LA-Dis Iris-Dis LA-Dis Iris-Dis LA-Dis Iris-Dis Conj-Dis Eyelid-Dis LA-Dis Iris-Dis < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 e) P-Value Conj-Dis Eyelid-Dis LA-Dis Iris-Dis Ours-Mosaic < 0.001 < 0.001 < 0.001 < 0.001 Ours-AIGC < 0.001 < 0.001 < 0.001 < 0.001 Ours-Face Swap < 0.001 0.0350 < 0.001 < 0.001 Ours-Digital Mask < 0.001 < 0.001 < 0.001 < 0.001 Ours-G2Face < 0.001 0.0197 < 0.001 < 0.001 Conj-Dis Eyelid-Dis LA-Dis Iris-Dis < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 Original Face Swap Digital Mask G2Face Ours Mosaic AIGC Conj-Dis: No/No Eyelid-Dis: Yes/No Conj-Dis: No/No Eyelid-Dis: No/No Conj-Dis: No/No Eyelid-Dis: No/No Conj-Dis: Yes/No Eyelid-Dis: No/No Conj-Dis: No/No Eyelid-Dis: No/No Conj-Dis: No/No Eyelid-Dis: Yes/No Conj-Dis: No/No Eyelid-Dis: Yes/No LA-Dis: No/No Iris-Dis: Yes/No LA-Dis:No/No Iris-Dis: No/No LA-Dis:No/No Iris-Dis: No/No LA-Dis:Yes/No Iris-Dis: Yes/No LA-Dis: No/No Iris-Dis: No/No LA-Dis: Yes/No Iris-Dis: Yes/No LA-Dis: No/No Iris-Dis: Yes/No a) Physicians Physicians d Majority Voting Statistician Majority Voting Confusion Matrix of Face Swap, WUPH 5 1 1 0 12 Protected 10 0 10 0 1 8 Remotely 0 0 3 0 0 0 2 3 14 0 ? 4 2 With Disease, 1 2 0 0 Yes or No? Outcome Analysis b) Confusion Matrix of Digital Mask, WUPH Yes-or-No, SNPH Yes-or-No, ECXHCSU Yes-or-No, WUPH Normal BCC Eo E Ptosis TED Actual 16 3 2 1 1 1 P < 0.001 P < 0.001 P < 0.001 14 1.0 1.0 1.0 12 4 2 5 0 1 0 0 1 0 2 8 0.8 0.8 0.8 6 2 0 0 17 0 4 2 0 0 0 0 6 0.6 0.6 0.6 G2Face Ours Mosaic AIGC Face Swap G2Face Ours Mosaic AIGC Face Swap G2Face Ours Confusion Matrix of G2Face, WUPH 6 0 1 1 0 8 1 6 2 0 3 6 0 0 3 0 0 c) 4 Clinical Consistency, ECXHCSU Clinical Consistency, WUPH Clinical Consistency, SNPH 3 3 2 10 1 2 0 0 0 0.81 0.81 0 0.81 Normal BCC Eo E Ptosis TED Predicted CL AIGC 0.4 0.4 Eo E 0.4 Face Swap Confusion Matrix of Ours, WUPH Normal MN Normal Normal Normal BCC Eo E Ptosis TED Actual 17.5 8 0 0 0 0 15.0 Digital Mask SCC 0 11 0 0 1 12.5 10.0 0 0 2 0 G2Face TED 7.5 Ptosis Ours 0 0 0 19 5.0 TED Microblepharon Ptosis Ptosis 2.5 0 0 0 0 6 0.0 Normal BCC Eo E Ptosis TED Predicted Ours κ=0.9376 (95% CI: 0.9033-0.9719) Ours κ=0.9621 (95% CI: 0.9346-0.9896) Ours κ=0.9433 (95% CI: 0.8674-1.0192) b) a) Vi T Consistency Res Net50 Consistency Res Net50 Consistency Vi T Consistency ECXHCSU SNPH ECXHCSU SNPH 1.0 1.0 P < 0.001 1.0 P < 0.001 1.0 P < 0.001 P = 0.002 0.8 0.8 0.8 0.8 0.6 0.6 0.6 0.6 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.0 Mosaic AIGC Face Swap Digital Mask G2Face Ours Mosaic AIGC Face Swap Digital Mask G2Face Ours Mosaic AIGC Face Swap Digital Mask G2Face Ours Mosaic AIGC Face Swap Digital Mask G2Face Ours c) Res Net50 AUROC Res Net50 AUROC Vi T AUROC Vi T AUROC SNPH ECXHCSU SNPH ECXHCSU 0.9 1.0 1.0 1.0 P < 0.001 P = 0.037 P = 0.010 P < 0.001 0.9 0.8 0.9 0.9 0.8 AUROC AUROC AUROC AUROC 0.8 0.7 0.8 0.7 0.7 0.6 0.6 0.7 0.6 0.5 0.5 Mosaic AIGC Face Swap Digital Mask G2Face Ours Original Mosaic AIGC Face Swap Digital Mask G2Face Ours Original Mosaic AIGC Face Swap Digital Mask G2Face Ours Original Mosaic AIGC Face Swap Digital Mask G2Face Ours Original d) Vi T, SNPH (Original) Vi T, SNPH (Face Swap) Vi T, SNPH (G2Face) Vi T, SNPH (Ours) 1.0 1.0 1.0 1.0 0.8 0.8 0.8 0.8 Sensitivity Sensitivity Sensitivity Sensitivity 0.6 0.6 0.6 0.6 BCC, AUROC=0.925 CM, AUROC=0.882 CL, AUROC=0.962 Normal, AUROC=1.000 Ptosis, AUROC=0.976 SCC, AUROC=0.867 Strabismus, AUROC=0.720 TAO, AUROC=0.950 BCC, AUROC=0.908 CM, AUROC=0.769 CL, AUROC=0.699 Normal, AUROC=0.994 Ptosis, AUROC=0.816 SCC, AUROC=0.658 Strabismus, AUROC=0.737 TAO, AUROC=0.934 BCC, AUROC=0.904 CM, AUROC=0.618 CL, AUROC=0.790 Normal, AUROC=1.000 Ptosis, AUROC=0.976 SCC, AUROC=0.751 Strabismus, AUROC=0.801 TAO, AUROC=0.962 BCC, AUROC=0.950 CM, AUROC=0.866 CL, AUROC=0.957 Normal, AUROC=1.000 Ptosis, AUROC=0.978 SCC, AUROC=0.839 Strabismus, AUROC=0.769 TAO, AUROC=0.966 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0.0 0.0 0.0 0.0 0.0 0.5 1.0 Specificity 0.0 0.5 1.0 Specificity 0.0 0.5 1.0 Specificity 0.0 0.5 1.0 Specificity c) a) Protected b) SNPH, ID Protection (Arc Face) Patient Database 96.54 SNPH, ID Protection (Ada Cos) SNPH, ID Protection (Arc Face) 100 100 94.81 95.24 94.81 94.81 100 94.81 93.51 93.08 91.78 90.91 87.88 90 1-ID Accuracy (%) 1-ID Accuracy (%) 90 68.40 74.03 1-ID Accuracy (%) 60 80 80 74.90 40 70 63.64 20 60 0 Mosaic AIGC Face Swap Digital Mask Face De ID Ours Mosaic AIGC Face Swap Digital Mask Face De ID Ours Original Left Eye Right Eye Ours Visual Similarity d) e) f) SNPH, Reverse, SNPH, Treatment Efficacy SNPH, Reverse, Visual Similarity ID Similarity Ours- Reverse (99%) Original G2Face- Reverse (90%) 97.19 98.47 100 1.00 74.72 80 93.48 95 0.75 0.50 90 40 0.25 85 20 0.00 0 80 G2Face Ours G2Face Ours Single G2Face Ours