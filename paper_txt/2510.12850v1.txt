Developing AI systems capable of nuanced ethical reasoning is critical as they increasingly influence human decisions, yet existing models often rely on superficial correlations rather than principled moral understanding. This paper introduces Ethic-BERT, a BERT-based model for ethical content classification across four domains: Commonsense, Justice, Virtue, and Deontology. Lever- aging the ETHICS dataset, our approach integrates robust preprocessing to address vocabulary sparsity and contextual ambiguities, alongside advanced fine- tuning strategies like full model unfreezing, gradient accumulation, and adaptive learning rate scheduling. To evaluate robustness, we employ an adversarially fil- tered ’Hard Test’ split, isolating complex ethical dilemmas. Experimental results demonstrate Ethic-BERT’s superiority over baseline models, achieving 82.32% average accuracy on the standard test, with notable improvements in Justice and Virtue. In addition, the proposed Ethic-BERT attains 15.28% average accuracy improvement in the Hard Test. These findings contribute to performance improve- ment and reliable decision-making using bias-aware preprocessing and proposed enhanced AI model. Keywords: Ethical and non-ethical content classification, BERT, Deep learning, Ethical AI reasoning. 1 creating technology that is both responsible and aligned with societal values. As AI systems play an increasingly prominent role in mediating human interactions and making autonomous decisions, it is crucial to ensure they operate within the bounds of ethical principles . However, encoding the complexity of human moral reasoning into AI systems poses significant challenges, requiring innovative approaches to bridge the gap between human values and computational frameworks . Ethical morality involves the principles of right and wrong that guide human behav- ior, encompassing dimensions such as justice, fairness, well-being, duties, and virtues. These principles are deeply interconnected, often leading to conflicts that require nuanced decision-making. Humans rely on cultural, social, and personal contexts to navigate moral ambiguities, but replicating this capacity in AI systems demands sophisticated techniques . The integration of ethical reasoning into AI is par- ticularly important because of its potential societal impact . AI systems, if left unchecked, can amplify biases, produce harmful outputs, or make decisions that con- flict with shared human values . To address these issues, researchers have turned to text-based scenarios as a means of evaluating AI systems’ ability to understand and apply ethical reasoning. These text based scenarios allows for the representation of complex moral dilemmas, providing a practical medium for assessing how AI aligns with human ethical judgment. Recent advancements in NLP, particularly the development of transformer archi- tectures, have made it possible to achieve significant progress in understanding context and intent in textual data. Datasets like ETHICS leverage these advancements, presenting scenarios derived from philosophical theories, including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. These benchmarks chal- lenge AI systems to move beyond simple pattern recognition and address the moral complexity inherent in real-world scenarios. Despite these advancements, progress in embedding ethical reasoning into AI has been limited . Models trained on ETHICS dataset have shown some promise but struggle with nuanced scenarios and adversarial examples. The key challenges of achieving better results in ethical reasoning tasks include: • Lack of high-quality datasets that reduce ambiguity and enhance representativeness. • Existing models struggle with nuanced ethical reasoning, limiting accuracy in moral decision-making. • AI models rely on spurious correlations rather than deep moral reasoning, leading to misclassifications in complex ethical scenarios. • The dataset primarily reflects Western moral perspectives, reducing its applicability to diverse cultural and ethical viewpoints. In this research, we address these key challenges by the following key contributions: fine-tuning techniques to strengthen ethical reasoning capabilities. 2 and adversarially filtered test sets. textual scenarios, improving data quality. By advancing the interplay between ethical reasoning and AI, our work lays the groundwork for systems that are more aligned with human values and equipped to handle the complexities of real-world moral dilemmas. ethical content classification to manage misinformation, hate speech, and inappro- priate material. Recent research efforts have focused on developing robust detection techniques using machine learning (ML) and deep learning (DL) models to improve accuracy, efficiency, and adaptability. At the same time, the ethical and moral impli- cations of content have also become crucial, requiring models capable of analyzing not only spam content but also ethically unacceptable text. This review explores signifi- cant contributions in this field, focusing on advancements in detecting and mitigating harmful content while preserving contextual integrity. In the domain of explicit content detection, Bhatti et al. proposed an Explicit Content Detection (ECD) system targeting NSFW content using a residual network- based deep learning model. Their approach, integrating YCb Cr color space and skin tone detection, achieved 95% accuracy in classifying explicit images and videos. Sim- ilarly, Khandekar et al. focused on NLP techniques for detecting unethical and offensive text, leveraging LSTM and Bi LSTM networks, which outperformed tradi- tional models with an accuracy of 86.4%. Horne et al. discussed the ethical challenges in automated fake news detection, emphasizing algorithmic bias and lack of generalizability. Their analysis of 381,000 news articles revealed the limitations of detection models that overfit benchmark datasets. Kiritchenko and Nejadgholi introduced an “Ethics by Design” framework for abusive content detection, highlight- ing fairness, explainability, and bias mitigation. Their two-step process categorized identity-related content before assessing severity, reinforcing the importance of ethi- cal considerations in content moderation. Schramowski et al. examined the moral biases embedded in large pre-trained models like BERT, demonstrating how these biases could be leveraged to steer text generation away from toxicity. They identified a “moral direction” within the embedding space, which could be used to rate the nor- mativity of text. This aligns with the ethical text assessment aspect of our research. Mittal et al. conducted a comparative study on deep learning models for hate speech detection, concluding that fine-tuned Ro BERTa models outperformed CNNs and Bi LSTMs in a ternary classification system. Mnassri et al. explored a multi- task learning framework that integrated emotional features with hate speech detection using BERT and m BERT. Their approach improved performance by leveraging shared representations across tasks, reducing overfitting and false positives. Saleh et al. investigated the effectiveness of domain-specific word embeddings in hate speech detec- tion, concluding that while specialized embeddings enhanced detection of coded hate 3 speech, pre-trained BERT models achieved the highest F1-score with 96%. Jim et al. review advancements in sentiment analysis, highlighting machine learning, deep learning, and large language models. They explore applications, datasets, challenges, and future research directions to enhance performance. Sultan et al. analyzed shallow and deep learning techniques for cyberbullying detection across social media platforms. Their study, comparing six shallow learning algorithms with three deep models, found that Bi LSTM models achieved the best recall and accuracy. This underscores the challenges in identifying masked or subtle offensive content, emphasizing the need for sophisticated models. Wadud et al. examine methods for offensive text classification, emphasizing the need for improved multilingual detection. They introduce Deep-BERT, a model combining CNN and BERT, which enhances accuracy in identifying offensive content across different lan- guages. Also, spam detection has been widely explored using traditional ML models and DL approaches. Similarly, Maqsood et al. proposed a hybrid approach com- bining Random Forest, Multinomial Naive Bayes, and SVM with CNNs, observing that SVM outperformed other traditional ML models, while CNNs excelled on larger datasets. Guo et al. introduced a BERT-based spam detection framework, inte- grating classifiers such as Logistic Regression, Random Forest, K-Nearest Neighbors, and SVM. Their results, using datasets like UCI’s Spambase and the Kaggle Spam Filter Dataset, demonstrated that BERT significantly improved spam classification, achieving a precision of 97.86% and an F1-score of 97.84%. Meanwhile, Labonne and Moran explored Large Language Models (LLMs) in spam detection, developing Spam-T5, a fine-tuned version of Flan-T5. Their work showed that Spam-T5 per- formed exceptionally well in low-data settings, surpassing both traditional ML models and modern LLMs like BERT. Chakraborty et al. leveraged a fine-tuned BERT model with interval Type-2 fuzzy logic for sentiment classification, achieving superior performance in handling contextual variations. Similarly, Zhang et al. integrated BERT with large LLMs in a hybrid approach, improving sentiment intensity predic- tion and aspect extraction. In the domain of ethical content detection, Aziz et al. applied BERT with multi-layered graph convolutional networks to identify sentiment triplets, highlighting the model’s capability in detecting hate speech and ethically sensitive content. These studies reinforce the adaptability of transformer-based archi- tectures in capturing complex linguistic patterns and moral nuances, making them well-suited for ethical content classification. Hendrycks et al. introduced the ETHICS dataset to evaluate AI models’ ability to reason about morality across different ethical frameworks, including justice, virtue ethics, deontology, utilitarianism, and commonsense morality. Their findings indicate that pre-trained LLMs like BERT and Ro BERTa show only partial success in making ethical decisions and often fail to handle complex moral scenarios accurately. Even advanced models like Ro BERTa-large and ALBERT-xxlarge demonstrated low accuracy, particularly on adversarial test cases, highlighting their limitations in gen- eralizing ethical principles. A key issue with the dataset is that the utilitarianism subset lacks explicit labels, requiring models to infer relative rankings rather than performing direct classification. Additionally, the study relied on standard fine-tuning techniques, but accuracy could likely improve with more extensive fine-tuning and 4 domain-specific training. These limitations suggest that current models still struggle to integrate ethical reasoning effectively. Pre-trained LLMs have proven highly effective in understanding complex language and context for tasks like spam detection, hate speech recognition, offensive language identification, sentiment analysis, and other forms of harmful content detection. Their adaptability and precision make them well-suited for content moderation. Building on their success, this study applies pre-trained LLMs to ethical content classification, aiming for a more reliable, context-aware, and fair moderation system. soning using machine learning techniques. It includes details about the dataset, data preprocessing, and implementation of our machine learning pipeline. Additionally, we elaborate on the fine-tuning process, showcasing the innovations that adapt the pre-trained BERT model for the task of ethical reasoning analysis, as illustrated in 3.3.2 Tokenization Using Word Piece Word Piece tokenization improves model training by handling unseen words, reduc- ing vocabulary size, and enhancing embedding stability. It splits words into subwords based on frequency, preventing excessive fragmentation while maintaining meaningful representations. This helps models generalize better to rare and new words, making training more efficient and robust. In this process the input text was tokenized dynami- cally using BERT’s Word Piece algorithm . Each tokenized word w was decomposed into subword tokens. In Equation 2, V is BERT’s fixed vocabulary. Instead of relying on standard segmentation, we employed frequency-aware tokenization, ensuring sub- words were split efficiently based on their corpus occurrence. In Equation 3 P(T | w) denotes the probability of a subword sequence given a word. This prevented excessive fragmentation of rare words and improved embedding stability. During training, this adjustment helped the model generalize better to unseen words . Tw = {t1, t2, . . . , tn}, ti ∈V (2) T ′ w = arg max T P(T | w) (3) 3.3.3 Truncation and Padding Optimization Since BERT requires a fixed sequence length L, we dynamically truncated or padded input sequences during training. Padding was applied only when necessary. In Equation 4, a sequence S′ is shorter than L, padding tokens ([PAD]) are appended. The exponent notation (L −|S′|) represents the number of padding tokens added to match the fixed length L. For example, if S′ has 8 tokens but L = 12, then 4 [PAD] tokens are appended. To prevent overfitting due to excessive padding, we implemented batch-wise dynamic padding, which ensured that the sequence length L was adjusted based on the longest sequence in each batch. This minimized redundant [PAD] tokens, leading to faster training and reduced computational overhead . S′ = S′ + [PAD](L−|S′|) (4) 3.4 Selecting a BERT-Based Cased Model When selecting a model for AI ethical reasoning tasks, the choice must ensure accurate interpretation and context retention. A BERT-based cased model proposed by Devlin et al. , is particularly effective due to its ability to preserve case distinctions, which are often vital in formal and ethical text analysis. This ensures that proper nouns, legal terms, and acronyms retain their intended meanings, reducing ambiguity in ethical and policy analysis . Research highlights the importance of case sensitivity in legal and ethical texts, as it helps differentiate between terms like “Title IX” and “title ix” or “US” and “us,” preventing misinterpretation. Case-sensitive models also enhance bias detection and policy evaluation by preserving textual integrity . By leveraging this approach, we improve the accuracy and reliability of our ethical assessments. 7 3.5 Implementation Details Our implementation is centered around a fine-tuned BERT-based cased model, chosen for its strong contextual understanding and adaptability to text classification tasks. In the following, we detail the architecture, training process, and fine-tuning innovations, along with the mathematical formulations underpinning these methods illustrated in Fig. 2: Fine tuning of BERT for ethical reasoning θ(t+1) = θ(t) − η √ˆvt + ε ˆmt (7) Equation 7, ˆmt and ˆvt are bias-corrected estimates of the first and second moments of gradients, and ε is a small constant for numerical stability. 3.5.3 Fine-Tuning Innovations To maximize the model’s adaptability to the ethical reasoning task, we implemented the following innovations: Full Fine-Tuning: All layers of the BERT model were unfrozen, allowing param- eter adjustments across the entire network . From Equation 8, the loss gradient ∇θL was backpropagated through all layers where, L is the number of transformer layers. Fully fine-tuning in ethical classification tasks helps the model grasp domain- specific ethical nuances, leading to more precise and fair decisions. It refines the model’s understanding beyond general pre-trained knowledge, aligning it with ethical guidelines. This approach minimizes bias, enhances reliability, and ensures responsible decision-making. L Y ∂L ∂θi = ∂L ∂Hj ∂Hj−1 · ∂Hi ∂HL · ∂θi , i = 1, 2, . . . , L (8) j=i+1 Gradient Accumulation: To address memory constraints, gradient accumula- tion was employed. Gradients g(b) for each mini-batch b were accumulated over Nacc steps . Equation 9, gradients ∇L(b) from each mini-batch b are summed over Nacc steps. This method allows training with small mini-batches while effectively simulating a larger batch size. Equation 10 updates the model parameters θ after accumulat- ing gradients over multiple steps. The learning rate η scales the average accumulated gradient, ensuring stable optimization. It ensures better representation of ethical con- siderations in data while maintaining computational feasibility. The approach helps to mitigate biases and enhances fairness by enabling effective learning from smaller yet diverse datasets. Nacc X b=1 ∇θL(b) (9) gacc = θ(t+1) = θ(t) −η gacc Nacc (10) 9 Adaptive Learning Rate: An adaptive learning rate schedule was used, reducing the learning rate as training progressed . In Equation 11, η0 is the initial learning rate, and t is the training step. Applying an adaptive learning rate in model training dynamically adjusts the step size based on gradient variations, improving convergence speed and stability. This technique helps prevent overshooting in high-gradient regions while accelerating learning in flatter areas, leading to more efficient optimization. In ethical classification tasks, adaptive learning rates enhance fairness and robustness by ensuring balanced learning across diverse and sensitive data distributions. ηt = η0 · 1 √ t (11) 3.5.4 Regularization and Robustness Dropout regularization was applied in the classification head to mitigate overfitting. During training , activations Htask in the classification head were stochastically zeroed out. In Equation 12, D ∼Bernoulli(1 −p) is a dropout mask, ⊙represents element-wise multiplication, and p is the dropout rate. At inference time, activations were scaled by (1−p) to maintain consistent output expectations shown in Equation 13. Regularization techniques, dropout, and batch normalization, help prevent overfitting by constraining model complexity. These methods ensure that the model generalizes well to unseen ethical scenarios, reducing biases and improving fairness. In ethical classification tasks, regularization enhances robustness by making the model resilient to noisy or imbalanced data, leading to more reliable and ethically sound decisions. H′ task = D ⊙Htask (12) Hinference task = (1 −p) · Htask (13) 3.6 Evaluation Matrix The model’s performance was evaluated using accuracy, precision, recall, F1-score , and AUC ensuring robust validation of ethical reasoning capabilities. and Hard Test Split datasets, along with a comparison to existing models such as Ro BERTa-large and ALBERT-xxlarge. The results demonstrate the impact of our innovative fine-tuning and preprocessing techniques in delivering strong performance, particularly in specific ethical reasoning domains. 4.1 Performance on Test Split Table 3 shows the performance of the proposed BERT model on the Test Split. The model achieved high Accuracy in Commonsense, Justice, Virtue domains and Deontol- ogy, reaching 86.46%, 78.22%, 83.40%, and 81.23% respectively. These results highlight 10 the model’s ability to effectively adapt to the task in these domains. The AUC val- ues for domains—90.78, 87.36, 88.78, 89.93—further affirm the model’s capability to separate positive and negative classes accurately. The confusion matrices for the Test dataset are presented in The subfigures 4a, 4b, 4c, and 4d correspond to the Common Sense, Justice, Virtue, and Deontology frameworks, respectively, highlighting differences in model perfor- mance across ethical reasoning approaches. The Figure 5 illustrate model performance over five epochs, highlighting key trends. Training loss consistently decreases, while accuracy improves, indicating effective learning. Some models maintain stable valida- tion accuracy, suggesting good generalization. In some cases, training and validation loss patterns differ, which may indicate areas for refinement. Adjustments like regular- ization could improve performance. Overall, the models demonstrate effective learning, with some showing stronger generalization. (a) Common Sense (b) Justice (c) Virtue 14 (d) Deontology Fig. 5: Training vs validation loss and accuracy curves on training dataset. existing models. In the Hard Test Split, the model showcased its robustness, achieving improvements over baseline approaches in more challenging scenarios. Our approach introduced key innovations, including comprehensive fine-tuning by unfreezing all lay- ers of the BERT model, implementing gradient accumulation, and utilizing advanced tokenization and data augmentation. These techniques allowed the model to effectively combine its pretrained knowledge with task-specific adaptations, resulting in supe- rior performance. Despite these successes, challenges persist in the Commonsense and Deontology domains, especially on the Hard Test Split. Addressing these gaps could involve enriching the training data with more contextually diverse examples, incorpo- rating external knowledge sources, or adopting domain-specific pretraining strategies. In general, this work highlights the potential of fine-tuned transformer models to tackle complex reasoning tasks in AI ethics. The findings underscore the importance of thoughtful preprocessing and training techniques in improving the robustness and generalization of the model. Acknowledgments The authors would like to express their sincere gratitude to the Ubiquitous, Cloud, and Human-Computer Interaction (UCH) Research Group, Department of Computer Science, American International University-Bangladesh for supporting this research.