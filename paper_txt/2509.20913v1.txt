Objectives: To develop a deep learning framework to evaluate if and how incor- porating micro-level mobility features, alongside historical crime and sociodemo- graphic data, enhances predictive performance in crime forecasting at fine-grained spatial and temporal resolutions. Methods: We advance the literature on computational methods and crime fore- casting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los Angeles, and Philadelphia). We employ crime incident data obtained from each city’s police de- partment, combined with sociodemographic data from the American Community Survey and human mobility data from Advan, collected from 2019 to 2023. This data is aggregated into grids with equally sized cells of 0.077 sq. miles (0.2 sq. kms) and used to train our deep learning forecasting model, a Convolutional Long Short- Term Memory (Conv LSTM) network, which predicts crime occurrences 12 hours ahead using 14-day and 2-day input sequences. We also compare its performance against three baseline models: logistic regression, random forest, and standard LSTM. Results: Incorporating mobility features improves predictive performance, espe- cially when using shorter input sequences. Noteworthy, however, the best results are obtained when both mobility and sociodemographic features are used together, with our deep learning model achieving the highest recall, precision, and F1 score in all four cities, outperforming alternative methods. With this configuration, longer input sequences enhance predictions for violent crimes, while shorter sequences are more effective for property crimes. Conclusion: These findings underscore the importance of integrating diverse data sources for spatiotemporal crime forecasting, mobility included. They also high- light the advantages (and limits) of deep learning when dealing with fine-grained spatial and temporal scales. Keywords: Crime forecasting, Human mobility, Machine Learning, Deep Learning This paper has been published in the Journal of Quantitative Criminology: 10.1007/s10940-025-09629-3 The increasing availability of data, coupled with advancements in analytical techniques and the progressive democratization of programming languages, has contributed to fa- cilitate the computational study of criminal phenomena . One of the areas that most benefited from the convergence of these three factors is the spatiotemporal prediction of crime, which is particularly important for theory and policy testing and development, as it can shed light on the behavioral dynamics that explain crime occurrence and provide actionable knowledge that can be leveraged to anticipate and prevent crime . The spatial study of crime has its roots in the first half of the 19th century [4–6] and became extremely popular in the second half of the 20th century. One of the most widely accepted findings to emerge from this area of research is that crime tends to concentrate in small geographical areas. This observation led to the so-called criminology of place, first introduced by , which focuses on the dynamics of crime in microgeographic units within cities, such as addresses, facilities, street segments, or small clusters of street segments . Since then, the growing interest in these micro-dynamics has led to practical applications in crime prevention, such as hot spots policing . Along with the spatial study of crime, another factor that has long been considered in the empirical and theoretical literature is the temporal dimension of crime . The two dimensions, space and time, are intrinsically interconnected: both components of a crime event are fundamental due to their correlation as time constraints result in space constraints . Given this fundamentally indissoluble link, crime forecasting systems rely on both spatial and temporal information in order to deliver more accurate predictions. However, beyond the spatial and temporal aspects of crime, many other factors play a role in shaping its concentration and patterned nature. One of the most studied factors is the social context, often examined through dimensions such as socioeconomic inequalities and ethnic diversity. Poverty, for instance, often fosters conditions that increase the likelihood of crime by limiting people’s access to opportunities and resources [13–16]. Meanwhile, neighborhoods with significant ethnic diversity sometimes experience crime patterns shaped by the social tensions and discrimination that residents face . Besides these socio-economic dimensions, another central phenomenon impacting the spatiotemporal concentration of crime is human mobility, as suggested by various theories uncovering its link to crime incidents . For example, areas with high foot traffic or frequent visitors often experience more crime due to the increased presence of potential victims and offenders. Conversely, more deserted areas can foster crime due to the absence of individuals who can discourage offenders from committing crimes . Similarly, shifts in population dynamics—such as the influx of new residents or temporary workers—can disrupt established community networks, creating opportunities for criminal behavior . Due to the nonlinear and multidimensional spatiotemporal dynamics that often charac- terize crime patterns, quantitative criminologists—along with scholars from other fields— 1 have thus recently become increasingly interested in the promising properties of machine and deep learning methods. Compared to more traditional statistical methods, these algorithms have, in fact, often shown to be more effective in capturing complex patterns, outplaying less flexible methods in prediction and forecasting tasks . However, de- spite the growing interest in these technological trends and the growing recognition of the importance of mobility in explaining crime, as of the current date, there exist very few studies on crime forecasting employing deep learning models and combining histor- ical crime data with human mobility data. Moreover, such studies use relatively large geographic units of analysis (e.g., census tract or police beats) or have a large temporal granularity (i.e., weeks, months, or years), resulting in predictions that cannot easily yield actionable recommendations. Additionally, these studies often focus on only one city or employ just one year of historical data, leading to models with low generalizability to different contexts [22–25]. To address these gaps, in this research we propose a deep learning framework—based on Convolutional Long Short-Term Memory (Conv LSTM) layers—to perform spatiotem- poral crime forecasting in microgeographic units, utilizing historical crime data in con- junction with micro-level mobility and sociodemographic data from 2019 to 2023 across four U.S. cities. More specifically, the main contribution of this work is that, to the best of our knowledge, this is the first study to use mobility data in spatiotemporal crime forecasting with very fine spatial and temporal resolutions (0.077 sq. miles and 12 hours, respectively), covering a period of five years of historical data and including four different cities. This design thus enables a broader comparative analysis, allowing for a more in- depth exploration of the benefits and limitations of predicting crime occurrences at highly detailed geographical and temporal levels. This, in turn, advances our understanding of crime both theoretically and practically, while further investigating whether and how arti- ficial intelligence can serve as an effective ally for criminology. Our short-term forecasting effort, focused on hourly time scales and fine-grained spatial resolution, is motivated by two interrelated goals. By testing the predictive capabilities of deep learning approaches, we aim not only to advance theoretical insights into the mechanisms underlying crime occurrence and distribution, but also to generate practical knowledge that can inform pol- icy decisions and situational interventions. These may include, for example, shift-by-shift allocation of patrol officers, but also extend to non-policing measures, such as adjusting lighting in high-risk areas, modifying urban infrastructure to reduce anonymity or escape routes, deploying community outreach teams during peak risk hours, or temporarily al- tering the accessibility of specific spaces (e.g., parks, transit hubs) based on dynamic risk assessments. The remainder of the paper proceeds as follows. In Section 2, we outline the main theories that motivate the use of mobility data for crime forecasting and our contributions to the literature. In Sections 3 and 4, we describe the datasets used, the extensive data processing carried out, and the deep learning algorithm we designed to perform the crime forecasting task. In Section 5, we evaluate the performance of the model and the impact of 2 using mobility data on forecasting outcomes. Lastly, in Section 6, we discuss the results, highlight the limitations of this study, and address its ethical implications. 2.1 Theoretical framework 2.1.1 Crime concentration The primary framework of this study is the one stemming from the law of crime concen- tration, proposed by Weisburd in his 2014 Sutherland Address to the American Society of Criminology. It states that “for a defined measure of crime at a specific microgeographic unit, the concentration of crime will fall within a narrow bandwidth of percentages for a defined cumulative proportion of crime" . This law was inspired by early descriptive findings on the study of crime that appeared in the 19th century as well as empirical evidence amassed over the last decades of the 20th century and claims that most crime concentrates in small areas regardless of the city . The law of crime concentration, one of the most universally accepted features of crime as a human phenomenon, is among the many applications of the Pareto principle, which posits that while numerous factors may contribute to an observed outcome, a small subset of these factors often accounts for the majority of the observations . This principle has been shown to govern many social phenomena. For instance, it appears in education, particularly in the dynamics of classroom disruption ; in network theory applied to social influence in online commu- nities ; in the field of economics, specifically regarding wealth distribution ; and in other areas of criminology, for example, asserting that most crimes in a population are committed by a small number of individuals . We frame the present study starting from the law of crime concentration because, if crime is clustered in space, it follows that there are statistical patterns to be inferred. Learning these patterns, along with their sources of variation, is the key goal of our forecasting framework. However, while the law of crime concentration represents the fundamental intuition behind this work, there exist complementary theoretical traditions that should be taken into consideration to more comprehensively understand how concentration occurs and, importantly, how to explain and predict concentration in space and time. Modern urban environments are increasingly shaped by factors such as commuting patterns, population flows, gentrification, and urban sprawl : these fluid phenomena are crucial to capturing crime patterns, making it imperative to incorporate such dynamic elements into the study of crime, especially when seeking to forecast crime emergence. 3 2.1.2 Crime and human mobility As a social phenomenon, human mobility both influences and is influenced by the envi- ronments people engage with. These dynamics also play a critical role in the occurrence of criminal events, which arise from the interplay between offenders, potential targets, and their surroundings. Certain conditions—such as a lack of guardianship, transient populations, or economic disadvantage—can amplify the likelihood of crime occurring. Therefore, to better understand these processes, researchers have proposed theories that examine how human mobility, social structures, and neighborhood characteristics con- tribute to the spatial and temporal distribution of crime. For instance, crimes are more likely to occur when offenders and potential targets con- verge in environments that lack capable guardianship. This concept is central to Routine Activity Theory (RAT), introduced by . According to RAT, offenders act when they possess both the motivation and the capacity to commit a crime, often selecting victims either deliberately, based on characteristics like identity or possessions, or opportunisti- cally, based on accessibility or vulnerability. Guardians—such as police officers, family members, or institutions like law enforcement—serve as deterrents but must be able to act effectively upon witnessing criminal behavior. The spatial dimension of RAT is further refined by crime pattern theory , which incorporates elements from geometric theory. Geometric theory explores how the built environment shapes the geographic distribution of crime, leading individuals to form awareness spaces consisting of their main routine activity nodes . Hence, the intersection of offender awareness spaces and potential crime opportunities is where crime hot spots will arise . RAT is therefore important for crime forecasting and for this study specifically, as it provides a conceptual framework for understanding the relationship between human mobility patterns and the dynamics of crime opportunities. Based on these theoretical prepositions, incorporating mobility data can enhance predictive accuracy by allowing us to detect changes in routine activi- ties (e.g., population density, and business hours) that influence the likelihood of crime. The theoretical postulations regarding the relationship between crime incidents and hu- man mobility have been backed by extensive empirical evidence [24, 36–38]. Benefiting from the growing integration between traditional data and more innovative sources (for a review see ), these studies usually include mobility features in the form of footfall1, showing that this feature can ameliorate the performance of crime forecasting models when compared to models that only use historical crime features. Persistent crime patterns, however, are also closely linked to the socioeconomic condi- tions of neighborhoods. Communities with high levels of poverty, residential instability, and ethnic diversity often experience weakened social networks and limited informal so- cial control. These conditions form the basis of the Social Disorganization Theory (SDT) developed by the Chicago School . Social disorganization fosters environments where subcultures of crime and deviance flourish, further contributing to higher crime rates 1The footfall is defined as the number of people in a given location during a defined time span. 4 . Despite some challenges and inconsistencies in its empirical research , SDT remains highly relevant as it provides a lens through which we can investigate how human mobility, neighborhood dynamics, and social structures intersect to influence crime rates. Hence, using mobility data in combination with sociodemographic data allows us to more comprehensively capture patterns and trends, as well as changes in these patterns and trends, occurring in urban environments. Leveraging both dimensions, we argue, would improve our ability to understand when and where crime will happen. 2.2 Contributions to crime forecasting Crime forecasting research employing advanced computational methods encompasses a range of scales and methodologies, each designed to address specific contexts and objectives. A review of the past decade’s literature reveals five core elements in study design. Here, we outline our contributions and innovations with respect to each of these elements. Element 1: What urban focus? Firstly, most studies focus on a single city in the U.S., such as New York City or Chicago [43–46]. A smaller number of studies aim for broader comparability. For example, analyzed crime patterns in four U.S. cities highlighting the benefits of comparative analysis in diverse urban environments. Another notable example is , which examined event- level predictions of urban crime across eight U.S. cities. Drawing inspiration from these multi-city approaches, our study investigates four U.S. cities to enhance generalizability of our results, seeking to underscore common patterns and potential differences across diverse urban areas. Element 2: What geographical scale? Secondly, the choice of geographic unit, which reflects a fundamental trade-off between spatial detail and computational accuracy, has mostly focused on relatively large areas of 1 km2 or larger . Despite this, there are some studies that used very fine geographic units of analysis. For instance, used a highly detailed grid made of square cells measuring 91 × 91 meters in their study of gun violence in Little Rock, AR, and used a grid of cells measuring 183 × 183 meters, approximately 0.122 km2 to crime incidents in Portland, OR. However, in such studies, the corresponding temporal granularity was relatively large (i.e., six months and two weeks, respectively). Our analysis adopts a spatial unit of 0.077 sq. miles,2 offering a compromise that captures micro-spatial patterns effectively while remaining computationally manageable, also avoiding excessive sparsity, which would have rendered any forecasting exercise theoretically and practically useless. 20.077 sq. miles are equivalent to 0.2 sq. kilometers in the metric system. 5 Element 3: What temporal granularity? As anticipated in the previous paragraph, the temporal granularity of predictions further distinguishes contributions in this field. Many studies focus on daily predictions , while some explore larger granularities. For instance, employed a coarser approach, generating yearly forecasts for gun violence. Similarly, examined monthly predictions to highlight long-term trends. Our work focuses on a 12-hour prediction interval, a granularity never explored in the literature in combination with fine spatial resolution. By narrowing the prediction window and using a very fine spatial granularity, we aim to provide more actionable insights for real-time resource allocation and urban safety measures, underscoring not only the potential of this method but also its limits. Element 4: What temporal focus? Temporal coverage of the historical data is another critical dimension, influencing the ability to capture trends and variability. Many studies rely just on one year of data, focusing on short-term predictions , while few others try to capture long-term patterns by extending their analysis to eight years or more . Other researchers, including and , settled on datasets spanning five years, finding a balance between temporal depth and data availability. Our study aligns with this medium-term focus, leveraging five years of data to identify meaningful patterns. Element 5: How many data sources? Lastly, recent advancements in crime forecasting have increasingly emphasized the impor- tance of integrating diverse data sources. While traditionally previous studies often relied exclusively on crime data , more recent approaches highlight the added value of contextual information as well. In general, the use of mobility data has increased substantially in the past years in criminology , and has played a major role in the com- putational social science revolution . Mobility is measured through various sources, including transportation data , GPS tracking , and social media or other types of digital traces , and it allows researchers to capture individual and collective behaviors and flows at fine-grained geographical and temporal scales. Despite this grow- ing interest and the evident potential of mobility data to enrich crime analysis, their role in short-term crime forecasting remains underexplored and largely unquantified. In par- ticular, we still lack systematic evidence and empirical estimations of the extent to which incorporating mobility features can enhance the accuracy of predicting when and where crimes are likely to occur—an insight that would be highly valuable to both scholars and practitioners. For this reason, our study embraces a multidimensional perspective, inte- grating crime, mobility, and sociodemographic data to capture the complex interplay of social, spatial, and temporal dynamics in crime occurrence, in the attempt to disentangle the contribution that each dimension provides to the forecasting ability of deep learning. 6 Therefore, by building on these five elements, our study advances the field by adopting a multi-city framework with exceptionally fine spatial and temporal resolution, utilizing five years of data and integrating three distinct data sources. These design choices align with recent advancements while pushing the boundaries of crime forecasting research. 3.1 Sample Selection In this research, we analyzed four U.S. cities: Baltimore, MD (Bal); Chicago, IL (Chi); Los Angeles, CA (Las); and Philadelphia, PA (Phi). These cities were chosen due to their high crime rates and their good coverage of mobility data (the specific criteria we defined and used are explained in detail in Section 3.3). In addition, their diversity with respect to location, urban structure, and sociodemographics made them particularly appealing for our comparative analysis between cities. For instance, Baltimore is a port city focused on shipping and trade, with a significant African American majority. In contrast, Los Angeles is nearly six times larger, characterized by a sprawling car-dependent structure, and is predominantly home to Latino and Asian communities. A more detailed overview of the four cities can be found in contained four variables: the date and time of the crime incident, the category of the crime incident, and the location of the crime given as latitude and longitude coordinates. Although several crime categories were available depending on the city, we first se- lected those that were common across all four settings and, among these, the ones more likely to be reported to the police. This latter criterion was important to ensure that the crime data we used for the analysis were not severely impacted by the risk of under- reporting, which would have introduced non-trivial issues to our forecasting exercises [64–66]. Specifically, we selected the following crime categories: burglary, motor vehicle theft (MVT), assault, homicide, and robbery. The first two are property crimes, while the remaining three are violent crimes. Our analysis is thus three-fold: we developed models forecasting all crime together, as well as models focusing on property and violent crimes separately. The percentages of each type of crime and the total number of crime incidents for each city are shown in Average Crimes in 2019 Average Crimes in 2020 Average Crimes in 2021 Avg crime incidents every 12h 60 60 60 50 50 50 40 40 40 30 30 30 20 20 20 10 10 0 Bal Chi Las Phi 0 Average Crimes in 2022 Average Crimes in 2023 Avg crime incidents every 12h 60 60 50 40 40 30 30 20 20 10 10 0 In order to select the subset of cities to be used for this study, we were aware that not only the number of POIs was relevant, but also the relationship POIs had with popula- tion size and the city’s area since population density and POI density allow for a better understanding of the potential human mobility coverage of each POI. For this reason, we defined a variable called the people-to-POI ratio, representing the number of people per POI. This ratio was calculated by dividing the population density by the POI density, hence indicating the number of people per POI within 1 km2. Therefore, since this is a normalized value, it offers a fairer way to compare the cities, particularly in terms of the richness of the data in capturing as much as possible the underlying mobility dimension of a city. We computed this ratio for the main 735 cities in the U.S. and obtained a list ordered by ascending people-to-POI ratio. From this list, we focused on cities with high crime rates, verified that (a) each did not fall in the upper extreme of the distribution in terms of people-to-POI ratio5 and (b) each had complete crime data from 2019 to 2023 publicly available. This resulted in the final selection of four U.S. cities mentioned in the previous subsection (i.e., Baltimore, Chicago, Los Angeles, and Philadelphia). Table 3 presents the people-to-POI ratio for each of the four cities, with a lower ratio indicating better POI coverage. In Appendix B, we visualize the histogram of the distribution of the people-to-POI ratio for all 735 cities. Area Population density Num. of POIs People-to-POI ratio Bal 210 2,930 28,218 21.78 Chi 591 4,581 95,049 28.46 Las 1,216 3,273 102,478 38.80 Phi 347 4,518 55,496 28.25 people visited each POI category in each cell during each time window, thus representing the underlying mobility characteristics of all our cities, mapping potential time-specific patterns at the daily level (i.e., dynamics changing between day and night), as well as meso-level (i.e., different dynamics on different days of the week) and macro-level (e.g., structural changes due to gentrification) seasonality and trends. Second, we calculated the POI category diversity using the Shannon Diversity Index7, defined as: n Õ H= − i=1 pi· ln(pi), (1) where pirepresents the proportion of each POI category iwithin each spatial unit of analysis, and nis the total number of POI categories (11 in our case). We computed this measure to map the heterogeneity of a given spatial unit in terms of human activities carried out, hypothesizing that this information can be valuable in understanding its potential crime attractors or targets, as well as representing the complexity of its human and social fabric. To calculate the proportion of each POI category within each spatial unit, we first determined the total number of POIs across all categories within each cell by summing the individual counts. The proportion for each category was then computed by dividing the count of that category by the total count within the cell. This process yielded a total of 12 features: 11 representing footfall per POI category and one representing POI diversity. Maps displaying the distribution of POI diversity for each of the four U.S. cities can be found in Appendix F. 3.4 Sociodemographic data Besides crime and mobility data, we enriched our feature space by including a range of sociodemographic variables. These features allow us to take into consideration the broader social context of each geographic unit beyond the type of activities and premises that are captured by our mobility data. The data we used were obtained from the American Community Survey. Specifically, we used the ACS 5-year estimates for 2019, 2020, and 202189. These datasets contain more than 20,000 variables related to the social, economic, demographic, and housing characteristics of the U.S. population for different geographic granularities (e.g., region, county, county subdivision, and block group). Firstly, we chose the block group as our geographic level since it is the smallest granu- larity available. Afterward, we selected 26 variables related to gender, age, race/ethnicity, 7The Shannon Diversity Index (also called the Shannon-Wiener Index) is used to characterize entity diversity in a given community or set of elements. The higher the index value, the greater the entity diversity. The data for 2022 and 2023 had not been released at the time of this study, so we decided to use the 2021 data for both years, as we do not expect significant variations. 11 employment, income, education, and marital status—features that have been identified as correlated with crime in the scientific literature [69–71]. The specific variables are listed in Appendix E. Their raw values (in the case of medians) or percentages (in the case of counts) were used as our 26 sociodemographic features. 4.1 Data preparation To formalize the microgeographic unit of analysis, we used a grid-based approach, which consisted of defining a grid containing the city borders and then calculating the values of all our features for each spatial cell during every 12-hour period throughout the study’s time span. Therefore the problem definition becomes as follows: Consider that we divide a city into an N× Ngrid (with Nvarying for each city to ensure that each cell has an area of 0.077 sq. miles), hence into a matrix of the form G = (gi,j)0<i,j≤N. Then for each grid cell gi,j, we compute 39 sets of predictive features for each 12-hour block. The first predictive feature is the historical crimes, where we have that the crime feature at the 12-hour block tis represented by Ct i,j= {ct−T i,j, ct−T+1 i,j , . . . , ct−1 i,j} where Tis our look-back period, hence how many blocks of 12 hours before twe use to predict this feature at the 12-hour block t. The second set is the 12 mobility features, which are denoted as Mt i,j= {mt−T yi,j, mt−T+1 yi,j , . . . , mt−1 yi,j} for each mobility feature ywith y= 1, 2, . . . , 12 and for each 12-hour block t. Lastly, we have the 26 sociodemographic features, hence for each sociodemographic feature zwith z= 1, 2, . . . , 26 and each 12-hour block t, we get the features St zi,j= {st−T zi,j, st−T+1 zi,j , . . . , st−1 zi,j}10. Consequently, and borrowing this example from computer vision, each frame can be interpreted as an N×Nimage with 39 channels, where each channel represents a different feature (see Figure 2). In other words, our forecasting task can be thought of as similar to predicting the next frame in a video: given a series of previous frames, each characterized by a specific set of features, what will the next frame look like? These frames are then grouped into sequences, with the length determined by the look-back (LB) period T. Each feature within these sequences is then normalized using min-max scaling, ensuring consistent value ranges across the dataset. Also, to maintain uniform grid sizes across all four cities and to augment the number of samples available, we randomly sampled five M× Msubsections for each sequence of frames, keeping only the sequences with at least 2 crimes in that area in order to balance the dataset as much as possible. For this study, we set M= 16, which covers approximately 19.31 sq. miles (50 km2), and used two LB periods of T= 28 and T= 4 (equivalent to 14 days and 2 10Since the size of each block group varies significantly, the preprocessing of the sociodemographic features involved identifying the block group(s) that intersected with each cell, and extracting the corre- sponding sociodemographic data. In cases where multiple block groups overlapped with a cell, a simple average was computed. For cells with no intersection, the value was set to Na N. 12 Sociodem feature 26 Mobility feature 12 Sociodem feature 1 Mobility feature 1 Crime feature (t2, t3, t4), (t3, t4, t5), (t4, t5, t6), (t5, t6, t7), (t6, t7, t8), (t7, t8, t9), and (t8, t9, t10). Applying a 90/10 chronological split assigns the first seven samples to the train set and the last one to the test set. Next, to avoid data leakage, we remove the last Tsequences of the training set, since they contain frames that are part of the test set. This results in a training set with five sequences and a test set with one sequence. For our actual dataset, when T= 14, this procedure resulted in training and test sets containing 3,222 and 362 sequences, respectively, which would increased to 16,110 and 1,810 samples after extracting the five subgrid sections. However, sequences with no crime events were discarded, leading to the loss of some samples. Additionally, to keep the same sample size for all data configurations (i.e., for the four cities and three crime aggregations), we retained the number of samples corresponding to the smallest dataset among those. This yielded a final training set size of 12,546 sequences and a test set size of 1,510 sequences. Lastly, before using these datasets for our analysis, we applied a spatial mask to exclude all cells that met at least one of the following criteria: (i) they are located outside city borders, (ii) they consist entirely of water, or (iii) they do not intersect with any census block group. A visualization of the included and excluded cells is provided in Appendix H. 4.2 Convolutional LSTMs Our main model is based on Conv LSTM layers, proposed by (author?) . This type of neural network consists of two main components: Long Short-Term Memory (LSTM) layers and convolution operations. LSTMs are powerful neural networks designed to handle temporal patterns. For instance, they have been used to predict daily streamflow through weekly forecast horizons , for speech recognition by modeling temporal dependencies in audio signals , and for determining the probability that certain social media content will gain popularity . They are able to perform such tasks by using an internal structure referred to as memory cells. Each cell contains a cell state (c), a hidden state (h), and three types of gates that determine the flow of information using sigmoid and tanh activation functions. The types of gates are the forget gate ft, the input gate it, and the output gate ot. Their corresponding equations for the multivariate version of the LSTM (i.e., the fully connected LSTM or FC-LSTM), where both inputs and outputs are 1-dimensional vectors, are as follows: ft= σ(Wxfxt+ Whfht−1 + Wci⊙ct−1 + bf) (2) it= σ(Wxixt+ Whiht−1 + Wcf⊙ct−1 + bi) (3) ot= σ(Wxoxt+ Wxcht−1 + Wco⊙ct+ bo), (4) where ht−1 is the previous hidden state, xtis the current input, Ware the weight matrices (where the subscripts indicate the two variables that are connected by this matrix), bxis the 14 bias for that gate, and σrepresents the sigmoid function . The goal of the forget gate is to determine which information from the previous cell state ct−1 should be forgotten. The aim of the input gate is to select which new information should be added to the current cell state ct. Lastly, the output gate is used to assess what information should be included in the current hidden state ht. Each candidate cell is formalized through the equations related to the cell state ctand the hidden state ht: ct= ftct−1 + ittanh(Wxcxt+ Whcht−1 + bc) (5) ht= ot⊙tanh(ct). (6) These equations highlight how the candidate cell is used in combination with the forget gate and the input gate in order to obtain the current cell state. Subsequently, the cell state and the output gate are employed to obtain the hidden state, which summarizes all the information learned. Lastly, the hidden state is utilized to make the final prediction ytby using the appropriate activation function for the task at hand. Therefore, memory cells allow LSTMs to retain only the information that is most relevant to the forecasting task. The advantage of these internal structures is that they trap the gradient11 in the cell, preventing it from becoming too large or too small and thus avoiding in this way the vanishing/exploding gradients problem that significantly limited its predecessor, the recurrent neural networks [77–79]. However, while LSTMs are effective at capturing temporal information, they are not engineered to account for spatial information. To address this, Conv LSTMs extend tradi- tional LSTMs by replacing the one-dimensional vectors with three-dimensional tensors, where the first dimension represents time and the other two are the spatial dimensions (the number of rows and the number of columns) of the input frame.Therefore, the matrix multiplications in the FC-LSTM equations—used in both the input-to-state and state-to- state transitions—are replaced by convolutional operations, and the weight matrices are replaced by convolutional kernels . To illustrate how the introduction of convolutional operations allows the model to preserve spatial information, consider a simple example where the input xtis a single- channel 3 × 3 spatial grid and the output should be of the same shape. Then the operation Wxtis done as follows. In the case of LSTM, xtis flattened into a 9 × 1 vector and this operation becomes a simple matrix multiplication with W being a 9 × 9 weight matrix. Hence, the operation results in a 9 × 1 vector, which is then reshaped into a 3 × 3 matrix to recover its 2-dimensional shape. Therefore, we can see that the fully connected operation disregards the spatial structure of xt. In contrast, for Conv LSTM, assume Wis a 2 × 2 convolutional kernel. The convolutional operation is then performed by sliding this kernel over the input grid and performing element-wise multiplication of its values with the corresponding elements in the input grid, followed by summing the results. This 11The gradient is a vector that points in the direction of the steepest increase in a multivariate function. 15 operation is repeated for the rest of the grid, with the kernel sliding across the grid one element at a time in both the horizontal and vertical directions. To ensure the output has the same spatial dimensions as the input, zero-padding12 is applied. Therefore, this operation results in a 3 × 3 matrix as output, preserving the spatial dependencies of the input, and maintaining the spatial structure of xtthroughout the computation. Another advantage of this type of neural network is that the use of three-dimensional tensors considerably reduces the number of parameters to be trained when compared to LSTMs. This ability to incorporate both spatial and temporal information makes Conv L- STMs appealing for capturing the complex spatiotemporal dynamical patterns in crime events, hence offering a suitable algorithmic framework for our task. 4.3 Model architecture Conv LSTMs were initially defined for precipitation nowcasting, using past radar se- quences to predict future radar frames . Since then, they have been applied in various contexts, such as quantifying the contributions of climate change and human activities to vegetation change, and predicting short-term traffic flow . Therefore, this type of neural network is typically used for video frame prediction, as it accounts for both the temporal and spatial components of the data, making it ideal for the data structure we constructed. While this is usually performed with either grayscale images (1 channel) or RGB images (3 channels)13, we adjusted the model architecture to train on images with 39 channels (corresponding to our 39 features) and output a 1-channel image. In order to engineer our model, we started with a basic architecture for video frame prediction using Conv LSTMs and adapted it to our specific problem. Then we fine-tuned several hyperparameters (see Appendix G for the specific values used) until we obtained the model architecture depicted in 123, and 999) to control for the role of stochasticity in model training and to obtain the average and standard deviation of each model’s performance. Given the unbalanced distribution in our target variable and the differential cost of wrong predictions between false positives and false negatives, we primarily focus on recall to quantifythepredictiveperformance ofthemodel. This isduetothehighcostsassociated with false negatives, as they signify a failure to allocate crime prevention resources to a location in need. Besides these two metrics, we also calculate a second version of recall and precision that considers false positives with neighboring positive cells15 as true positives. This choice is motivated by two aspects. First, the presence of crime in a location affects the nearby areas (and the same occurs with police presence) and spillover effects may lead to incidents in locations that fall within adjacent spatial cells. Second, given obfuscated georeferenced crime data, crime incidents that actually occurred in a given cell may be recorded differently and therefore fall in a neighboring one: calculating modified metrics would then allow us to predict them without losing such relevant data points. We calculated these two modified metrics as follows. Let O = (oi,j)1≤i,j≤Mdenote the ground truth crime occurrence matrix and P = (pi,j)1≤i,j≤Mthe predicted crime occurrence matrix, where oi,j, pi,j∈{0, 1} and Mis the size of the matrix as defined in Section 4.1. Next, let Ni,j(O) represent the set of indices of the cells with a Chebyshev distance16 of 1 from cell (i, j) ∈O (i.e., its nearest neighbors). Using these three elements, the classification outcomes are defined as follows: True negatives(tn) : Cells where both oi,j= 0 and pi,j= 0;   True positives(tp) : Cells where both oi,j= 1 and pi,j= 1; False negatives( fn) : Cells where oi,j= 1 and pi,j= 0; Cell Classification: fpnn, if ∃(i′, j′) ∈Ni,j(O) such that oi′,j′ = 1, (neighboring false positive)   False positives : fp, (otherwise, standard false positive) (7) Therefore, the modified metrics become: Recall NN = tp+ fpnn tp+ fpnn+ fn, Precision NN = tp+ fpnn tp+ fpnn+ fp. (8) 15We define as neighboring cells all the cells that are touching the target cell. Hence, there can be between 3 and 8 neighboring cells, depending on the target cells’ location in the grid. 16The Chebyshev distance, also known as the chessboard distance, between two cells (i1, j1) and (i2, j2) in a grid is defined as: D((i1, j1), (i2, j2)) = max(|i1 −i2|, |j1 −j2|). This metric captures the maximum horizontal, vertical, or diagonal displacement between two cells. 18 These modified metrics (called NN, an acronym for nearest neighbor) are particularly relevant and applicable in our case since having cells of size 0.077 sq. miles implies that a police officer would take approximately 5 minutes (if the cells share an entire border) or 8 minutes (if they only share a corner) walking (or 2.5 and 4 minutes running, respectively) to go from the center of one cell to the center of a neighboring cell, assuming an average walking speed of 5 km/h. This highlights how small our geographic unit of analysis is and underscores how NN performance can be relevant for first responders. Lastly, to determine the effectiveness of our proposed model, we compare it against three baseline models, chosen to provide a comprehensive evaluation by spanning sta- tistical methods, traditional machine learning techniques, and deep learning approaches: (1) Logistic Regression (LR), a commonly used baseline in statistics and machine learning that assumes linear relationships between the features and the target variable; (2) Ran- dom Forest (RF), a traditional machine learning ensemble approach well-regarded for its ability to handle nonlinear relationships; and (3) LSTM, a deep learning model capable of capturing temporal dependencies but unable to account for spatial dependencies.17 5.1 General performance 5.1.1 All crimes Firstly, we assessed the model’s performance against the three chosen baselines—LR, RF, and LSTM—by considering all five crime categories together, to predict whether there will be a crime in the next time window without discriminating by type. On the one hand, this approach has the advantage that it contains more crime incidents, thereby making the dataset relatively more balanced, and stabilizing the learning process. On the other hand, combining crimes with distinct seasonal patterns may introduce further challenges by obscuring individual crime trends. The results in Figure 5 show that Conv LSTM is the overall best-performing model, which can be clearly seen when comparing the standard and modified F1 scores across the four models. It also achieves the highest standard recall values in all cities, with a notable increase in performance when comparing the results of the modified metrics to the standard metrics. To exemplify, recall for our Conv LSTM model is always higher than 0.65 17We recognize that self-exciting models (see, for reference, and ) were specifically designed to address the challenges of short-term crime forecasting and have demonstrated considerable value in the criminological literature. However, we deliberately chose not to include these models among our baseline comparisons for the present study. This decision is motivated by the fact that a fair and meaningful comparison would require extending the Conv LSTM architecture with attention mechanisms, which better capture the dependencies central to the concept of self-excitation. Such a modification would represent a significant methodological advancement in its own right and is therefore beyond the intended scope of this work. 19 across all cities, meaning that we are able to correctly forecast crime presence for 65% of the cell-time unit observations in which crime actually occurred. Precision is extremely low, being consistently below 10%, meaning that only one in ten forecasts of crime presence are actually true positives. However, when considering the modified precision metric, performance increases substantially: on average, when we forecast the presence of crime for a given spatial cell, there is approximately a 50% chance that crime will occur in some of the adjacent spatial cells in that same time unit. Given our fine-grained spatial scale and the highly unbalanced nature of our algorithmic task, this represents an important level of accuracy that does not hinder potential timely response in the short aftermath of a crime. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB = 14 days LB = 2 days the modified metrics. Although LSTM is the second-best model for recall, its precision is lower than that of LR, highlighting the importance of the spatial patterns captured by Conv LSTM’s convolutional operations for improving both recall and precision. It is also worth noting that, despite LSTM’s comparable performance, it has approximately ten times more parameters than Conv LSTM, leading to higher computational costs and an increased risk of overfitting, particularly with limited data. Lastly, the comparison of the LB periods reveals mixed results across models. For Conv LSTM, the 2-day LB period typically shows slightly lower performance compared to the 14-day period, although it achieves higher modified precision in Baltimore, Chicago, and Philadelphia. In contrast, LSTM generally performs better with the 2-day LB period. Meanwhile, LR demonstrates similar performance across both LB periods, while RF seems to benefit from longer LB periods. If we instead focus on the F1 scores, we observe that both LB periods have similar performance, with only Los Angeles showing noticeably higher performance when using the 14-day period for all models except LSTM. A robustness check using 7-day and 1-day LB periods (see Appendix J) shows that the shorter LB period performs similarly or worse, except for the modified value in Baltimore. These mixed results suggest that the optimal LB period to be used to forecast crime depends both on the chosen algorithm and on the city itself. 5.1.2 Violent crimes Figure 6 presents the results when considering only violent crimes (i.e., assault, homicide, and robbery). Although this dataset contains fewer crime incidents, it remains relatively balanced since assault accounts for approximately half of the crime incidents in each city, as shown in (below 0.2). RF continues to be the worst-performing model, generally exhibiting very low recall and precision, although its performance improves in Chicago, Los Angeles, and Philadelphia, compared to its results for violent crimes. LSTM still achieves the highest recall among the baselines but shows slightly worse overall performance (as indicated by the F1 score) compared to its application to all crimes or violent crimes alone. Finally, the Conv LSTM model remains the best-performing model, maintaining strong recall values but showing a decrease in precision—a trend consistent with the baselines. Baltimore Chicago Los Angeles Philadelphia 1.0 Metric value 0.5 Recall Recall NN 0.0 Recall Recall NN Recall Recall NN Recall Recall NN 1.0 Metric value 0.5 Precision Precision NN 0.0 Precision Precision NN Precision Precision NN Precision Precision NN 1.0 Metric value 0.5 0.0 F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric F1 F1 NN Metric Conv LSTM LSTM LR RF LB = 14 days LB = 2 days (see Appendix J). These results suggest that the patterns driving property crimes may generally operate over shorter timescales compared to those of violent crimes, although there is some variation depending on the city. 5.2 Effect of mobility features After evaluating the models’ general performance, we sought to assess the impact of including mobility features in our deep learning framework. Thus, we retrained the best-performing model, Conv LSTM, using various feature sets: CMS, CM, CS, and C, where C represents crime features, M represents mobility features, and S represents sociodemographic features. This retraining was conducted using the best configuration alone, hence including all five crime types. Figure 8 shows the results of the average performance and standard deviation for each of the four cities. If we focus on the results using an LB period of 14 days, the first observation is the considerable variation in standard recall within each city, depending on the feature set used for training. Looking more closely, we see that Chicago and Los Angeles have the lowest performance when focusing on the standard metrics, with the recall always below 0.76 and the precision always below 0.08. The same trend is observed when looking at the modified metrics. Overall, based on both the standard and modified recall and precision values, the setup incorporating all three feature types consistently performs best, as expected. Specifically, among the 24 configurations seen in Figure 8, the C setup achieves the highest performance in 8.3% of cases, the CM setup in 8.3%, CS in 29.2%, and CMS in 54.2%. Therefore, the results indicate that incorporating additional feature types alongside crime data improves the model’s ability to forecast effectively. Conversely, when analyzing the results with an LB period of 2 days, performance consistently decreases slightly when using the standard metrics. However, Chicago and Los Angeles remain the cities with the lowest performance, and the CMS configuration again emerges as the best overall. In this case, the C setup does not achieve the highest performance in any instance, while the CM setup outperforms others in 12.5% of cases, CS in 29.2%, and CMS in 58.3%. Thus, while shorter LB periods lead to a slight reduction in overall performance, the advantage of incorporating additional feature types alongside crime data becomes more evident. Afterward, to further understand the impact of mobility features on predictive perfor- mance, we calculated the percentage difference between the results of CS and CMS (to assess the effect of incorporating mobility features) and CM and CMS (to evaluate the effect of sociodemographic features) for both LB periods. As shown in Table 4, the results generally indicate a slight decrease in recall when a third feature set is added—both when going from CM to CMS and from CS to CMS—resulting in a negative percentage point difference. In contrast, precision increases in all configurations, with the exception of its modified value for Chicago when using a 14-day LB and Baltimore when using the 24 2-day LB. Another noteworthy observation is that the LB period of 2 days always shows a greater percentage point increase in overall performance (F1 score) compared to the 14-day period, suggesting that shorter LB periods benefit more from the incorporation of mobility features. Additionally, comparing the F1 scores between CMS-CM and CMS-CS, we can see that sociodemographic features have a greater impact than mobility features in increasing performance; however, both contribute to enhancing the model’s ability to forecast crime, with the best performance achieved when they are used together. Baltimore Chicago Los Angeles Philadelphia 1.00 0.75 Recall 0.50 0.25 0.00 1.00 0.75 Precision 0.50 0.25 0.00 1.00 0.75 0.50 F1 0.25 0.00 1.00 0.75 Recall NN 0.50 0.25 0.00 1.00 Precision NN 0.75 0.50 0.25 0.00 1.00 0.75 F1 NN 0.50 0.25 C CM CS CMS 0.00 C CM CS CMS C CM CS CMS C CM CS CMS LB = 14 days LB = 2 days Firstly, we used five years of human mobility data, in combination with crime and sociodemographic data from four U.S. cities, to assess the extent to which these features could enhance the prediction of the spatiotemporal occurrence of crime. Secondly, we defined a small geographic unit of analysis of 0.077 sq. miles and a fine temporal gran- ularity of 12 hours in order to assess whether this fine-grained spatiotemporal unit of analysis allows us to meaningfully forecast crime in four different urban contexts in the U.S. To evaluate this, we designed a deep learning model using Conv LSTM layers and compared its performance to three baseline models to determine whether the performance improvement of using an advanced computational method compensated for the complex- ity involved in building it. Lastly, we compared the results from two different LB periods to assess the impact of longer versus shorter time windows on predictive performance. The results led to several key findings. We first demonstrated that Conv LSTM con- sistently outperformed the three baseline models, particularly in recall, but struggled with precision, especially when using standard metrics. LSTM showed the closest per- formance in both recall and precision, though it remained slightly below Conv LSTM. LR, while competitive in precision, suffered from low recall, and RF failed to capture meaningful patterns across all data configurations. Therefore, on the one hand, the effort involved in building such an elaborate model was worthwhile, as the other models either struggled with high-dimensional data or, as in the case of LSTM, had to be highly complex to achieve comparable performance. Moreover, it should be noted that Conv LSTM’s high recall values suggest it can predict most instances where at least one crime is likely to occur, minimizing false negatives as intended. However, the low precision results highlight how the extremely unbalanced nature of our forecasting tasks—a byproduct of the fine-grained spatial and temporal scales of the study—makes this approach highly imperfect in terms of reducing false positives. Concerning the crime types, we found that our model performed best when using all crime types together, rather than separating them into violent and property crimes. This outcome was expected since this configuration results in a more balanced dataset, hence leading to more stable training. Additionally, the comparison across four different LB periods indicated that violent crimes tend to yield higher overall predictive performance with longer LB periods, whereas property crimes appear to benefit from shorter ones. When evaluating our model on different sets of features, we were able to determine that the overall best performance was achieved when using all three feature sets together (i.e., crime, mobility, and sociodemographic data), while the worst performance occurred when only historical crime data were used. This suggests that including additional data sources, in combination with crime data, allows the algorithm to learn more effectively the patterns leading to crime occurrence. Moreover, the percentage difference between CS and CMS indicates that mobility data improve performance, especially when using shorter LB periods, stressing how mobility captures aspects of an urban context that transcend its static socio-economic ecology. These findings not only contribute to those strands of criminological literature con- 27 cerned with spatiotemporal crime forecasting and the investigation of the link between mobility and crime, but are also relevant to policymakers. In fact, they contribute to the ongoing debate on the challenges associated with the use of computational approaches by law enforcement agencies [3, 86–89]. Although offering an exhaustive, systematic analysis of the limits of deep learning for crime forecasting is beyond the scope of this work, our results underscore that not even extremely sophisticated algorithms can reach high per- formance in both recall and precision when focusing on fine-grained spatial and temporal scales. This finding—detected for all cities and crime types—bears important implica- tions. Relying on the predictions of our models, for instance, would certainly allow first responders and law enforcement to predict practically all actual offenses, but at the cost of wrongfully targeting certain micro-areas due to high rates of false positives, possibly leading to unintended consequences. While we show that this issue is largely diminished when considering the modified nearest neighbor metrics, the low baseline precision scores speak to the limits that computational models still face when dealing with such a sparse, unbalanced, and volatile phenomenon as crime. 6.1 Limitations This work, despite its promising results, is not without limitations. First, the use of a grid- based approach, while being very common in the literature (see, among others, ) imposes an arbitrary division on our space, and it has been shown that network-based models can outperform grid-based models for crime prediction . Furthermore, an arbitrary division of urban space may also pose a problem, for instance, in terms of administrative or police precinct boundaries, since two points in the same spatial cell might fall under the jurisdiction of two different police precincts. However, a grid-based approach was mandatory given our design focused on the adaptation of computer vision models to crime forecasting. A second potential limitation is that the crime data we used suffered from location obfuscation due to privacy concerns, which limits how finely we can define our spatial unit of analysis. However, our cell size is still considerably smaller than the typical block size in American cities (see for some estimates), including those analyzed in this research. Additionally, given the low precision obtained with our current grid cell size, we are concerned that further reducing the cell size would exacerbate the problem of false positives. Therefore, we argue that obfuscation does not constitute a major issue in our current design, as it is not as micro in scale. Finally, arguably the main limitation of this study is that it does not rely on individual trajectories to capture mobility dynamics in the cities under analysis. More nuanced mobility features, such as mobility flows, have been shown to further improve forecasting performance and would certainly have offered a richer set of opportunities to study how crime concentrates and varies over time along with variation in human movements across different parts of the same urban context. Unfortunately, we were unable to obtain 28 such information due to the limited availability of mobility data for crime research. 6.2 Ethical Implications This study adheres to ethical research practices, particularly regarding the use of mobility and crime data. The data used in this study do not contain individual-level information; instead, they represent aggregate mobility information at the premise level, further aggre- gated at the spatial cell level. This fully ensures the privacy and anonymity of individuals while allowing us to study broader patterns of human activity and their relationship with crime. We also acknowledge the inherent limitations in the reporting of crime data. Certain crimes may be underreported in specific areas of a city due to socioeconomic, cultural, or institutional factors, potentially introducing biases into the analysis. Nonetheless, we have sought to design our framework focusing on those crimes that are less likely to suffer from this issue, as well as crimes that are less prone to be the subject of specific policing decision-making processes, such as those concerning drug-related offenses. Additionally, our model’s statistical results, particularly its low precision, raise signif- icant concerns that could have ethical implications if deployed in real-world scenarios. Relying on these results to allocate police resources risks unjustified overpolicing certain areas, as false positives are pervasive across cities and crime types, possibly exacerbating existing inequalities or disproportionately impacting specific communities. We therefore emphasize that the results of this study are intended to advance the the- oretical understanding of crime and to contribute to the development of methodological approaches in spatiotemporal crime forecasting, rather than to serve as direct recommen- dations for enforcement actions. Future research should focus on addressing these ethical concerns, improving model precision, and incorporating diverse stakeholder perspectives to ensure fair and equitable algorithmic design. Acknowledgments Anonymized for submission. We thank Laia Albors Zumel for comments and suggestions regarding earlier versions of this manuscript and Giuseppe Veltri for partial financial support in the first phase of this research. Author Contributions This work is the result of the joint efforts by all authors, A.A.Z., M.T., and G.M.C. A.A.Z., M.T., and G.M.C. jointly contributed to design the study and to its methodological setup, A.A.Z. performed the coding and modeling part, A.A.Z. and G.M.C. both contributed to the analysis of the results, A.A.Z. wrote the first draft of the paper and M.T. and G.M.C. 29 reviewed and refined it. M.T. contributed funding to acquire the mobility data. G.M.C. supervised the project. Competing Interests The authors declare that they have no financial or non-financial interests, either directly or indirectly, related to the content of this work. Data and Code Availability The code and links to the raw data used in this study have been deposited in Zenodo and are accessible via DOI: 10.5281/zenodo.16042193.